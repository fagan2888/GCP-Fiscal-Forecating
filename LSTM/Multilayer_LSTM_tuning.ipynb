{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* Country: United States, France, Germany, Japan, United Kingdom, Italy, Canada\n",
    "* Time period: 1950-2018, 69 years\n",
    "* Target variable: `ngdp_rpch` for annual data, `ngdp_r_sa_pcha` and `ngdp_r_sa_pchy` (respectively) for quarterly data\n",
    "* Train-test split: 1950-2009 (train, â‰¤ x  years, depends on data availability), x - y (test, z years)   \n",
    "  _Need further discussion. Here I divide the dataset by x/y just as the working paper did. Now for the ML model family we do not need to do such split._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Module 1: Importing the libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "keras = tf.keras\n",
    "\n",
    "# Print all outputs in a code block\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# from tf.random import set_seed\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# from keras.callbacks import ResetStatesCallback()\n",
    "\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten,Dense\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # Use the %tensorflow_version magic if in colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import random\n",
    "# from tensorflow.random import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "\n",
    "seed_global = 42\n",
    "\n",
    "# Source: https://machinelearningmastery.com/reproducible-results-neural-networks-keras/\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(seed_global)\n",
    "\n",
    "#  Giving an eror \n",
    "# from tensorflow import set_random_seed\n",
    "# set_random_seed(seed_global)\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/58638701/importerror-cannot-import-name-set-random-seed-from-tensorflow-c-users-po\n",
    "\n",
    "tf.random.set_seed(seed_global)\n",
    "\n",
    "# Copy paste this code snippet in every model code chunk \n",
    "seed(seed_global)\n",
    "tf.random.set_seed(seed_global)\n",
    "\n",
    "# --Ignore--\n",
    "# tf.random.set_seed(seed)\n",
    "# # This is giving me an error\n",
    "\n",
    "# #  Global Seed\n",
    "# # random.seed (2019) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery gdp_quarterly_q\n",
    "\n",
    "SELECT *\n",
    "FROM `deep-nexus.temp_for_imf_data.WEO_G7_Quarterly`\n",
    "ORDER BY time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "gdp_quarterly_q.year = (gdp_quarterly_q.time+40)//4 + 1950\n",
    "gdp_quarterly_q.quarter = (gdp_quarterly_q.time+40)%4 + 1\n",
    "gdp_quarterly_q.time = gdp_quarterly_q.year.astype('str') + 'Q' + gdp_quarterly_q.quarter.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>ifscode</th>\n",
       "      <th>time</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>111</td>\n",
       "      <td>1950Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>112</td>\n",
       "      <td>1950Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.776393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.776393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>France</td>\n",
       "      <td>132</td>\n",
       "      <td>1950Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.016824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.016824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Germany</td>\n",
       "      <td>134</td>\n",
       "      <td>1950Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Italy</td>\n",
       "      <td>136</td>\n",
       "      <td>1950Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          country  ifscode    time  gdpwgt  lc  le       llf  lulcm  lur  \\\n",
       "0   United States      111  1950Q1     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "1  United Kingdom      112  1950Q1     NaN NaN NaN       NaN    NaN  NaN   \n",
       "2          France      132  1950Q1     NaN NaN NaN       NaN    NaN  NaN   \n",
       "3         Germany      134  1950Q1     NaN NaN NaN       NaN    NaN  NaN   \n",
       "4           Italy      136  1950Q1     NaN NaN NaN       NaN    NaN  NaN   \n",
       "\n",
       "   ncg_r  ...  pcpi_sa  pcpi_sa_pcha  pcpi_sa_pchy      pppgdp  pppsh  \\\n",
       "0    NaN  ...      NaN           NaN           NaN  301.782705    NaN   \n",
       "1    NaN  ...      NaN           NaN           NaN   72.776393    NaN   \n",
       "2    NaN  ...      NaN           NaN           NaN   47.016824    NaN   \n",
       "3    NaN  ...      NaN           NaN           NaN         NaN    NaN   \n",
       "4    NaN  ...      NaN           NaN           NaN         NaN    NaN   \n",
       "\n",
       "       pppwgt  tmgwgt  tmwgt  txgwgt  txwgt  \n",
       "0  301.782705     NaN    NaN     NaN    NaN  \n",
       "1   72.776393     NaN    NaN     NaN    NaN  \n",
       "2   47.016824     NaN    NaN     NaN    NaN  \n",
       "3         NaN     NaN    NaN     NaN    NaN  \n",
       "4         NaN     NaN    NaN     NaN    NaN  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdp_quarterly_q = pd.DataFrame(gdp_quarterly_q)\n",
    "gdp_quarterly_q.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['United States', 'United Kingdom', 'France', 'Germany', 'Italy',\n",
       "       'Canada', 'Japan'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Countries: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['United States']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting a subset of countries\n",
    "gdp_quarterly_q.country.unique()\n",
    "\n",
    "selected_countries = list(gdp_quarterly_q.country.unique())[0:1]\n",
    "print(\"\\nSelected Countries: \\n\")\n",
    "selected_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_2 = gdp_quarterly_q\n",
    "\n",
    "# for i in selected_countries:\n",
    "#    dataset_2[i] = gdp_quarterly_q[gdp_quarterly_q['country'] == i]\n",
    "\n",
    "# https://stackoverflow.com/questions/51583888/concatenate-dataframe-name-with-variable-value-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest Regressor\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # random forest model creation\n",
    "\n",
    "# # Set Seed\n",
    "# seed(seed_global)\n",
    "# tensorflow.random.set_seed(seed_global)\n",
    "\n",
    "\n",
    "# rfr = RandomForestRegressor(n_estimators = 1000)\n",
    "\n",
    "# rfr.fit(X_train, y_train)\n",
    "\n",
    "# # predictions\n",
    "# y_pred = rfr.predict(X_test)\n",
    "\n",
    "# metrics_mse[\"random_forest\"] =  mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# print(\"Random Forest Test MSE: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Variable Importance\n",
    "\n",
    "# # Top i factors by importance\n",
    "\n",
    "# i = 20\n",
    "# importances = rf_reg.feature_importances_\n",
    "# indices = np.argsort(importances)[-(i-1):]\n",
    "# features = X.columns\n",
    "\n",
    "# plt.figure(figsize=(6,6))\n",
    "# plt.title('Feature Importances - Random Forest Regressor')\n",
    "# plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "# plt.yticks(range(len(indices)), features[indices])\n",
    "# plt.xlabel('Relative Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>ifscode</th>\n",
       "      <th>time</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>111</td>\n",
       "      <td>1950Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>United States</td>\n",
       "      <td>111</td>\n",
       "      <td>1950Q2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>United States</td>\n",
       "      <td>111</td>\n",
       "      <td>1950Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>United States</td>\n",
       "      <td>111</td>\n",
       "      <td>1950Q4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>United States</td>\n",
       "      <td>111</td>\n",
       "      <td>1951Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>348.993057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>348.993057</td>\n",
       "      <td>8.709073</td>\n",
       "      <td>11.508033</td>\n",
       "      <td>10.159067</td>\n",
       "      <td>12.875869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          country  ifscode    time  gdpwgt  lc  le       llf  lulcm  lur  \\\n",
       "0   United States      111  1950Q1     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "7   United States      111  1950Q2     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "14  United States      111  1950Q3     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "21  United States      111  1950Q4     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "28  United States      111  1951Q1     NaN NaN NaN  0.062002    NaN  NaN   \n",
       "\n",
       "    ncg_r  ...  pcpi_sa  pcpi_sa_pcha  pcpi_sa_pchy      pppgdp  pppsh  \\\n",
       "0     NaN  ...      NaN           NaN           NaN  301.782705    NaN   \n",
       "7     NaN  ...      NaN           NaN           NaN  301.782705    NaN   \n",
       "14    NaN  ...      NaN           NaN           NaN  301.782705    NaN   \n",
       "21    NaN  ...      NaN           NaN           NaN  301.782705    NaN   \n",
       "28    NaN  ...      NaN           NaN           NaN  348.993057    NaN   \n",
       "\n",
       "        pppwgt    tmgwgt      tmwgt     txgwgt      txwgt  \n",
       "0   301.782705       NaN        NaN        NaN        NaN  \n",
       "7   301.782705       NaN        NaN        NaN        NaN  \n",
       "14  301.782705       NaN        NaN        NaN        NaN  \n",
       "21  301.782705       NaN        NaN        NaN        NaN  \n",
       "28  348.993057  8.709073  11.508033  10.159067  12.875869  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter data by country\n",
    "\n",
    "dataset = gdp_quarterly_q[gdp_quarterly_q['country'].isin(selected_countries)]\n",
    "\n",
    "dataset = pd.DataFrame(dataset)\n",
    "# print (\"#\", \"column name\", \"missing values\")\n",
    "# for i in range(len(dataset.columns)):\n",
    "#     print(i, dataset.columns[i], \" \", dataset.iloc[i].isnull().count())\n",
    "\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10628.0</td>\n",
       "      <td>153.952333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.571</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>2565.6</td>\n",
       "      <td>0.469925</td>\n",
       "      <td>12729.7</td>\n",
       "      <td>1.139334</td>\n",
       "      <td>...</td>\n",
       "      <td>247.273333</td>\n",
       "      <td>3.141889</td>\n",
       "      <td>2.109865</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>15.284951</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>2221.075</td>\n",
       "      <td>2739.425</td>\n",
       "      <td>1444.025</td>\n",
       "      <td>2220.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10786.0</td>\n",
       "      <td>154.951667</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>111.839</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>2578.3</td>\n",
       "      <td>0.495011</td>\n",
       "      <td>12782.9</td>\n",
       "      <td>0.417920</td>\n",
       "      <td>...</td>\n",
       "      <td>249.250333</td>\n",
       "      <td>3.236639</td>\n",
       "      <td>2.222997</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10876.1</td>\n",
       "      <td>155.449000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.132</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>0.531358</td>\n",
       "      <td>12909.2</td>\n",
       "      <td>0.988039</td>\n",
       "      <td>...</td>\n",
       "      <td>250.578667</td>\n",
       "      <td>2.148827</td>\n",
       "      <td>2.668825</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10994.3</td>\n",
       "      <td>155.879000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.681</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.540123</td>\n",
       "      <td>13019.8</td>\n",
       "      <td>0.856753</td>\n",
       "      <td>...</td>\n",
       "      <td>251.828667</td>\n",
       "      <td>2.010362</td>\n",
       "      <td>2.632912</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>11057.4</td>\n",
       "      <td>156.776667</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>111.370</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>2605.7</td>\n",
       "      <td>-0.011512</td>\n",
       "      <td>13066.3</td>\n",
       "      <td>0.357148</td>\n",
       "      <td>...</td>\n",
       "      <td>252.759000</td>\n",
       "      <td>1.485933</td>\n",
       "      <td>2.218463</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gdpwgt       lc          le       llf    lulcm       lur   ncg_r  \\\n",
       "1897  18155.700000  10628.0  153.952333  0.160311  110.571  4.133333  2565.6   \n",
       "1904  18819.741667  10786.0  154.951667  0.162071  111.839  4.066667  2578.3   \n",
       "1911  18819.741667  10876.1  155.449000  0.162071  110.132  3.900000  2592.0   \n",
       "1918  18819.741667  10994.3  155.879000  0.162071  110.681  3.800000  2606.0   \n",
       "1925  18819.741667  11057.4  156.776667  0.162071  111.370  3.800000  2605.7   \n",
       "\n",
       "      ncg_rpch    ncp_r  ncp_rpch  ...     pcpi_sa  pcpi_sa_pcha  \\\n",
       "1897  0.469925  12729.7  1.139334  ...  247.273333      3.141889   \n",
       "1904  0.495011  12782.9  0.417920  ...  249.250333      3.236639   \n",
       "1911  0.531358  12909.2  0.988039  ...  250.578667      2.148827   \n",
       "1918  0.540123  13019.8  0.856753  ...  251.828667      2.010362   \n",
       "1925 -0.011512  13066.3  0.357148  ...  252.759000      1.485933   \n",
       "\n",
       "      pcpi_sa_pchy    pppgdp      pppsh    pppwgt    tmgwgt     tmwgt  \\\n",
       "1897      2.109865  19519.40  15.284951  19519.40  2221.075  2739.425   \n",
       "1904      2.222997  20580.25  15.195560  20580.25  2379.800  2932.075   \n",
       "1911      2.668825  20580.25  15.195560  20580.25  2379.800  2932.075   \n",
       "1918      2.632912  20580.25  15.195560  20580.25  2379.800  2932.075   \n",
       "1925      2.218463  20580.25  15.195560  20580.25  2379.800  2932.075   \n",
       "\n",
       "        txgwgt     txwgt  \n",
       "1897  1444.025  2220.625  \n",
       "1904  1538.375  2356.725  \n",
       "1911  1538.375  2356.725  \n",
       "1918  1538.375  2356.725  \n",
       "1925  1538.375  2356.725  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable selection\n",
    "\n",
    "# Input Columns\n",
    "# Selecting 10 variables\n",
    "\n",
    "dataset_input = dataset\n",
    "\n",
    "dataset_input = dataset_input.drop(columns = ['country', 'ifscode', 'time', 'ngdp_r_sa_pcha', 'ngdp_r_sa_pchy', 'ngdp_dpchy'])\n",
    "\n",
    "# Dropped ngdp_dpchy as all values are null\n",
    "\n",
    "dataset_input.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "      <th>time</th>\n",
       "      <th>ngdp_r_sa_pcha</th>\n",
       "      <th>1_step_ahead_ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10456.7</td>\n",
       "      <td>153.815333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.185</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2553.6</td>\n",
       "      <td>0.145104</td>\n",
       "      <td>12586.3</td>\n",
       "      <td>0.586595</td>\n",
       "      <td>...</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>15.284951</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>2221.075</td>\n",
       "      <td>2739.425</td>\n",
       "      <td>1444.025</td>\n",
       "      <td>2220.625</td>\n",
       "      <td>2017Q3</td>\n",
       "      <td>3.202964</td>\n",
       "      <td>3.545494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10628.0</td>\n",
       "      <td>153.952333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.571</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>2565.6</td>\n",
       "      <td>0.469925</td>\n",
       "      <td>12729.7</td>\n",
       "      <td>1.139334</td>\n",
       "      <td>...</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>15.284951</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>2221.075</td>\n",
       "      <td>2739.425</td>\n",
       "      <td>1444.025</td>\n",
       "      <td>2220.625</td>\n",
       "      <td>2017Q4</td>\n",
       "      <td>3.545494</td>\n",
       "      <td>2.552107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10786.0</td>\n",
       "      <td>154.951667</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>111.839</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>2578.3</td>\n",
       "      <td>0.495011</td>\n",
       "      <td>12782.9</td>\n",
       "      <td>0.417920</td>\n",
       "      <td>...</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>2018Q1</td>\n",
       "      <td>2.552107</td>\n",
       "      <td>3.512025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10876.1</td>\n",
       "      <td>155.449000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.132</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>0.531358</td>\n",
       "      <td>12909.2</td>\n",
       "      <td>0.988039</td>\n",
       "      <td>...</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>2018Q2</td>\n",
       "      <td>3.512025</td>\n",
       "      <td>2.926498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10994.3</td>\n",
       "      <td>155.879000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.681</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.540123</td>\n",
       "      <td>13019.8</td>\n",
       "      <td>0.856753</td>\n",
       "      <td>...</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>2018Q3</td>\n",
       "      <td>2.926498</td>\n",
       "      <td>1.089155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gdpwgt       lc          le       llf    lulcm       lur   ncg_r  \\\n",
       "1890  18155.700000  10456.7  153.815333  0.160311  110.185  4.300000  2553.6   \n",
       "1897  18155.700000  10628.0  153.952333  0.160311  110.571  4.133333  2565.6   \n",
       "1904  18819.741667  10786.0  154.951667  0.162071  111.839  4.066667  2578.3   \n",
       "1911  18819.741667  10876.1  155.449000  0.162071  110.132  3.900000  2592.0   \n",
       "1918  18819.741667  10994.3  155.879000  0.162071  110.681  3.800000  2606.0   \n",
       "\n",
       "      ncg_rpch    ncp_r  ncp_rpch  ...    pppgdp      pppsh    pppwgt  \\\n",
       "1890  0.145104  12586.3  0.586595  ...  19519.40  15.284951  19519.40   \n",
       "1897  0.469925  12729.7  1.139334  ...  19519.40  15.284951  19519.40   \n",
       "1904  0.495011  12782.9  0.417920  ...  20580.25  15.195560  20580.25   \n",
       "1911  0.531358  12909.2  0.988039  ...  20580.25  15.195560  20580.25   \n",
       "1918  0.540123  13019.8  0.856753  ...  20580.25  15.195560  20580.25   \n",
       "\n",
       "        tmgwgt     tmwgt    txgwgt     txwgt    time  ngdp_r_sa_pcha  \\\n",
       "1890  2221.075  2739.425  1444.025  2220.625  2017Q3        3.202964   \n",
       "1897  2221.075  2739.425  1444.025  2220.625  2017Q4        3.545494   \n",
       "1904  2379.800  2932.075  1538.375  2356.725  2018Q1        2.552107   \n",
       "1911  2379.800  2932.075  1538.375  2356.725  2018Q2        3.512025   \n",
       "1918  2379.800  2932.075  1538.375  2356.725  2018Q3        2.926498   \n",
       "\n",
       "      1_step_ahead_ngdp_r_sa_pcha  \n",
       "1890                     3.545494  \n",
       "1897                     2.552107  \n",
       "1904                     3.512025  \n",
       "1911                     2.926498  \n",
       "1918                     1.089155  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outcome vaiable (Column Name) = ngdp_r_sa_pcha\n",
    "\n",
    "outcome_variable = \"ngdp_r_sa_pcha\"\n",
    "predicted_variable = \"1_step_ahead_\" + outcome_variable\n",
    "\n",
    "dataset_1 = dataset_input\n",
    "dataset_1[\"time\"] = dataset[\"time\"]\n",
    "# dataset_1[num_cols] = dataset[num_cols]\n",
    "dataset_1[outcome_variable] = dataset[outcome_variable]\n",
    "\n",
    "dataset_1[predicted_variable] = dataset_1[outcome_variable].shift(-1)\n",
    "\n",
    "# # Source: https://stackoverflow.com/questions/20095673/shift-column-in-pandas-dataframe-up-by-one\n",
    "\n",
    "dataset_1 = dataset_1[:-1] \n",
    "\n",
    "dataset_1.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>1_step_ahead_ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>2017Q3</td>\n",
       "      <td>3.545494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>2017Q4</td>\n",
       "      <td>2.552107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>3.512025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>2018Q2</td>\n",
       "      <td>2.926498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>2018Q3</td>\n",
       "      <td>1.089155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        time  1_step_ahead_ngdp_r_sa_pcha\n",
       "1890  2017Q3                     3.545494\n",
       "1897  2017Q4                     2.552107\n",
       "1904  2018Q1                     3.512025\n",
       "1911  2018Q2                     2.926498\n",
       "1918  2018Q3                     1.089155"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output Columns\n",
    "\n",
    "# ngdp_r_sa_pcha: WEO: Gross domestic product, constant prices, seasonally adjusted, quarter-over-quarter percent change, annualized (Percent, Units).\n",
    "\n",
    "dataset_Y = dataset_1[[\"time\", predicted_variable]]\n",
    "dataset_Y.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding the lagged variables to the input dataset: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "      <th>ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10456.7</td>\n",
       "      <td>153.815333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.185</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2553.6</td>\n",
       "      <td>0.145104</td>\n",
       "      <td>12586.3</td>\n",
       "      <td>0.586595</td>\n",
       "      <td>...</td>\n",
       "      <td>2.153214</td>\n",
       "      <td>1.981427</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>15.284951</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>2221.075</td>\n",
       "      <td>2739.425</td>\n",
       "      <td>1444.025</td>\n",
       "      <td>2220.625</td>\n",
       "      <td>3.202964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10628.0</td>\n",
       "      <td>153.952333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.571</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>2565.6</td>\n",
       "      <td>0.469925</td>\n",
       "      <td>12729.7</td>\n",
       "      <td>1.139334</td>\n",
       "      <td>...</td>\n",
       "      <td>3.141889</td>\n",
       "      <td>2.109865</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>15.284951</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>2221.075</td>\n",
       "      <td>2739.425</td>\n",
       "      <td>1444.025</td>\n",
       "      <td>2220.625</td>\n",
       "      <td>3.545494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10786.0</td>\n",
       "      <td>154.951667</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>111.839</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>2578.3</td>\n",
       "      <td>0.495011</td>\n",
       "      <td>12782.9</td>\n",
       "      <td>0.417920</td>\n",
       "      <td>...</td>\n",
       "      <td>3.236639</td>\n",
       "      <td>2.222997</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>2.552107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10876.1</td>\n",
       "      <td>155.449000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.132</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>0.531358</td>\n",
       "      <td>12909.2</td>\n",
       "      <td>0.988039</td>\n",
       "      <td>...</td>\n",
       "      <td>2.148827</td>\n",
       "      <td>2.668825</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>3.512025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10994.3</td>\n",
       "      <td>155.879000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.681</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.540123</td>\n",
       "      <td>13019.8</td>\n",
       "      <td>0.856753</td>\n",
       "      <td>...</td>\n",
       "      <td>2.010362</td>\n",
       "      <td>2.632912</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>2.926498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gdpwgt       lc          le       llf    lulcm       lur   ncg_r  \\\n",
       "1890  18155.700000  10456.7  153.815333  0.160311  110.185  4.300000  2553.6   \n",
       "1897  18155.700000  10628.0  153.952333  0.160311  110.571  4.133333  2565.6   \n",
       "1904  18819.741667  10786.0  154.951667  0.162071  111.839  4.066667  2578.3   \n",
       "1911  18819.741667  10876.1  155.449000  0.162071  110.132  3.900000  2592.0   \n",
       "1918  18819.741667  10994.3  155.879000  0.162071  110.681  3.800000  2606.0   \n",
       "\n",
       "      ncg_rpch    ncp_r  ncp_rpch  ...  pcpi_sa_pcha  pcpi_sa_pchy    pppgdp  \\\n",
       "1890  0.145104  12586.3  0.586595  ...      2.153214      1.981427  19519.40   \n",
       "1897  0.469925  12729.7  1.139334  ...      3.141889      2.109865  19519.40   \n",
       "1904  0.495011  12782.9  0.417920  ...      3.236639      2.222997  20580.25   \n",
       "1911  0.531358  12909.2  0.988039  ...      2.148827      2.668825  20580.25   \n",
       "1918  0.540123  13019.8  0.856753  ...      2.010362      2.632912  20580.25   \n",
       "\n",
       "          pppsh    pppwgt    tmgwgt     tmwgt    txgwgt     txwgt  \\\n",
       "1890  15.284951  19519.40  2221.075  2739.425  1444.025  2220.625   \n",
       "1897  15.284951  19519.40  2221.075  2739.425  1444.025  2220.625   \n",
       "1904  15.195560  20580.25  2379.800  2932.075  1538.375  2356.725   \n",
       "1911  15.195560  20580.25  2379.800  2932.075  1538.375  2356.725   \n",
       "1918  15.195560  20580.25  2379.800  2932.075  1538.375  2356.725   \n",
       "\n",
       "      ngdp_r_sa_pcha  \n",
       "1890        3.202964  \n",
       "1897        3.545494  \n",
       "1904        2.552107  \n",
       "1911        3.512025  \n",
       "1918        2.926498  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding the lagged variables to the input dataset: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "      <th>ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10456.7</td>\n",
       "      <td>153.815333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.185</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2553.6</td>\n",
       "      <td>0.145104</td>\n",
       "      <td>12586.3</td>\n",
       "      <td>0.586595</td>\n",
       "      <td>...</td>\n",
       "      <td>2.153214</td>\n",
       "      <td>1.981427</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>15.284951</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>2221.075</td>\n",
       "      <td>2739.425</td>\n",
       "      <td>1444.025</td>\n",
       "      <td>2220.625</td>\n",
       "      <td>3.202964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10628.0</td>\n",
       "      <td>153.952333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.571</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>2565.6</td>\n",
       "      <td>0.469925</td>\n",
       "      <td>12729.7</td>\n",
       "      <td>1.139334</td>\n",
       "      <td>...</td>\n",
       "      <td>3.141889</td>\n",
       "      <td>2.109865</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>15.284951</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>2221.075</td>\n",
       "      <td>2739.425</td>\n",
       "      <td>1444.025</td>\n",
       "      <td>2220.625</td>\n",
       "      <td>3.545494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10786.0</td>\n",
       "      <td>154.951667</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>111.839</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>2578.3</td>\n",
       "      <td>0.495011</td>\n",
       "      <td>12782.9</td>\n",
       "      <td>0.417920</td>\n",
       "      <td>...</td>\n",
       "      <td>3.236639</td>\n",
       "      <td>2.222997</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>2.552107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10876.1</td>\n",
       "      <td>155.449000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.132</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>0.531358</td>\n",
       "      <td>12909.2</td>\n",
       "      <td>0.988039</td>\n",
       "      <td>...</td>\n",
       "      <td>2.148827</td>\n",
       "      <td>2.668825</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>3.512025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10994.3</td>\n",
       "      <td>155.879000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.681</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.540123</td>\n",
       "      <td>13019.8</td>\n",
       "      <td>0.856753</td>\n",
       "      <td>...</td>\n",
       "      <td>2.010362</td>\n",
       "      <td>2.632912</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>2.926498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gdpwgt       lc          le       llf    lulcm       lur   ncg_r  \\\n",
       "1890  18155.700000  10456.7  153.815333  0.160311  110.185  4.300000  2553.6   \n",
       "1897  18155.700000  10628.0  153.952333  0.160311  110.571  4.133333  2565.6   \n",
       "1904  18819.741667  10786.0  154.951667  0.162071  111.839  4.066667  2578.3   \n",
       "1911  18819.741667  10876.1  155.449000  0.162071  110.132  3.900000  2592.0   \n",
       "1918  18819.741667  10994.3  155.879000  0.162071  110.681  3.800000  2606.0   \n",
       "\n",
       "      ncg_rpch    ncp_r  ncp_rpch  ...  pcpi_sa_pcha  pcpi_sa_pchy    pppgdp  \\\n",
       "1890  0.145104  12586.3  0.586595  ...      2.153214      1.981427  19519.40   \n",
       "1897  0.469925  12729.7  1.139334  ...      3.141889      2.109865  19519.40   \n",
       "1904  0.495011  12782.9  0.417920  ...      3.236639      2.222997  20580.25   \n",
       "1911  0.531358  12909.2  0.988039  ...      2.148827      2.668825  20580.25   \n",
       "1918  0.540123  13019.8  0.856753  ...      2.010362      2.632912  20580.25   \n",
       "\n",
       "          pppsh    pppwgt    tmgwgt     tmwgt    txgwgt     txwgt  \\\n",
       "1890  15.284951  19519.40  2221.075  2739.425  1444.025  2220.625   \n",
       "1897  15.284951  19519.40  2221.075  2739.425  1444.025  2220.625   \n",
       "1904  15.195560  20580.25  2379.800  2932.075  1538.375  2356.725   \n",
       "1911  15.195560  20580.25  2379.800  2932.075  1538.375  2356.725   \n",
       "1918  15.195560  20580.25  2379.800  2932.075  1538.375  2356.725   \n",
       "\n",
       "      ngdp_r_sa_pcha  \n",
       "1890        3.202964  \n",
       "1897        3.545494  \n",
       "1904        2.552107  \n",
       "1911        3.512025  \n",
       "1918        2.926498  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Window size and crearting the lagged columns\n",
    "\n",
    "#  Using a lag = 0 for identifying initial variable importance by fitting a randowm forest \n",
    "# window size \n",
    "lag = 0\n",
    "dataset_input_l = dataset_1\n",
    "\n",
    "# Drop the 1) preducted outcome variable and 2) time variable \n",
    "\n",
    "dataset_input_l = dataset_input_l.drop(columns = [\"time\", predicted_variable])\n",
    "\n",
    "print(\"Before adding the lagged variables to the input dataset: \")\n",
    "dataset_input_l.tail(5)\n",
    "\n",
    "# Lagging each column in num_columns by the entire range of lag factors\n",
    "\n",
    "for j in dataset_input_l.columns:\n",
    "    for i in range(1, (lag + 1), 1):\n",
    "        new_col = str(j)+\"-\"+str(i)\n",
    "        dataset_input_l[str(new_col)] = dataset_input_l[str(j)].shift(i)\n",
    "    \n",
    "print(\"After adding the lagged variables to the input dataset: \")\n",
    "dataset_input_l.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>1_step_ahead_ngdp_r_sa_pcha</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "      <th>ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1950Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1950Q2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1950Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1950Q4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1951Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>348.993057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>348.993057</td>\n",
       "      <td>8.709073</td>\n",
       "      <td>11.508033</td>\n",
       "      <td>10.159067</td>\n",
       "      <td>12.875869</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      time  1_step_ahead_ngdp_r_sa_pcha  gdpwgt  lc  le       llf  lulcm  lur  \\\n",
       "0   1950Q1                          NaN     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "7   1950Q2                          NaN     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "14  1950Q3                          NaN     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "21  1950Q4                          NaN     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "28  1951Q1                          NaN     NaN NaN NaN  0.062002    NaN  NaN   \n",
       "\n",
       "    ncg_r  ncg_rpch  ...  pcpi_sa_pcha  pcpi_sa_pchy      pppgdp  pppsh  \\\n",
       "0     NaN       NaN  ...           NaN           NaN  301.782705    NaN   \n",
       "7     NaN       NaN  ...           NaN           NaN  301.782705    NaN   \n",
       "14    NaN       NaN  ...           NaN           NaN  301.782705    NaN   \n",
       "21    NaN       NaN  ...           NaN           NaN  301.782705    NaN   \n",
       "28    NaN       NaN  ...           NaN           NaN  348.993057    NaN   \n",
       "\n",
       "        pppwgt    tmgwgt      tmwgt     txgwgt      txwgt  ngdp_r_sa_pcha  \n",
       "0   301.782705       NaN        NaN        NaN        NaN             NaN  \n",
       "7   301.782705       NaN        NaN        NaN        NaN             NaN  \n",
       "14  301.782705       NaN        NaN        NaN        NaN             NaN  \n",
       "21  301.782705       NaN        NaN        NaN        NaN             NaN  \n",
       "28  348.993057  8.709073  11.508033  10.159067  12.875869             NaN  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(275, 63)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns names:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['time', '1_step_ahead_ngdp_r_sa_pcha', 'gdpwgt', 'lc', 'le', 'llf',\n",
       "       'lulcm', 'lur', 'ncg_r', 'ncg_rpch', 'ncp_r', 'ncp_rpch', 'ncp_rpchy',\n",
       "       'nfbrgdp', 'nfb_r', 'nfdd_r', 'nfdd_rpch', 'nfie_r', 'nfisn_r',\n",
       "       'nfisr_r', 'nfis_r', 'nfi_r', 'nfi_rpch', 'ngdp', 'ngdp_d', 'ngdp_dpch',\n",
       "       'ngdp_d_sa', 'ngdp_d_sa_pchy', 'ngdp_r', 'ngdp_rpch', 'ngdp_rpchy',\n",
       "       'ngdp_r_sa', 'ngdp_r_sa_ar', 'ngdp_sa', 'ngdp_sa_ar', 'nmg_r',\n",
       "       'nmg_rpch', 'nms_r', 'nm_r', 'nm_rpch', 'nshr', 'ntdd_r', 'ntdd_rpch',\n",
       "       'ntdd_rpchy', 'nxg_r', 'nxg_rpch', 'nxs_r', 'nx_r', 'nx_rpch', 'pcpi',\n",
       "       'pcpi_pch', 'pcpi_pchy', 'pcpi_sa', 'pcpi_sa_pcha', 'pcpi_sa_pchy',\n",
       "       'pppgdp', 'pppsh', 'pppwgt', 'tmgwgt', 'tmwgt', 'txgwgt', 'txwgt',\n",
       "       'ngdp_r_sa_pcha'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining Input and Output Values\n",
    "\n",
    "# X1 = dataset_input\n",
    "# X = pd.concat([X1, X2, dataset_Y], axis=1)\n",
    "X = pd.concat([dataset_Y, dataset_input_l], axis=1)\n",
    "X.head(5)\n",
    "X.shape\n",
    "\n",
    "print(\"\\nColumns names:\\n\")\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After dropping rows with missing data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(155, 63)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>1_step_ahead_ngdp_r_sa_pcha</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "      <th>ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>1980Q1</td>\n",
       "      <td>-7.985864</td>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1573.6</td>\n",
       "      <td>99.862333</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>77.648</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>1447.3</td>\n",
       "      <td>0.885264</td>\n",
       "      <td>...</td>\n",
       "      <td>16.741448</td>\n",
       "      <td>14.210019</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>1.261758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>1980Q2</td>\n",
       "      <td>-0.476985</td>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1599.2</td>\n",
       "      <td>98.953333</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>80.939</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>1467.3</td>\n",
       "      <td>1.381884</td>\n",
       "      <td>...</td>\n",
       "      <td>14.194984</td>\n",
       "      <td>14.425770</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>-7.985864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>1980Q3</td>\n",
       "      <td>7.668385</td>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1628.6</td>\n",
       "      <td>98.899000</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>83.201</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>1452.7</td>\n",
       "      <td>-0.995025</td>\n",
       "      <td>...</td>\n",
       "      <td>7.721136</td>\n",
       "      <td>12.935323</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>-0.476985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>1980Q4</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1687.6</td>\n",
       "      <td>99.498667</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>84.538</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>1449.5</td>\n",
       "      <td>-0.220279</td>\n",
       "      <td>...</td>\n",
       "      <td>11.693861</td>\n",
       "      <td>12.538360</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>7.668385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>1981Q1</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>2611.683590</td>\n",
       "      <td>1739.6</td>\n",
       "      <td>100.239000</td>\n",
       "      <td>0.108677</td>\n",
       "      <td>86.287</td>\n",
       "      <td>7.433333</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.793377</td>\n",
       "      <td>...</td>\n",
       "      <td>11.531024</td>\n",
       "      <td>11.261071</td>\n",
       "      <td>3207.025</td>\n",
       "      <td>21.713333</td>\n",
       "      <td>3207.025</td>\n",
       "      <td>248.575</td>\n",
       "      <td>293.825</td>\n",
       "      <td>230.425</td>\n",
       "      <td>280.775</td>\n",
       "      <td>8.070747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       time  1_step_ahead_ngdp_r_sa_pcha       gdpwgt      lc          le  \\\n",
       "840  1980Q1                    -7.985864  2352.456802  1573.6   99.862333   \n",
       "847  1980Q2                    -0.476985  2352.456802  1599.2   98.953333   \n",
       "854  1980Q3                     7.668385  2352.456802  1628.6   98.899000   \n",
       "861  1980Q4                     8.070747  2352.456802  1687.6   99.498667   \n",
       "868  1981Q1                    -2.926867  2611.683590  1739.6  100.239000   \n",
       "\n",
       "          llf   lulcm       lur   ncg_r  ncg_rpch  ...  pcpi_sa_pcha  \\\n",
       "840  0.106979  77.648  6.300000  1447.3  0.885264  ...     16.741448   \n",
       "847  0.106979  80.939  7.333333  1467.3  1.381884  ...     14.194984   \n",
       "854  0.106979  83.201  7.666667  1452.7 -0.995025  ...      7.721136   \n",
       "861  0.106979  84.538  7.400000  1449.5 -0.220279  ...     11.693861   \n",
       "868  0.108677  86.287  7.433333  1461.0  0.793377  ...     11.531024   \n",
       "\n",
       "     pcpi_sa_pchy    pppgdp      pppsh    pppwgt   tmgwgt    tmwgt   txgwgt  \\\n",
       "840     14.210019  2857.325  21.531397  2857.325  212.800  252.675  187.275   \n",
       "847     14.425770  2857.325  21.531397  2857.325  212.800  252.675  187.275   \n",
       "854     12.935323  2857.325  21.531397  2857.325  212.800  252.675  187.275   \n",
       "861     12.538360  2857.325  21.531397  2857.325  212.800  252.675  187.275   \n",
       "868     11.261071  3207.025  21.713333  3207.025  248.575  293.825  230.425   \n",
       "\n",
       "       txwgt  ngdp_r_sa_pcha  \n",
       "840  230.150        1.261758  \n",
       "847  230.150       -7.985864  \n",
       "854  230.150       -0.476985  \n",
       "861  230.150        7.668385  \n",
       "868  280.775        8.070747  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>1_step_ahead_ngdp_r_sa_pcha</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "      <th>ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>2017Q3</td>\n",
       "      <td>3.545494</td>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10456.7</td>\n",
       "      <td>153.815333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.185</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2553.6</td>\n",
       "      <td>0.145104</td>\n",
       "      <td>...</td>\n",
       "      <td>2.153214</td>\n",
       "      <td>1.981427</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>15.284951</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>2221.075</td>\n",
       "      <td>2739.425</td>\n",
       "      <td>1444.025</td>\n",
       "      <td>2220.625</td>\n",
       "      <td>3.202964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>2017Q4</td>\n",
       "      <td>2.552107</td>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10628.0</td>\n",
       "      <td>153.952333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.571</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>2565.6</td>\n",
       "      <td>0.469925</td>\n",
       "      <td>...</td>\n",
       "      <td>3.141889</td>\n",
       "      <td>2.109865</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>15.284951</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>2221.075</td>\n",
       "      <td>2739.425</td>\n",
       "      <td>1444.025</td>\n",
       "      <td>2220.625</td>\n",
       "      <td>3.545494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>3.512025</td>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10786.0</td>\n",
       "      <td>154.951667</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>111.839</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>2578.3</td>\n",
       "      <td>0.495011</td>\n",
       "      <td>...</td>\n",
       "      <td>3.236639</td>\n",
       "      <td>2.222997</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>2.552107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>2018Q2</td>\n",
       "      <td>2.926498</td>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10876.1</td>\n",
       "      <td>155.449000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.132</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>0.531358</td>\n",
       "      <td>...</td>\n",
       "      <td>2.148827</td>\n",
       "      <td>2.668825</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>3.512025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>2018Q3</td>\n",
       "      <td>1.089155</td>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10994.3</td>\n",
       "      <td>155.879000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.681</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.540123</td>\n",
       "      <td>...</td>\n",
       "      <td>2.010362</td>\n",
       "      <td>2.632912</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>2.926498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time  1_step_ahead_ngdp_r_sa_pcha        gdpwgt       lc          le  \\\n",
       "1890  2017Q3                     3.545494  18155.700000  10456.7  153.815333   \n",
       "1897  2017Q4                     2.552107  18155.700000  10628.0  153.952333   \n",
       "1904  2018Q1                     3.512025  18819.741667  10786.0  154.951667   \n",
       "1911  2018Q2                     2.926498  18819.741667  10876.1  155.449000   \n",
       "1918  2018Q3                     1.089155  18819.741667  10994.3  155.879000   \n",
       "\n",
       "           llf    lulcm       lur   ncg_r  ncg_rpch  ...  pcpi_sa_pcha  \\\n",
       "1890  0.160311  110.185  4.300000  2553.6  0.145104  ...      2.153214   \n",
       "1897  0.160311  110.571  4.133333  2565.6  0.469925  ...      3.141889   \n",
       "1904  0.162071  111.839  4.066667  2578.3  0.495011  ...      3.236639   \n",
       "1911  0.162071  110.132  3.900000  2592.0  0.531358  ...      2.148827   \n",
       "1918  0.162071  110.681  3.800000  2606.0  0.540123  ...      2.010362   \n",
       "\n",
       "      pcpi_sa_pchy    pppgdp      pppsh    pppwgt    tmgwgt     tmwgt  \\\n",
       "1890      1.981427  19519.40  15.284951  19519.40  2221.075  2739.425   \n",
       "1897      2.109865  19519.40  15.284951  19519.40  2221.075  2739.425   \n",
       "1904      2.222997  20580.25  15.195560  20580.25  2379.800  2932.075   \n",
       "1911      2.668825  20580.25  15.195560  20580.25  2379.800  2932.075   \n",
       "1918      2.632912  20580.25  15.195560  20580.25  2379.800  2932.075   \n",
       "\n",
       "        txgwgt     txwgt  ngdp_r_sa_pcha  \n",
       "1890  1444.025  2220.625        3.202964  \n",
       "1897  1444.025  2220.625        3.545494  \n",
       "1904  1538.375  2356.725        2.552107  \n",
       "1911  1538.375  2356.725        3.512025  \n",
       "1918  1538.375  2356.725        2.926498  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping all rows with missing data\n",
    "print(\"\\nAfter dropping rows with missing data\")\n",
    "# X = X.iloc[lag:]\n",
    "# X = X.iloc[:-1]\n",
    "X = X.dropna()\n",
    "X.shape\n",
    "X.head(5)\n",
    "X.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Outcome variable dimension (155, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(155, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_step_ahead_ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>-7.985864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>-0.476985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>7.668385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>8.070747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>-2.926867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     1_step_ahead_ngdp_r_sa_pcha\n",
       "840                    -7.985864\n",
       "847                    -0.476985\n",
       "854                     7.668385\n",
       "861                     8.070747\n",
       "868                    -2.926867"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Input matrix: X\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(155, 62)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "      <th>ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>1980Q1</td>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1573.6</td>\n",
       "      <td>99.862333</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>77.648</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>1447.3</td>\n",
       "      <td>0.885264</td>\n",
       "      <td>4277.9</td>\n",
       "      <td>...</td>\n",
       "      <td>16.741448</td>\n",
       "      <td>14.210019</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>1.261758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>1980Q2</td>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1599.2</td>\n",
       "      <td>98.953333</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>80.939</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>1467.3</td>\n",
       "      <td>1.381884</td>\n",
       "      <td>4181.5</td>\n",
       "      <td>...</td>\n",
       "      <td>14.194984</td>\n",
       "      <td>14.425770</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>-7.985864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>1980Q3</td>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1628.6</td>\n",
       "      <td>98.899000</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>83.201</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>1452.7</td>\n",
       "      <td>-0.995025</td>\n",
       "      <td>4227.4</td>\n",
       "      <td>...</td>\n",
       "      <td>7.721136</td>\n",
       "      <td>12.935323</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>-0.476985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>1980Q4</td>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1687.6</td>\n",
       "      <td>99.498667</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>84.538</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>1449.5</td>\n",
       "      <td>-0.220279</td>\n",
       "      <td>4284.5</td>\n",
       "      <td>...</td>\n",
       "      <td>11.693861</td>\n",
       "      <td>12.538360</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>7.668385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>1981Q1</td>\n",
       "      <td>2611.683590</td>\n",
       "      <td>1739.6</td>\n",
       "      <td>100.239000</td>\n",
       "      <td>0.108677</td>\n",
       "      <td>86.287</td>\n",
       "      <td>7.433333</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.793377</td>\n",
       "      <td>4298.8</td>\n",
       "      <td>...</td>\n",
       "      <td>11.531024</td>\n",
       "      <td>11.261071</td>\n",
       "      <td>3207.025</td>\n",
       "      <td>21.713333</td>\n",
       "      <td>3207.025</td>\n",
       "      <td>248.575</td>\n",
       "      <td>293.825</td>\n",
       "      <td>230.425</td>\n",
       "      <td>280.775</td>\n",
       "      <td>8.070747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       time       gdpwgt      lc          le       llf   lulcm       lur  \\\n",
       "840  1980Q1  2352.456802  1573.6   99.862333  0.106979  77.648  6.300000   \n",
       "847  1980Q2  2352.456802  1599.2   98.953333  0.106979  80.939  7.333333   \n",
       "854  1980Q3  2352.456802  1628.6   98.899000  0.106979  83.201  7.666667   \n",
       "861  1980Q4  2352.456802  1687.6   99.498667  0.106979  84.538  7.400000   \n",
       "868  1981Q1  2611.683590  1739.6  100.239000  0.108677  86.287  7.433333   \n",
       "\n",
       "      ncg_r  ncg_rpch   ncp_r  ...  pcpi_sa_pcha  pcpi_sa_pchy    pppgdp  \\\n",
       "840  1447.3  0.885264  4277.9  ...     16.741448     14.210019  2857.325   \n",
       "847  1467.3  1.381884  4181.5  ...     14.194984     14.425770  2857.325   \n",
       "854  1452.7 -0.995025  4227.4  ...      7.721136     12.935323  2857.325   \n",
       "861  1449.5 -0.220279  4284.5  ...     11.693861     12.538360  2857.325   \n",
       "868  1461.0  0.793377  4298.8  ...     11.531024     11.261071  3207.025   \n",
       "\n",
       "         pppsh    pppwgt   tmgwgt    tmwgt   txgwgt    txwgt  ngdp_r_sa_pcha  \n",
       "840  21.531397  2857.325  212.800  252.675  187.275  230.150        1.261758  \n",
       "847  21.531397  2857.325  212.800  252.675  187.275  230.150       -7.985864  \n",
       "854  21.531397  2857.325  212.800  252.675  187.275  230.150       -0.476985  \n",
       "861  21.531397  2857.325  212.800  252.675  187.275  230.150        7.668385  \n",
       "868  21.713333  3207.025  248.575  293.825  230.425  280.775        8.070747  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " columns in input dataset\n",
      ":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['time', 'gdpwgt', 'lc', 'le', 'llf', 'lulcm', 'lur', 'ncg_r',\n",
       "       'ncg_rpch', 'ncp_r', 'ncp_rpch', 'ncp_rpchy', 'nfbrgdp', 'nfb_r',\n",
       "       'nfdd_r', 'nfdd_rpch', 'nfie_r', 'nfisn_r', 'nfisr_r', 'nfis_r',\n",
       "       'nfi_r', 'nfi_rpch', 'ngdp', 'ngdp_d', 'ngdp_dpch', 'ngdp_d_sa',\n",
       "       'ngdp_d_sa_pchy', 'ngdp_r', 'ngdp_rpch', 'ngdp_rpchy', 'ngdp_r_sa',\n",
       "       'ngdp_r_sa_ar', 'ngdp_sa', 'ngdp_sa_ar', 'nmg_r', 'nmg_rpch', 'nms_r',\n",
       "       'nm_r', 'nm_rpch', 'nshr', 'ntdd_r', 'ntdd_rpch', 'ntdd_rpchy', 'nxg_r',\n",
       "       'nxg_rpch', 'nxs_r', 'nx_r', 'nx_rpch', 'pcpi', 'pcpi_pch', 'pcpi_pchy',\n",
       "       'pcpi_sa', 'pcpi_sa_pcha', 'pcpi_sa_pchy', 'pppgdp', 'pppsh', 'pppwgt',\n",
       "       'tmgwgt', 'tmwgt', 'txgwgt', 'txwgt', 'ngdp_r_sa_pcha'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating input and output variables\n",
    "\n",
    "X1 = X\n",
    "\n",
    "Y1 = X1[predicted_variable]\n",
    "\n",
    "Y1 = pd.DataFrame(Y1) # very important step, gave me formatting errors, and wasted 2 hour in debugging   \n",
    "\n",
    "print(\"\\n Outcome variable dimension\", Y1.shape)\n",
    "Y1.shape\n",
    "Y1.head(5)\n",
    "\n",
    "# Dropping outcome variable from input matrix\n",
    "X1 = X1.drop(columns = [predicted_variable])\n",
    "print(\"\\n Input matrix: X\")\n",
    "X1.shape\n",
    "X1.head(5)\n",
    "\n",
    "print(\"\\n columns in input dataset\\n:\")\n",
    "X1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# items in training set: 107\n",
      "\n",
      "# items in test set: 48\n",
      "\n",
      " input training set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(107, 62)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "      <th>ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>1980Q1</td>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1573.6</td>\n",
       "      <td>99.862333</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>77.648</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>1447.3</td>\n",
       "      <td>0.885264</td>\n",
       "      <td>4277.9</td>\n",
       "      <td>...</td>\n",
       "      <td>16.741448</td>\n",
       "      <td>14.210019</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>1.261758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>1980Q2</td>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1599.2</td>\n",
       "      <td>98.953333</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>80.939</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>1467.3</td>\n",
       "      <td>1.381884</td>\n",
       "      <td>4181.5</td>\n",
       "      <td>...</td>\n",
       "      <td>14.194984</td>\n",
       "      <td>14.425770</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>-7.985864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>1980Q3</td>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1628.6</td>\n",
       "      <td>98.899000</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>83.201</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>1452.7</td>\n",
       "      <td>-0.995025</td>\n",
       "      <td>4227.4</td>\n",
       "      <td>...</td>\n",
       "      <td>7.721136</td>\n",
       "      <td>12.935323</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>-0.476985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>1980Q4</td>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1687.6</td>\n",
       "      <td>99.498667</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>84.538</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>1449.5</td>\n",
       "      <td>-0.220279</td>\n",
       "      <td>4284.5</td>\n",
       "      <td>...</td>\n",
       "      <td>11.693861</td>\n",
       "      <td>12.538360</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>7.668385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>1981Q1</td>\n",
       "      <td>2611.683590</td>\n",
       "      <td>1739.6</td>\n",
       "      <td>100.239000</td>\n",
       "      <td>0.108677</td>\n",
       "      <td>86.287</td>\n",
       "      <td>7.433333</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.793377</td>\n",
       "      <td>4298.8</td>\n",
       "      <td>...</td>\n",
       "      <td>11.531024</td>\n",
       "      <td>11.261071</td>\n",
       "      <td>3207.025</td>\n",
       "      <td>21.713333</td>\n",
       "      <td>3207.025</td>\n",
       "      <td>248.575</td>\n",
       "      <td>293.825</td>\n",
       "      <td>230.425</td>\n",
       "      <td>280.775</td>\n",
       "      <td>8.070747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       time       gdpwgt      lc          le       llf   lulcm       lur  \\\n",
       "840  1980Q1  2352.456802  1573.6   99.862333  0.106979  77.648  6.300000   \n",
       "847  1980Q2  2352.456802  1599.2   98.953333  0.106979  80.939  7.333333   \n",
       "854  1980Q3  2352.456802  1628.6   98.899000  0.106979  83.201  7.666667   \n",
       "861  1980Q4  2352.456802  1687.6   99.498667  0.106979  84.538  7.400000   \n",
       "868  1981Q1  2611.683590  1739.6  100.239000  0.108677  86.287  7.433333   \n",
       "\n",
       "      ncg_r  ncg_rpch   ncp_r  ...  pcpi_sa_pcha  pcpi_sa_pchy    pppgdp  \\\n",
       "840  1447.3  0.885264  4277.9  ...     16.741448     14.210019  2857.325   \n",
       "847  1467.3  1.381884  4181.5  ...     14.194984     14.425770  2857.325   \n",
       "854  1452.7 -0.995025  4227.4  ...      7.721136     12.935323  2857.325   \n",
       "861  1449.5 -0.220279  4284.5  ...     11.693861     12.538360  2857.325   \n",
       "868  1461.0  0.793377  4298.8  ...     11.531024     11.261071  3207.025   \n",
       "\n",
       "         pppsh    pppwgt   tmgwgt    tmwgt   txgwgt    txwgt  ngdp_r_sa_pcha  \n",
       "840  21.531397  2857.325  212.800  252.675  187.275  230.150        1.261758  \n",
       "847  21.531397  2857.325  212.800  252.675  187.275  230.150       -7.985864  \n",
       "854  21.531397  2857.325  212.800  252.675  187.275  230.150       -0.476985  \n",
       "861  21.531397  2857.325  212.800  252.675  187.275  230.150        7.668385  \n",
       "868  21.713333  3207.025  248.575  293.825  230.425  280.775        8.070747  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "840   -7.985864\n",
       "847   -0.476985\n",
       "854    7.668385\n",
       "861    8.070747\n",
       "868   -2.926867\n",
       "Name: 1_step_ahead_ngdp_r_sa_pcha, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " input test set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48, 62)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "      <th>ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>2006Q4</td>\n",
       "      <td>12236.20</td>\n",
       "      <td>7624.0</td>\n",
       "      <td>145.606000</td>\n",
       "      <td>0.151394</td>\n",
       "      <td>96.534</td>\n",
       "      <td>4.433333</td>\n",
       "      <td>2443.5</td>\n",
       "      <td>0.846059</td>\n",
       "      <td>10504.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.630622</td>\n",
       "      <td>1.965396</td>\n",
       "      <td>13814.600</td>\n",
       "      <td>18.732023</td>\n",
       "      <td>13814.600</td>\n",
       "      <td>1715.450</td>\n",
       "      <td>2026.425</td>\n",
       "      <td>921.925</td>\n",
       "      <td>1305.225</td>\n",
       "      <td>3.449636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>2007Q1</td>\n",
       "      <td>13021.65</td>\n",
       "      <td>7806.8</td>\n",
       "      <td>146.135000</td>\n",
       "      <td>0.153119</td>\n",
       "      <td>96.994</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2444.9</td>\n",
       "      <td>0.057295</td>\n",
       "      <td>10563.3</td>\n",
       "      <td>...</td>\n",
       "      <td>3.979630</td>\n",
       "      <td>2.431651</td>\n",
       "      <td>14451.875</td>\n",
       "      <td>18.104068</td>\n",
       "      <td>14451.875</td>\n",
       "      <td>1895.725</td>\n",
       "      <td>2243.550</td>\n",
       "      <td>1044.925</td>\n",
       "      <td>1472.600</td>\n",
       "      <td>0.945307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>2007Q2</td>\n",
       "      <td>13021.65</td>\n",
       "      <td>7845.4</td>\n",
       "      <td>145.850667</td>\n",
       "      <td>0.153119</td>\n",
       "      <td>95.793</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2460.5</td>\n",
       "      <td>0.638063</td>\n",
       "      <td>10582.8</td>\n",
       "      <td>...</td>\n",
       "      <td>4.607759</td>\n",
       "      <td>2.665287</td>\n",
       "      <td>14451.875</td>\n",
       "      <td>18.104068</td>\n",
       "      <td>14451.875</td>\n",
       "      <td>1895.725</td>\n",
       "      <td>2243.550</td>\n",
       "      <td>1044.925</td>\n",
       "      <td>1472.600</td>\n",
       "      <td>2.312389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>2007Q3</td>\n",
       "      <td>13021.65</td>\n",
       "      <td>7885.1</td>\n",
       "      <td>145.943667</td>\n",
       "      <td>0.153119</td>\n",
       "      <td>95.084</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>2472.8</td>\n",
       "      <td>0.499898</td>\n",
       "      <td>10642.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.556194</td>\n",
       "      <td>2.348975</td>\n",
       "      <td>14451.875</td>\n",
       "      <td>18.104068</td>\n",
       "      <td>14451.875</td>\n",
       "      <td>1895.725</td>\n",
       "      <td>2243.550</td>\n",
       "      <td>1044.925</td>\n",
       "      <td>1472.600</td>\n",
       "      <td>2.189473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>2007Q4</td>\n",
       "      <td>13021.65</td>\n",
       "      <td>7978.2</td>\n",
       "      <td>146.271333</td>\n",
       "      <td>0.153119</td>\n",
       "      <td>94.925</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>2489.1</td>\n",
       "      <td>0.659172</td>\n",
       "      <td>10672.8</td>\n",
       "      <td>...</td>\n",
       "      <td>4.997587</td>\n",
       "      <td>4.031137</td>\n",
       "      <td>14451.875</td>\n",
       "      <td>18.104068</td>\n",
       "      <td>14451.875</td>\n",
       "      <td>1895.725</td>\n",
       "      <td>2243.550</td>\n",
       "      <td>1044.925</td>\n",
       "      <td>1472.600</td>\n",
       "      <td>2.455478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time    gdpwgt      lc          le       llf   lulcm       lur  \\\n",
       "1589  2006Q4  12236.20  7624.0  145.606000  0.151394  96.534  4.433333   \n",
       "1596  2007Q1  13021.65  7806.8  146.135000  0.153119  96.994  4.500000   \n",
       "1603  2007Q2  13021.65  7845.4  145.850667  0.153119  95.793  4.500000   \n",
       "1610  2007Q3  13021.65  7885.1  145.943667  0.153119  95.084  4.666667   \n",
       "1617  2007Q4  13021.65  7978.2  146.271333  0.153119  94.925  4.800000   \n",
       "\n",
       "       ncg_r  ncg_rpch    ncp_r  ...  pcpi_sa_pcha  pcpi_sa_pchy     pppgdp  \\\n",
       "1589  2443.5  0.846059  10504.5  ...     -1.630622      1.965396  13814.600   \n",
       "1596  2444.9  0.057295  10563.3  ...      3.979630      2.431651  14451.875   \n",
       "1603  2460.5  0.638063  10582.8  ...      4.607759      2.665287  14451.875   \n",
       "1610  2472.8  0.499898  10642.5  ...      2.556194      2.348975  14451.875   \n",
       "1617  2489.1  0.659172  10672.8  ...      4.997587      4.031137  14451.875   \n",
       "\n",
       "          pppsh     pppwgt    tmgwgt     tmwgt    txgwgt     txwgt  \\\n",
       "1589  18.732023  13814.600  1715.450  2026.425   921.925  1305.225   \n",
       "1596  18.104068  14451.875  1895.725  2243.550  1044.925  1472.600   \n",
       "1603  18.104068  14451.875  1895.725  2243.550  1044.925  1472.600   \n",
       "1610  18.104068  14451.875  1895.725  2243.550  1044.925  1472.600   \n",
       "1617  18.104068  14451.875  1895.725  2243.550  1044.925  1472.600   \n",
       "\n",
       "      ngdp_r_sa_pcha  \n",
       "1589        3.449636  \n",
       "1596        0.945307  \n",
       "1603        2.312389  \n",
       "1610        2.189473  \n",
       "1617        2.455478  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1589    0.945307\n",
       "1596    2.312389\n",
       "1603    2.189473\n",
       "1610    2.455478\n",
       "1617   -2.279453\n",
       "Name: 1_step_ahead_ngdp_r_sa_pcha, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_step_ahead_ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>0.945307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>2.312389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>2.189473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>2.455478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>-2.279453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1_step_ahead_ngdp_r_sa_pcha\n",
       "1589                     0.945307\n",
       "1596                     2.312389\n",
       "1603                     2.189473\n",
       "1610                     2.455478\n",
       "1617                    -2.279453"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "# Sequential train-test split\n",
    "train_test_ratio = 0.69\n",
    "\n",
    "training = int(round(X1.shape[0]*train_test_ratio, 0))\n",
    "test = X1.shape[0] - training\n",
    "\n",
    "print(\"# items in training set:\", training)\n",
    "print(\"\\n# items in test set:\", test)\n",
    "\n",
    "X_train = X1.iloc[0:(training),:]\n",
    "y_train = Y1.iloc[0:(training),0]\n",
    "X_test = X1.iloc[training:(X1.shape[0]),:]\n",
    "y_test = Y1.iloc[training:(X1.shape[0]),0]\n",
    "y_test_outcome_value = Y1.iloc[training:(X1.shape[0]),:]\n",
    "\n",
    "print(\"\\n input training set:\")\n",
    "X_train.shape\n",
    "X_train.head(5)\n",
    "\n",
    "y_train.head(5)\n",
    "\n",
    "print(\"\\n input test set:\")\n",
    "X_test.shape\n",
    "X_test.head(5)\n",
    "\n",
    "y_test.head(5)\n",
    "y_test_outcome_value.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py:4117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "      <th>ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1573.6</td>\n",
       "      <td>99.862333</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>77.648</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>1447.3</td>\n",
       "      <td>0.885264</td>\n",
       "      <td>4277.9</td>\n",
       "      <td>-0.142390</td>\n",
       "      <td>...</td>\n",
       "      <td>16.741448</td>\n",
       "      <td>14.210019</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>1.261758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1599.2</td>\n",
       "      <td>98.953333</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>80.939</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>1467.3</td>\n",
       "      <td>1.381884</td>\n",
       "      <td>4181.5</td>\n",
       "      <td>-2.253442</td>\n",
       "      <td>...</td>\n",
       "      <td>14.194984</td>\n",
       "      <td>14.425770</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>-7.985864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1628.6</td>\n",
       "      <td>98.899000</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>83.201</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>1452.7</td>\n",
       "      <td>-0.995025</td>\n",
       "      <td>4227.4</td>\n",
       "      <td>1.097692</td>\n",
       "      <td>...</td>\n",
       "      <td>7.721136</td>\n",
       "      <td>12.935323</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>-0.476985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>2352.456802</td>\n",
       "      <td>1687.6</td>\n",
       "      <td>99.498667</td>\n",
       "      <td>0.106979</td>\n",
       "      <td>84.538</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>1449.5</td>\n",
       "      <td>-0.220279</td>\n",
       "      <td>4284.5</td>\n",
       "      <td>1.350712</td>\n",
       "      <td>...</td>\n",
       "      <td>11.693861</td>\n",
       "      <td>12.538360</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>21.531397</td>\n",
       "      <td>2857.325</td>\n",
       "      <td>212.800</td>\n",
       "      <td>252.675</td>\n",
       "      <td>187.275</td>\n",
       "      <td>230.150</td>\n",
       "      <td>7.668385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>2611.683590</td>\n",
       "      <td>1739.6</td>\n",
       "      <td>100.239000</td>\n",
       "      <td>0.108677</td>\n",
       "      <td>86.287</td>\n",
       "      <td>7.433333</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.793377</td>\n",
       "      <td>4298.8</td>\n",
       "      <td>0.333761</td>\n",
       "      <td>...</td>\n",
       "      <td>11.531024</td>\n",
       "      <td>11.261071</td>\n",
       "      <td>3207.025</td>\n",
       "      <td>21.713333</td>\n",
       "      <td>3207.025</td>\n",
       "      <td>248.575</td>\n",
       "      <td>293.825</td>\n",
       "      <td>230.425</td>\n",
       "      <td>280.775</td>\n",
       "      <td>8.070747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          gdpwgt      lc          le       llf   lulcm       lur   ncg_r  \\\n",
       "840  2352.456802  1573.6   99.862333  0.106979  77.648  6.300000  1447.3   \n",
       "847  2352.456802  1599.2   98.953333  0.106979  80.939  7.333333  1467.3   \n",
       "854  2352.456802  1628.6   98.899000  0.106979  83.201  7.666667  1452.7   \n",
       "861  2352.456802  1687.6   99.498667  0.106979  84.538  7.400000  1449.5   \n",
       "868  2611.683590  1739.6  100.239000  0.108677  86.287  7.433333  1461.0   \n",
       "\n",
       "     ncg_rpch   ncp_r  ncp_rpch  ...  pcpi_sa_pcha  pcpi_sa_pchy    pppgdp  \\\n",
       "840  0.885264  4277.9 -0.142390  ...     16.741448     14.210019  2857.325   \n",
       "847  1.381884  4181.5 -2.253442  ...     14.194984     14.425770  2857.325   \n",
       "854 -0.995025  4227.4  1.097692  ...      7.721136     12.935323  2857.325   \n",
       "861 -0.220279  4284.5  1.350712  ...     11.693861     12.538360  2857.325   \n",
       "868  0.793377  4298.8  0.333761  ...     11.531024     11.261071  3207.025   \n",
       "\n",
       "         pppsh    pppwgt   tmgwgt    tmwgt   txgwgt    txwgt  ngdp_r_sa_pcha  \n",
       "840  21.531397  2857.325  212.800  252.675  187.275  230.150        1.261758  \n",
       "847  21.531397  2857.325  212.800  252.675  187.275  230.150       -7.985864  \n",
       "854  21.531397  2857.325  212.800  252.675  187.275  230.150       -0.476985  \n",
       "861  21.531397  2857.325  212.800  252.675  187.275  230.150        7.668385  \n",
       "868  21.713333  3207.025  248.575  293.825  230.425  280.775        8.070747  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling the training & test sets \n",
    "\n",
    "# Dropping the \"time\" column\n",
    "\n",
    "X_train.drop(columns = ['time'], inplace = True)\n",
    "X_test.drop(columns = ['time'], inplace = True)\n",
    "\n",
    "train_columns = list(X_train.columns)\n",
    "# train_columns\n",
    "\n",
    "# X_test  = X_test.drop(columns = [\"time\"], inplace = True)\n",
    "\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled training input dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "      <th>ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.731506</td>\n",
       "      <td>0.015493</td>\n",
       "      <td>0.492115</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.531239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004329</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120993</td>\n",
       "      <td>0.507389</td>\n",
       "      <td>0.020435</td>\n",
       "      <td>0.835901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863638</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204154</td>\n",
       "      <td>0.556650</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.336250</td>\n",
       "      <td>0.007377</td>\n",
       "      <td>0.781195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516968</td>\n",
       "      <td>0.887036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.431355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019278</td>\n",
       "      <td>0.013137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253309</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.499109</td>\n",
       "      <td>0.016553</td>\n",
       "      <td>0.840178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729705</td>\n",
       "      <td>0.856949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.899275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.026228</td>\n",
       "      <td>0.028071</td>\n",
       "      <td>0.029355</td>\n",
       "      <td>0.038232</td>\n",
       "      <td>0.317610</td>\n",
       "      <td>0.522167</td>\n",
       "      <td>0.013998</td>\n",
       "      <td>0.712191</td>\n",
       "      <td>0.018852</td>\n",
       "      <td>0.603113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720985</td>\n",
       "      <td>0.760141</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.811984</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.023808</td>\n",
       "      <td>0.023199</td>\n",
       "      <td>0.058735</td>\n",
       "      <td>0.04709</td>\n",
       "      <td>0.922389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gdpwgt        lc        le       llf     lulcm       lur     ncg_r  \\\n",
       "0  0.000000  0.000000  0.021103  0.000000  0.000000  0.354680  0.000000   \n",
       "1  0.000000  0.004329  0.001190  0.000000  0.120993  0.507389  0.020435   \n",
       "2  0.000000  0.009301  0.000000  0.000000  0.204154  0.556650  0.005518   \n",
       "3  0.000000  0.019278  0.013137  0.000000  0.253309  0.517241  0.002248   \n",
       "4  0.026228  0.028071  0.029355  0.038232  0.317610  0.522167  0.013998   \n",
       "\n",
       "   ncg_rpch     ncp_r  ncp_rpch  ...  pcpi_sa_pcha  pcpi_sa_pchy    pppgdp  \\\n",
       "0  0.731506  0.015493  0.492115  ...      1.000000      0.983648  0.000000   \n",
       "1  0.835901  0.000000  0.000000  ...      0.863638      1.000000  0.000000   \n",
       "2  0.336250  0.007377  0.781195  ...      0.516968      0.887036  0.000000   \n",
       "3  0.499109  0.016553  0.840178  ...      0.729705      0.856949  0.000000   \n",
       "4  0.712191  0.018852  0.603113  ...      0.720985      0.760141  0.031915   \n",
       "\n",
       "      pppsh    pppwgt    tmgwgt     tmwgt    txgwgt    txwgt  ngdp_r_sa_pcha  \n",
       "0  0.762432  0.000000  0.000000  0.000000  0.000000  0.00000        0.531239  \n",
       "1  0.762432  0.000000  0.000000  0.000000  0.000000  0.00000        0.000000  \n",
       "2  0.762432  0.000000  0.000000  0.000000  0.000000  0.00000        0.431355  \n",
       "3  0.762432  0.000000  0.000000  0.000000  0.000000  0.00000        0.899275  \n",
       "4  0.811984  0.031915  0.023808  0.023199  0.058735  0.04709        0.922389  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled test input dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "      <th>ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095128</td>\n",
       "      <td>0.103261</td>\n",
       "      <td>0.019358</td>\n",
       "      <td>0.782466</td>\n",
       "      <td>0.033506</td>\n",
       "      <td>0.885535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.520792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161348</td>\n",
       "      <td>0.050333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.119305</td>\n",
       "      <td>0.054238</td>\n",
       "      <td>0.443550</td>\n",
       "      <td>0.161546</td>\n",
       "      <td>0.122325</td>\n",
       "      <td>0.114130</td>\n",
       "      <td>0.025968</td>\n",
       "      <td>0.525182</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.695181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846199</td>\n",
       "      <td>0.588765</td>\n",
       "      <td>0.094193</td>\n",
       "      <td>0.822434</td>\n",
       "      <td>0.094193</td>\n",
       "      <td>0.384966</td>\n",
       "      <td>0.278011</td>\n",
       "      <td>0.177483</td>\n",
       "      <td>0.156942</td>\n",
       "      <td>0.670491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.119305</td>\n",
       "      <td>0.065691</td>\n",
       "      <td>0.427312</td>\n",
       "      <td>0.161546</td>\n",
       "      <td>0.051318</td>\n",
       "      <td>0.114130</td>\n",
       "      <td>0.099622</td>\n",
       "      <td>0.714620</td>\n",
       "      <td>0.063593</td>\n",
       "      <td>0.520215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.887625</td>\n",
       "      <td>0.622825</td>\n",
       "      <td>0.094193</td>\n",
       "      <td>0.822434</td>\n",
       "      <td>0.094193</td>\n",
       "      <td>0.384966</td>\n",
       "      <td>0.278011</td>\n",
       "      <td>0.177483</td>\n",
       "      <td>0.156942</td>\n",
       "      <td>0.768803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119305</td>\n",
       "      <td>0.077471</td>\n",
       "      <td>0.432623</td>\n",
       "      <td>0.161546</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.157696</td>\n",
       "      <td>0.669553</td>\n",
       "      <td>0.086532</td>\n",
       "      <td>0.697215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.752320</td>\n",
       "      <td>0.576712</td>\n",
       "      <td>0.094193</td>\n",
       "      <td>0.822434</td>\n",
       "      <td>0.094193</td>\n",
       "      <td>0.384966</td>\n",
       "      <td>0.278011</td>\n",
       "      <td>0.177483</td>\n",
       "      <td>0.156942</td>\n",
       "      <td>0.759964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.119305</td>\n",
       "      <td>0.105095</td>\n",
       "      <td>0.451335</td>\n",
       "      <td>0.161546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163043</td>\n",
       "      <td>0.234655</td>\n",
       "      <td>0.721506</td>\n",
       "      <td>0.098175</td>\n",
       "      <td>0.566902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913335</td>\n",
       "      <td>0.821944</td>\n",
       "      <td>0.094193</td>\n",
       "      <td>0.822434</td>\n",
       "      <td>0.094193</td>\n",
       "      <td>0.384966</td>\n",
       "      <td>0.278011</td>\n",
       "      <td>0.177483</td>\n",
       "      <td>0.156942</td>\n",
       "      <td>0.779093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gdpwgt        lc        le       llf     lulcm       lur     ncg_r  \\\n",
       "0  0.000000  0.000000  0.413340  0.000000  0.095128  0.103261  0.019358   \n",
       "1  0.119305  0.054238  0.443550  0.161546  0.122325  0.114130  0.025968   \n",
       "2  0.119305  0.065691  0.427312  0.161546  0.051318  0.114130  0.099622   \n",
       "3  0.119305  0.077471  0.432623  0.161546  0.009400  0.141304  0.157696   \n",
       "4  0.119305  0.105095  0.451335  0.161546  0.000000  0.163043  0.234655   \n",
       "\n",
       "   ncg_rpch     ncp_r  ncp_rpch  ...  pcpi_sa_pcha  pcpi_sa_pchy    pppgdp  \\\n",
       "0  0.782466  0.033506  0.885535  ...      0.476190      0.520792  0.000000   \n",
       "1  0.525182  0.056100  0.695181  ...      0.846199      0.588765  0.094193   \n",
       "2  0.714620  0.063593  0.520215  ...      0.887625      0.622825  0.094193   \n",
       "3  0.669553  0.086532  0.697215  ...      0.752320      0.576712  0.094193   \n",
       "4  0.721506  0.098175  0.566902  ...      0.913335      0.821944  0.094193   \n",
       "\n",
       "      pppsh    pppwgt    tmgwgt     tmwgt    txgwgt     txwgt  ngdp_r_sa_pcha  \n",
       "0  1.000000  0.000000  0.161348  0.050333  0.000000  0.000000        0.850587  \n",
       "1  0.822434  0.094193  0.384966  0.278011  0.177483  0.156942        0.670491  \n",
       "2  0.822434  0.094193  0.384966  0.278011  0.177483  0.156942        0.768803  \n",
       "3  0.822434  0.094193  0.384966  0.278011  0.177483  0.156942        0.759964  \n",
       "4  0.822434  0.094193  0.384966  0.278011  0.177483  0.156942        0.779093  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling all the numerical variables\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# train_columns = list(X_train.columns) # removing 'time' column for feature scaling\n",
    "# train_columns\n",
    "\n",
    "\n",
    "print(\"\\nScaled training input dataset:\")\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns = train_columns)\n",
    "\n",
    "X_train.head(5)\n",
    "\n",
    "print(\"\\nScaled test input dataset:\")\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns = train_columns)\n",
    "\n",
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does not exist\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "      <th>ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.731506</td>\n",
       "      <td>0.015493</td>\n",
       "      <td>0.492115</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004329</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120993</td>\n",
       "      <td>0.507389</td>\n",
       "      <td>0.020435</td>\n",
       "      <td>0.835901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863638</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204154</td>\n",
       "      <td>0.556650</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.336250</td>\n",
       "      <td>0.007377</td>\n",
       "      <td>0.781195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516968</td>\n",
       "      <td>0.887036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019278</td>\n",
       "      <td>0.013137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253309</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.499109</td>\n",
       "      <td>0.016553</td>\n",
       "      <td>0.840178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729705</td>\n",
       "      <td>0.856949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.899275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.026228</td>\n",
       "      <td>0.028071</td>\n",
       "      <td>0.029355</td>\n",
       "      <td>0.038232</td>\n",
       "      <td>0.317610</td>\n",
       "      <td>0.522167</td>\n",
       "      <td>0.013998</td>\n",
       "      <td>0.712191</td>\n",
       "      <td>0.018852</td>\n",
       "      <td>0.603113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720985</td>\n",
       "      <td>0.760141</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.811984</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.023808</td>\n",
       "      <td>0.023199</td>\n",
       "      <td>0.058735</td>\n",
       "      <td>0.047090</td>\n",
       "      <td>0.922389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.929171</td>\n",
       "      <td>0.938396</td>\n",
       "      <td>0.950484</td>\n",
       "      <td>0.952835</td>\n",
       "      <td>0.662904</td>\n",
       "      <td>0.157635</td>\n",
       "      <td>0.975886</td>\n",
       "      <td>0.634163</td>\n",
       "      <td>0.958456</td>\n",
       "      <td>0.751513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435381</td>\n",
       "      <td>0.196149</td>\n",
       "      <td>0.928999</td>\n",
       "      <td>0.123489</td>\n",
       "      <td>0.928999</td>\n",
       "      <td>0.861511</td>\n",
       "      <td>0.870486</td>\n",
       "      <td>0.874804</td>\n",
       "      <td>0.881311</td>\n",
       "      <td>0.666370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.929171</td>\n",
       "      <td>0.953226</td>\n",
       "      <td>0.957333</td>\n",
       "      <td>0.952835</td>\n",
       "      <td>0.640515</td>\n",
       "      <td>0.157635</td>\n",
       "      <td>0.976499</td>\n",
       "      <td>0.550664</td>\n",
       "      <td>0.963293</td>\n",
       "      <td>0.594471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306060</td>\n",
       "      <td>0.185154</td>\n",
       "      <td>0.928999</td>\n",
       "      <td>0.123489</td>\n",
       "      <td>0.928999</td>\n",
       "      <td>0.861511</td>\n",
       "      <td>0.870486</td>\n",
       "      <td>0.874804</td>\n",
       "      <td>0.881311</td>\n",
       "      <td>0.605169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981906</td>\n",
       "      <td>0.975947</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.709081</td>\n",
       "      <td>0.123153</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.746615</td>\n",
       "      <td>0.981534</td>\n",
       "      <td>0.785331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215922</td>\n",
       "      <td>0.186381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.770543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990006</td>\n",
       "      <td>0.989492</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.693934</td>\n",
       "      <td>0.108374</td>\n",
       "      <td>0.993154</td>\n",
       "      <td>0.487360</td>\n",
       "      <td>0.989907</td>\n",
       "      <td>0.643350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299431</td>\n",
       "      <td>0.204070</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.512677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.669632</td>\n",
       "      <td>0.108374</td>\n",
       "      <td>0.996935</td>\n",
       "      <td>0.577563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308594</td>\n",
       "      <td>0.159796</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.494427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gdpwgt        lc        le       llf     lulcm       lur     ncg_r  \\\n",
       "0    0.000000  0.000000  0.021103  0.000000  0.000000  0.354680  0.000000   \n",
       "1    0.000000  0.004329  0.001190  0.000000  0.120993  0.507389  0.020435   \n",
       "2    0.000000  0.009301  0.000000  0.000000  0.204154  0.556650  0.005518   \n",
       "3    0.000000  0.019278  0.013137  0.000000  0.253309  0.517241  0.002248   \n",
       "4    0.026228  0.028071  0.029355  0.038232  0.317610  0.522167  0.013998   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "102  0.929171  0.938396  0.950484  0.952835  0.662904  0.157635  0.975886   \n",
       "103  0.929171  0.953226  0.957333  0.952835  0.640515  0.157635  0.976499   \n",
       "104  1.000000  0.981906  0.975947  1.000000  0.709081  0.123153  1.000000   \n",
       "105  1.000000  0.990006  0.989492  1.000000  0.693934  0.108374  0.993154   \n",
       "106  1.000000  1.000000  1.000000  1.000000  0.669632  0.108374  0.996935   \n",
       "\n",
       "     ncg_rpch     ncp_r  ncp_rpch  ...  pcpi_sa_pcha  pcpi_sa_pchy    pppgdp  \\\n",
       "0    0.731506  0.015493  0.492115  ...      1.000000      0.983648  0.000000   \n",
       "1    0.835901  0.000000  0.000000  ...      0.863638      1.000000  0.000000   \n",
       "2    0.336250  0.007377  0.781195  ...      0.516968      0.887036  0.000000   \n",
       "3    0.499109  0.016553  0.840178  ...      0.729705      0.856949  0.000000   \n",
       "4    0.712191  0.018852  0.603113  ...      0.720985      0.760141  0.031915   \n",
       "..        ...       ...       ...  ...           ...           ...       ...   \n",
       "102  0.634163  0.958456  0.751513  ...      0.435381      0.196149  0.928999   \n",
       "103  0.550664  0.963293  0.594471  ...      0.306060      0.185154  0.928999   \n",
       "104  0.746615  0.981534  0.785331  ...      0.215922      0.186381  1.000000   \n",
       "105  0.487360  0.989907  0.643350  ...      0.299431      0.204070  1.000000   \n",
       "106  0.577563  1.000000  0.666876  ...      0.308594      0.159796  1.000000   \n",
       "\n",
       "        pppsh    pppwgt    tmgwgt     tmwgt    txgwgt     txwgt  \\\n",
       "0    0.762432  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1    0.762432  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2    0.762432  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3    0.762432  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4    0.811984  0.031915  0.023808  0.023199  0.058735  0.047090   \n",
       "..        ...       ...       ...       ...       ...       ...   \n",
       "102  0.123489  0.928999  0.861511  0.870486  0.874804  0.881311   \n",
       "103  0.123489  0.928999  0.861511  0.870486  0.874804  0.881311   \n",
       "104  0.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "105  0.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "106  0.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "\n",
       "     ngdp_r_sa_pcha  \n",
       "0          0.531239  \n",
       "1          0.000000  \n",
       "2          0.431355  \n",
       "3          0.899275  \n",
       "4          0.922389  \n",
       "..              ...  \n",
       "102        0.666370  \n",
       "103        0.605169  \n",
       "104        0.770543  \n",
       "105        0.512677  \n",
       "106        0.494427  \n",
       "\n",
       "[107 rows x 61 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'time' in X_train.columns:\n",
    "    print(\"Does not exist\")\n",
    "else:\n",
    "    print(\"Does not exist\")\n",
    "    \n",
    "X_train\n",
    "\n",
    "# train_cols = X_train.columns\n",
    "\n",
    "# train_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set reshaped: (107, 61, 1)\n",
      "test set reshaped: (48, 61, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test sets\n",
    "\n",
    "X_train_1 = np.array(X_train).reshape(X_train.shape[0], X_train.shape[1],1)\n",
    "print(\"training set reshaped:\", X_train_1.shape)\n",
    "\n",
    "X_test_1 = np.array(X_test).reshape(X_test.shape[0], X_test.shape[1],1)\n",
    "print(\"test set reshaped:\", X_test_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from keras.layers import LSTM\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.init(project='IMF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "      'name': 'val_loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "                  'epochs': {'values': [10, 20, 40]},\n",
    "                  'batch_size': {'values': [16, 32, 64]},\n",
    "                  'nn_units': {'values': [16, 32, 64]},\n",
    "                  'dout_rate': {'values': [0.0, 0.2, 0.4, 0.6]}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: mdvu8oxq\n",
      "Sweep URL: https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\n"
     ]
    }
   ],
   "source": [
    "sweepid = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    # Specify the hyperparameter to be tuned along with\n",
    "    # an initial value\n",
    "    config_defaults = {\n",
    "        'epochs': 10,\n",
    "        'batch_size': 16,\n",
    "        'nn_units': 16,\n",
    "        'dout_rate': 0.0\n",
    "    }\n",
    "    \n",
    "    # Initialize a new wandb run\n",
    "    wandb.init(config=config_defaults)\n",
    "    \n",
    "    # Config is a variable that holds and saves hyperparameters and inputs\n",
    "    config = wandb.config\n",
    "    \n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = config.nn_units, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "    model.add(Dropout(config.dout_rate))\n",
    "    model.add(LSTM(units = config.nn_units, return_sequences = True))\n",
    "    model.add(Dropout(config.dout_rate))\n",
    "    model.add(LSTM(units = config.nn_units, return_sequences = True))\n",
    "    model.add(Dropout(config.dout_rate))\n",
    "    model.add(LSTM(units = config.nn_units, return_sequences = True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units = config.nn_units, return_sequences = True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units = config.nn_units, return_sequences = True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units = config.nn_units))\n",
    "    model.add(Dense(units = 1, activation='relu'))\n",
    "    \n",
    "    # Complie the model\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['mse'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_1, y_train, epochs = config.epochs, batch_size = config.batch_size, \n",
    "              validation_split = 0.3, callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: e320ewib with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 10\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: e320ewib\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/e320ewib\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/e320ewib</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 6s 83ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: e320ewib \n",
      "\n",
      "wandb: Agent Starting Run: o871ux4d with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 10\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: o871ux4d\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/o871ux4d\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/o871ux4d</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 19.7882 - mse: 19.7882 - val_loss: 12.3890 - val_mse: 12.3890\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 17.2183 - mse: 17.2183 - val_loss: 7.1481 - val_mse: 7.1481\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 11.7622 - mse: 11.7622 - val_loss: 4.9600 - val_mse: 4.9600\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.4717 - mse: 10.4717 - val_loss: 4.8120 - val_mse: 4.8120\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1952 - mse: 10.1952 - val_loss: 4.9200 - val_mse: 4.9200\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1631 - mse: 10.1631 - val_loss: 4.8406 - val_mse: 4.8406\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1306 - mse: 10.1306 - val_loss: 4.7905 - val_mse: 4.7905\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0874 - mse: 10.0874 - val_loss: 4.7818 - val_mse: 4.7818\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1438 - mse: 10.1438 - val_loss: 4.7553 - val_mse: 4.7553\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0959 - mse: 10.0959 - val_loss: 4.7551 - val_mse: 4.7551\n",
      "wandb: Agent Finished Run: o871ux4d \n",
      "\n",
      "wandb: Agent Starting Run: uys5wgo8 with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 10\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: uys5wgo8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/uys5wgo8\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/uys5wgo8</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 59ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: uys5wgo8 \n",
      "\n",
      "wandb: Agent Starting Run: 89kwnq7a with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 20\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: 89kwnq7a\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/89kwnq7a\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/89kwnq7a</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 89kwnq7a \n",
      "\n",
      "wandb: Agent Starting Run: lanl7tag with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 20\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: lanl7tag\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/lanl7tag\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/lanl7tag</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', '1918f2ad0a4464bb95b2e8887fef65fa7b922513']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 19.7401 - mse: 19.7401 - val_loss: 12.1583 - val_mse: 12.1583\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 16.8910 - mse: 16.8910 - val_loss: 6.8947 - val_mse: 6.8947\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 11.5560 - mse: 11.5560 - val_loss: 4.9107 - val_mse: 4.9107\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.4354 - mse: 10.4354 - val_loss: 4.8273 - val_mse: 4.8273\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1962 - mse: 10.1962 - val_loss: 4.9256 - val_mse: 4.9256\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1660 - mse: 10.1660 - val_loss: 4.8398 - val_mse: 4.8398\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1308 - mse: 10.1308 - val_loss: 4.7888 - val_mse: 4.7888\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0863 - mse: 10.0863 - val_loss: 4.7799 - val_mse: 4.7799\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1429 - mse: 10.1429 - val_loss: 4.7550 - val_mse: 4.7550\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0960 - mse: 10.0960 - val_loss: 4.7549 - val_mse: 4.7549\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1135 - mse: 10.1135 - val_loss: 4.7554 - val_mse: 4.7554\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0812 - mse: 10.0812 - val_loss: 4.7631 - val_mse: 4.7631\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0923 - mse: 10.0923 - val_loss: 4.7862 - val_mse: 4.7862\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1014 - mse: 10.1014 - val_loss: 4.7828 - val_mse: 4.7828\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0834 - mse: 10.0834 - val_loss: 4.7914 - val_mse: 4.7914\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1091 - mse: 10.1091 - val_loss: 4.8002 - val_mse: 4.8002\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1048 - mse: 10.1048 - val_loss: 4.7779 - val_mse: 4.7779\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0839 - mse: 10.0839 - val_loss: 4.7770 - val_mse: 4.7770\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0842 - mse: 10.0842 - val_loss: 4.7741 - val_mse: 4.7741\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0973 - mse: 10.0973 - val_loss: 4.7620 - val_mse: 4.7620\n",
      "wandb: Agent Finished Run: lanl7tag \n",
      "\n",
      "wandb: Agent Starting Run: khua8xus with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 20\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: khua8xus\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/khua8xus\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/khua8xus</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 58ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: khua8xus \n",
      "\n",
      "wandb: Agent Starting Run: mn4k1mwi with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 40\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: mn4k1mwi\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/mn4k1mwi\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/mn4k1mwi</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: mn4k1mwi \n",
      "\n",
      "wandb: Agent Starting Run: v3ulfkgv with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 40\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: v3ulfkgv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/v3ulfkgv\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/v3ulfkgv</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 19.7401 - mse: 19.7401 - val_loss: 12.1583 - val_mse: 12.1583\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 16.8910 - mse: 16.8910 - val_loss: 6.8947 - val_mse: 6.8947\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 11.5560 - mse: 11.5560 - val_loss: 4.9107 - val_mse: 4.9107\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.4354 - mse: 10.4354 - val_loss: 4.8273 - val_mse: 4.8273\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1962 - mse: 10.1962 - val_loss: 4.9256 - val_mse: 4.9256\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1660 - mse: 10.1660 - val_loss: 4.8398 - val_mse: 4.8398\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1308 - mse: 10.1308 - val_loss: 4.7888 - val_mse: 4.7888\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0863 - mse: 10.0863 - val_loss: 4.7799 - val_mse: 4.7799\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1429 - mse: 10.1429 - val_loss: 4.7550 - val_mse: 4.7550\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0960 - mse: 10.0960 - val_loss: 4.7549 - val_mse: 4.7549\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1135 - mse: 10.1135 - val_loss: 4.7554 - val_mse: 4.7554\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0812 - mse: 10.0812 - val_loss: 4.7631 - val_mse: 4.7631\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0923 - mse: 10.0923 - val_loss: 4.7862 - val_mse: 4.7862\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1014 - mse: 10.1014 - val_loss: 4.7828 - val_mse: 4.7828\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0834 - mse: 10.0834 - val_loss: 4.7914 - val_mse: 4.7914\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1091 - mse: 10.1091 - val_loss: 4.8002 - val_mse: 4.8002\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1048 - mse: 10.1048 - val_loss: 4.7779 - val_mse: 4.7779\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0839 - mse: 10.0839 - val_loss: 4.7770 - val_mse: 4.7770\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0842 - mse: 10.0842 - val_loss: 4.7741 - val_mse: 4.7741\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0973 - mse: 10.0973 - val_loss: 4.7620 - val_mse: 4.7620\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0996 - mse: 10.0996 - val_loss: 4.7703 - val_mse: 4.7703\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0930 - mse: 10.0930 - val_loss: 4.7672 - val_mse: 4.7672\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0947 - mse: 10.0947 - val_loss: 4.7608 - val_mse: 4.7608\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0886 - mse: 10.0886 - val_loss: 4.7629 - val_mse: 4.7629\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0849 - mse: 10.0849 - val_loss: 4.7668 - val_mse: 4.7668\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0930 - mse: 10.0930 - val_loss: 4.7699 - val_mse: 4.7699\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0807 - mse: 10.0807 - val_loss: 4.7868 - val_mse: 4.7868\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0968 - mse: 10.0968 - val_loss: 4.8127 - val_mse: 4.8127\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1123 - mse: 10.1123 - val_loss: 4.8165 - val_mse: 4.8165\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0850 - mse: 10.0850 - val_loss: 4.7856 - val_mse: 4.7856\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1217 - mse: 10.1217 - val_loss: 4.7571 - val_mse: 4.7571\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0948 - mse: 10.0948 - val_loss: 4.7540 - val_mse: 4.7540\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0880 - mse: 10.0880 - val_loss: 4.7565 - val_mse: 4.7565\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0896 - mse: 10.0896 - val_loss: 4.7593 - val_mse: 4.7593\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0884 - mse: 10.0884 - val_loss: 4.7734 - val_mse: 4.7734\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1242 - mse: 10.1242 - val_loss: 4.7936 - val_mse: 4.7936\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0899 - mse: 10.0899 - val_loss: 4.7740 - val_mse: 4.7740\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0852 - mse: 10.0852 - val_loss: 4.7627 - val_mse: 4.7627\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0820 - mse: 10.0820 - val_loss: 4.7567 - val_mse: 4.7567\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1028 - mse: 10.1028 - val_loss: 4.7515 - val_mse: 4.7515\n",
      "wandb: Agent Finished Run: v3ulfkgv \n",
      "\n",
      "wandb: Agent Starting Run: n1cb9prb with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 40\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: n1cb9prb\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/n1cb9prb\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/n1cb9prb</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 60ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: n1cb9prb \n",
      "\n",
      "wandb: Agent Starting Run: 012iz42n with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 10\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: 012iz42n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/012iz42n\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/012iz42n</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 59ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 012iz42n \n",
      "\n",
      "wandb: Agent Starting Run: e6b9betn with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 10\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: e6b9betn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/e6b9betn\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/e6b9betn</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 59ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: e6b9betn \n",
      "\n",
      "wandb: Agent Starting Run: sa2c79n3 with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 10\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: sa2c79n3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/sa2c79n3\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/sa2c79n3</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 60ms/step - loss: 19.5063 - mse: 19.5063 - val_loss: 9.2369 - val_mse: 9.2369\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 12.5222 - mse: 12.5222 - val_loss: 5.2256 - val_mse: 5.2256\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.2613 - mse: 10.2613 - val_loss: 4.7622 - val_mse: 4.7622\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.2091 - mse: 10.2091 - val_loss: 4.7659 - val_mse: 4.7659\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.2358 - mse: 10.2358 - val_loss: 4.8317 - val_mse: 4.8317\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1790 - mse: 10.1790 - val_loss: 4.8195 - val_mse: 4.8195\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.0906 - mse: 10.0906 - val_loss: 4.7799 - val_mse: 4.7799\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0840 - mse: 10.0840 - val_loss: 4.7529 - val_mse: 4.7529\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1182 - mse: 10.1182 - val_loss: 4.7532 - val_mse: 4.7533\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1509 - mse: 10.1509 - val_loss: 4.7554 - val_mse: 4.7554\n",
      "wandb: Agent Finished Run: sa2c79n3 \n",
      "\n",
      "wandb: Agent Starting Run: 2n4oqwzy with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 20\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: 2n4oqwzy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/2n4oqwzy\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/2n4oqwzy</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 58ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 2n4oqwzy \n",
      "\n",
      "wandb: Agent Starting Run: n5a2hohu with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 20\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: n5a2hohu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/n5a2hohu\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/n5a2hohu</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 58ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: n5a2hohu \n",
      "\n",
      "wandb: Agent Starting Run: byzkj8jf with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 20\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: byzkj8jf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/byzkj8jf\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/byzkj8jf</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 60ms/step - loss: 19.6612 - mse: 19.6612 - val_loss: 10.1421 - val_mse: 10.1421\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 13.1090 - mse: 13.1090 - val_loss: 5.1360 - val_mse: 5.1360\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 10.2606 - mse: 10.2606 - val_loss: 4.7931 - val_mse: 4.7931\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.2106 - mse: 10.2106 - val_loss: 4.7648 - val_mse: 4.7648\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.2367 - mse: 10.2367 - val_loss: 4.8302 - val_mse: 4.8302\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1755 - mse: 10.1755 - val_loss: 4.8051 - val_mse: 4.8051\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0875 - mse: 10.0875 - val_loss: 4.7716 - val_mse: 4.7716\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0883 - mse: 10.0883 - val_loss: 4.7545 - val_mse: 4.7545\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1260 - mse: 10.1260 - val_loss: 4.7523 - val_mse: 4.7523\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1530 - mse: 10.1530 - val_loss: 4.7597 - val_mse: 4.7597\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0852 - mse: 10.0852 - val_loss: 4.7867 - val_mse: 4.7867\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1536 - mse: 10.1536 - val_loss: 4.8009 - val_mse: 4.8009\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1920 - mse: 10.1920 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1247 - mse: 10.1247 - val_loss: 4.7526 - val_mse: 4.7526\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1488 - mse: 10.1488 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1368 - mse: 10.1368 - val_loss: 4.7913 - val_mse: 4.7913\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0994 - mse: 10.0994 - val_loss: 4.8150 - val_mse: 4.8150\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.1289 - mse: 10.1289 - val_loss: 4.8407 - val_mse: 4.8407\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1358 - mse: 10.1358 - val_loss: 4.7886 - val_mse: 4.7886\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1686 - mse: 10.1686 - val_loss: 4.7992 - val_mse: 4.7992\n",
      "wandb: Agent Finished Run: byzkj8jf \n",
      "\n",
      "wandb: Agent Starting Run: jhozjp99 with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 40\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: jhozjp99\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/jhozjp99\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/jhozjp99</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: jhozjp99 \n",
      "\n",
      "wandb: Agent Starting Run: c6tivmvs with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 40\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: c6tivmvs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/c6tivmvs\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/c6tivmvs</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 59ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: c6tivmvs \n",
      "\n",
      "wandb: Agent Starting Run: y98fp8hj with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 40\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: y98fp8hj\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/y98fp8hj\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/y98fp8hj</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 5s 62ms/step - loss: 19.6929 - mse: 19.6929 - val_loss: 10.4407 - val_mse: 10.4407\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 2s 21ms/step - loss: 13.3240 - mse: 13.3240 - val_loss: 5.1089 - val_mse: 5.1089\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 10.2684 - mse: 10.2684 - val_loss: 4.8115 - val_mse: 4.8115\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.2135 - mse: 10.2135 - val_loss: 4.7654 - val_mse: 4.7654\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.2339 - mse: 10.2339 - val_loss: 4.8240 - val_mse: 4.8240\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1725 - mse: 10.1725 - val_loss: 4.8015 - val_mse: 4.8015\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0889 - mse: 10.0889 - val_loss: 4.7714 - val_mse: 4.7714\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0883 - mse: 10.0883 - val_loss: 4.7542 - val_mse: 4.7542\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1254 - mse: 10.1254 - val_loss: 4.7521 - val_mse: 4.7521\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1529 - mse: 10.1529 - val_loss: 4.7603 - val_mse: 4.7603\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0850 - mse: 10.0850 - val_loss: 4.7872 - val_mse: 4.7872\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1548 - mse: 10.1548 - val_loss: 4.8004 - val_mse: 4.8004\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1945 - mse: 10.1945 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1261 - mse: 10.1261 - val_loss: 4.7525 - val_mse: 4.7525\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1487 - mse: 10.1487 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1373 - mse: 10.1373 - val_loss: 4.7934 - val_mse: 4.7934\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1004 - mse: 10.1004 - val_loss: 4.8167 - val_mse: 4.8167\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1294 - mse: 10.1294 - val_loss: 4.8411 - val_mse: 4.8411\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1360 - mse: 10.1360 - val_loss: 4.7873 - val_mse: 4.7873\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1686 - mse: 10.1686 - val_loss: 4.7981 - val_mse: 4.7981\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1546 - mse: 10.1546 - val_loss: 4.7631 - val_mse: 4.7631\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0901 - mse: 10.0901 - val_loss: 4.7556 - val_mse: 4.7556\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1144 - mse: 10.1144 - val_loss: 4.7659 - val_mse: 4.7659\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0856 - mse: 10.0856 - val_loss: 4.7547 - val_mse: 4.7547\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1183 - mse: 10.1183 - val_loss: 4.7565 - val_mse: 4.7565\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1115 - mse: 10.1115 - val_loss: 4.7519 - val_mse: 4.7519\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0847 - mse: 10.0847 - val_loss: 4.7681 - val_mse: 4.7681\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0839 - mse: 10.0839 - val_loss: 4.7898 - val_mse: 4.7898\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0883 - mse: 10.0883 - val_loss: 4.8013 - val_mse: 4.8013\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1168 - mse: 10.1168 - val_loss: 4.8220 - val_mse: 4.8220\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1461 - mse: 10.1461 - val_loss: 4.8276 - val_mse: 4.8276\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1335 - mse: 10.1335 - val_loss: 4.7531 - val_mse: 4.7531\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1188 - mse: 10.1188 - val_loss: 4.7516 - val_mse: 4.7516\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1302 - mse: 10.1302 - val_loss: 4.7524 - val_mse: 4.7524\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1180 - mse: 10.1180 - val_loss: 4.7613 - val_mse: 4.7613\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0878 - mse: 10.0878 - val_loss: 4.7669 - val_mse: 4.7669\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0806 - mse: 10.0806 - val_loss: 4.7797 - val_mse: 4.7797\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0879 - mse: 10.0879 - val_loss: 4.7985 - val_mse: 4.7985\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1516 - mse: 10.1516 - val_loss: 4.8439 - val_mse: 4.8439\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1122 - mse: 10.1122 - val_loss: 4.7851 - val_mse: 4.7851\n",
      "wandb: Agent Finished Run: y98fp8hj \n",
      "\n",
      "wandb: Agent Starting Run: k7hrd7qk with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 10\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: k7hrd7qk\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/k7hrd7qk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/k7hrd7qk</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 58ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: k7hrd7qk \n",
      "\n",
      "wandb: Agent Starting Run: k85q3zgp with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 10\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: k85q3zgp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/k85q3zgp\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/k85q3zgp</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 60ms/step - loss: 19.9836 - mse: 19.9836 - val_loss: 13.1988 - val_mse: 13.1988\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 18.1900 - mse: 18.1900 - val_loss: 8.6573 - val_mse: 8.6573\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 12.4327 - mse: 12.4327 - val_loss: 4.7522 - val_mse: 4.7522\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 9.9932 - mse: 9.9932 - val_loss: 4.9590 - val_mse: 4.9590\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.2349 - mse: 10.2349 - val_loss: 4.9382 - val_mse: 4.9382\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1689 - mse: 10.1689 - val_loss: 4.8409 - val_mse: 4.8409\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0992 - mse: 10.0992 - val_loss: 4.7764 - val_mse: 4.7764\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0873 - mse: 10.0873 - val_loss: 4.7618 - val_mse: 4.7618\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1307 - mse: 10.1307 - val_loss: 4.7522 - val_mse: 4.7522\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1034 - mse: 10.1034 - val_loss: 4.7581 - val_mse: 4.7581\n",
      "wandb: Agent Finished Run: k85q3zgp \n",
      "\n",
      "wandb: Agent Starting Run: 1ou1a5w6 with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 10\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: 1ou1a5w6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/1ou1a5w6\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/1ou1a5w6</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 5s 63ms/step - loss: 19.4893 - mse: 19.4893 - val_loss: 9.0660 - val_mse: 9.0660\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 12.4524 - mse: 12.4524 - val_loss: 5.2510 - val_mse: 5.2510\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.2664 - mse: 10.2664 - val_loss: 4.7602 - val_mse: 4.7602\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.2132 - mse: 10.2132 - val_loss: 4.7678 - val_mse: 4.7678\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.2372 - mse: 10.2372 - val_loss: 4.8295 - val_mse: 4.8295\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1795 - mse: 10.1795 - val_loss: 4.8213 - val_mse: 4.8213\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0918 - mse: 10.0918 - val_loss: 4.7817 - val_mse: 4.7817\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0833 - mse: 10.0833 - val_loss: 4.7527 - val_mse: 4.7527\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1166 - mse: 10.1166 - val_loss: 4.7533 - val_mse: 4.7533\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1505 - mse: 10.1505 - val_loss: 4.7549 - val_mse: 4.7549\n",
      "wandb: Agent Finished Run: 1ou1a5w6 \n",
      "\n",
      "wandb: Agent Starting Run: bagk44zq with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 20\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: bagk44zq\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/bagk44zq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/bagk44zq</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 58ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: bagk44zq \n",
      "\n",
      "wandb: Agent Starting Run: keyeol3s with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 20\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: keyeol3s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/keyeol3s\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/keyeol3s</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 60ms/step - loss: 19.9836 - mse: 19.9836 - val_loss: 13.1830 - val_mse: 13.1830\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 18.1613 - mse: 18.1613 - val_loss: 8.5995 - val_mse: 8.5995\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 12.3892 - mse: 12.3892 - val_loss: 4.7524 - val_mse: 4.7524\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 9.9953 - mse: 9.9953 - val_loss: 4.9575 - val_mse: 4.9575\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.2335 - mse: 10.2335 - val_loss: 4.9350 - val_mse: 4.9350\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1671 - mse: 10.1671 - val_loss: 4.8394 - val_mse: 4.8394\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0987 - mse: 10.0987 - val_loss: 4.7763 - val_mse: 4.7763\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0876 - mse: 10.0876 - val_loss: 4.7621 - val_mse: 4.7621\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1305 - mse: 10.1305 - val_loss: 4.7523 - val_mse: 4.7523\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1032 - mse: 10.1032 - val_loss: 4.7583 - val_mse: 4.7583\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0929 - mse: 10.0929 - val_loss: 4.7718 - val_mse: 4.7718\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1471 - mse: 10.1471 - val_loss: 4.8241 - val_mse: 4.8241\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1167 - mse: 10.1167 - val_loss: 4.8030 - val_mse: 4.8030\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0902 - mse: 10.0902 - val_loss: 4.7906 - val_mse: 4.7906\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0905 - mse: 10.0905 - val_loss: 4.7734 - val_mse: 4.7734\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1069 - mse: 10.1069 - val_loss: 4.7697 - val_mse: 4.7697\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0848 - mse: 10.0848 - val_loss: 4.7795 - val_mse: 4.7795\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0839 - mse: 10.0839 - val_loss: 4.7823 - val_mse: 4.7823\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1046 - mse: 10.1046 - val_loss: 4.7981 - val_mse: 4.7981\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1032 - mse: 10.1032 - val_loss: 4.7927 - val_mse: 4.7927\n",
      "wandb: Agent Finished Run: keyeol3s \n",
      "\n",
      "wandb: Agent Starting Run: vhwrb8nd with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 20\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: vhwrb8nd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/vhwrb8nd\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/vhwrb8nd</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 5s 61ms/step - loss: 19.6424 - mse: 19.6424 - val_loss: 10.1426 - val_mse: 10.1426\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 13.1208 - mse: 13.1208 - val_loss: 5.1345 - val_mse: 5.1345\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.2616 - mse: 10.2616 - val_loss: 4.7977 - val_mse: 4.7977\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.2098 - mse: 10.2098 - val_loss: 4.7650 - val_mse: 4.7650\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.2314 - mse: 10.2314 - val_loss: 4.8226 - val_mse: 4.8226\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1724 - mse: 10.1724 - val_loss: 4.8040 - val_mse: 4.8040\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0897 - mse: 10.0897 - val_loss: 4.7733 - val_mse: 4.7733\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0873 - mse: 10.0873 - val_loss: 4.7538 - val_mse: 4.7538\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1237 - mse: 10.1237 - val_loss: 4.7523 - val_mse: 4.7523\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1523 - mse: 10.1523 - val_loss: 4.7591 - val_mse: 4.7591\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0855 - mse: 10.0855 - val_loss: 4.7855 - val_mse: 4.7855\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1538 - mse: 10.1538 - val_loss: 4.8003 - val_mse: 4.8003\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1913 - mse: 10.1913 - val_loss: 4.7515 - val_mse: 4.7515\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1243 - mse: 10.1243 - val_loss: 4.7527 - val_mse: 4.7527\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1486 - mse: 10.1486 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1368 - mse: 10.1368 - val_loss: 4.7914 - val_mse: 4.7914\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0997 - mse: 10.0997 - val_loss: 4.8149 - val_mse: 4.8149\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1286 - mse: 10.1286 - val_loss: 4.8403 - val_mse: 4.8403\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1353 - mse: 10.1353 - val_loss: 4.7883 - val_mse: 4.7883\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1687 - mse: 10.1687 - val_loss: 4.7991 - val_mse: 4.7991\n",
      "wandb: Agent Finished Run: vhwrb8nd \n",
      "\n",
      "wandb: Agent Starting Run: k92lmenp with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 40\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: k92lmenp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/k92lmenp\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/k92lmenp</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 57ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: k92lmenp \n",
      "\n",
      "wandb: Agent Starting Run: v9hlr6sr with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 40\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: v9hlr6sr\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/v9hlr6sr\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/v9hlr6sr</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 59ms/step - loss: 19.9266 - mse: 19.9266 - val_loss: 12.9735 - val_mse: 12.9735\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 17.6877 - mse: 17.6877 - val_loss: 7.6121 - val_mse: 7.6121\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 11.7318 - mse: 11.7318 - val_loss: 4.7680 - val_mse: 4.7680\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 10.0314 - mse: 10.0314 - val_loss: 4.9442 - val_mse: 4.9442\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 10.2140 - mse: 10.2140 - val_loss: 4.9018 - val_mse: 4.9018\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 10.1492 - mse: 10.1492 - val_loss: 4.8261 - val_mse: 4.8261\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0951 - mse: 10.0951 - val_loss: 4.7772 - val_mse: 4.7772\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0894 - mse: 10.0894 - val_loss: 4.7654 - val_mse: 4.7654\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1280 - mse: 10.1280 - val_loss: 4.7534 - val_mse: 4.7534\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1016 - mse: 10.1016 - val_loss: 4.7592 - val_mse: 4.7592\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0935 - mse: 10.0935 - val_loss: 4.7713 - val_mse: 4.7713\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1457 - mse: 10.1457 - val_loss: 4.8215 - val_mse: 4.8215\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1156 - mse: 10.1156 - val_loss: 4.8008 - val_mse: 4.8008\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0895 - mse: 10.0895 - val_loss: 4.7894 - val_mse: 4.7894\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0901 - mse: 10.0901 - val_loss: 4.7731 - val_mse: 4.7731\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1068 - mse: 10.1068 - val_loss: 4.7699 - val_mse: 4.7699\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0848 - mse: 10.0848 - val_loss: 4.7801 - val_mse: 4.7801\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0840 - mse: 10.0840 - val_loss: 4.7829 - val_mse: 4.7829\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1048 - mse: 10.1048 - val_loss: 4.7986 - val_mse: 4.7986\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1032 - mse: 10.1032 - val_loss: 4.7927 - val_mse: 4.7927\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0815 - mse: 10.0815 - val_loss: 4.7727 - val_mse: 4.7727\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0926 - mse: 10.0926 - val_loss: 4.7559 - val_mse: 4.7559\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0839 - mse: 10.0839 - val_loss: 4.7515 - val_mse: 4.7515\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1489 - mse: 10.1489 - val_loss: 4.7566 - val_mse: 4.7566\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1484 - mse: 10.1484 - val_loss: 4.7526 - val_mse: 4.7526\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1159 - mse: 10.1159 - val_loss: 4.7601 - val_mse: 4.7601\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0859 - mse: 10.0859 - val_loss: 4.7669 - val_mse: 4.7669\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0820 - mse: 10.0820 - val_loss: 4.7713 - val_mse: 4.7713\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0845 - mse: 10.0845 - val_loss: 4.7760 - val_mse: 4.7760\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0960 - mse: 10.0960 - val_loss: 4.7813 - val_mse: 4.7813\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1102 - mse: 10.1102 - val_loss: 4.8080 - val_mse: 4.8080\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0936 - mse: 10.0936 - val_loss: 4.7866 - val_mse: 4.7866\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0872 - mse: 10.0872 - val_loss: 4.7715 - val_mse: 4.7715\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0970 - mse: 10.0970 - val_loss: 4.7578 - val_mse: 4.7578\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0955 - mse: 10.0955 - val_loss: 4.7638 - val_mse: 4.7638\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0999 - mse: 10.0999 - val_loss: 4.7595 - val_mse: 4.7595\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0990 - mse: 10.0990 - val_loss: 4.7541 - val_mse: 4.7541\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0932 - mse: 10.0932 - val_loss: 4.7550 - val_mse: 4.7550\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1166 - mse: 10.1166 - val_loss: 4.7536 - val_mse: 4.7536\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1017 - mse: 10.1017 - val_loss: 4.7626 - val_mse: 4.7626\n",
      "wandb: Agent Finished Run: v9hlr6sr \n",
      "\n",
      "wandb: Agent Starting Run: nztapws7 with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 40\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: nztapws7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/nztapws7\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/nztapws7</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 5s 61ms/step - loss: 19.6879 - mse: 19.6878 - val_loss: 10.4212 - val_mse: 10.4212\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 13.3289 - mse: 13.3289 - val_loss: 5.1073 - val_mse: 5.1073\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.2679 - mse: 10.2679 - val_loss: 4.8138 - val_mse: 4.8138\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.2120 - mse: 10.2120 - val_loss: 4.7652 - val_mse: 4.7652\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.2303 - mse: 10.2303 - val_loss: 4.8194 - val_mse: 4.8194\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1698 - mse: 10.1698 - val_loss: 4.7996 - val_mse: 4.7996\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0904 - mse: 10.0904 - val_loss: 4.7722 - val_mse: 4.7722\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0879 - mse: 10.0879 - val_loss: 4.7539 - val_mse: 4.7539\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.1244 - mse: 10.1244 - val_loss: 4.7520 - val_mse: 4.7520\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1524 - mse: 10.1524 - val_loss: 4.7602 - val_mse: 4.7602\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0853 - mse: 10.0853 - val_loss: 4.7866 - val_mse: 4.7866\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1550 - mse: 10.1550 - val_loss: 4.7998 - val_mse: 4.7998\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1944 - mse: 10.1944 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1262 - mse: 10.1262 - val_loss: 4.7526 - val_mse: 4.7526\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1485 - mse: 10.1485 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1374 - mse: 10.1374 - val_loss: 4.7935 - val_mse: 4.7935\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1008 - mse: 10.1008 - val_loss: 4.8167 - val_mse: 4.8167\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1292 - mse: 10.1292 - val_loss: 4.8408 - val_mse: 4.8408\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.1354 - mse: 10.1354 - val_loss: 4.7872 - val_mse: 4.7872\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1686 - mse: 10.1686 - val_loss: 4.7979 - val_mse: 4.7979\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1546 - mse: 10.1546 - val_loss: 4.7631 - val_mse: 4.7631\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0902 - mse: 10.0902 - val_loss: 4.7557 - val_mse: 4.7557\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1145 - mse: 10.1145 - val_loss: 4.7659 - val_mse: 4.7659\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0858 - mse: 10.0858 - val_loss: 4.7547 - val_mse: 4.7547\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1185 - mse: 10.1185 - val_loss: 4.7565 - val_mse: 4.7565\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1120 - mse: 10.1120 - val_loss: 4.7519 - val_mse: 4.7519\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0851 - mse: 10.0851 - val_loss: 4.7680 - val_mse: 4.7680\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0839 - mse: 10.0839 - val_loss: 4.7896 - val_mse: 4.7896\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0886 - mse: 10.0886 - val_loss: 4.8011 - val_mse: 4.8011\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.1170 - mse: 10.1170 - val_loss: 4.8218 - val_mse: 4.8218\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1464 - mse: 10.1464 - val_loss: 4.8275 - val_mse: 4.8275\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1336 - mse: 10.1336 - val_loss: 4.7532 - val_mse: 4.7532\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1186 - mse: 10.1186 - val_loss: 4.7516 - val_mse: 4.7516\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1304 - mse: 10.1304 - val_loss: 4.7524 - val_mse: 4.7524\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1183 - mse: 10.1183 - val_loss: 4.7610 - val_mse: 4.7610\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0884 - mse: 10.0884 - val_loss: 4.7665 - val_mse: 4.7665\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0812 - mse: 10.0812 - val_loss: 4.7793 - val_mse: 4.7793\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0882 - mse: 10.0882 - val_loss: 4.7981 - val_mse: 4.7981\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1519 - mse: 10.1519 - val_loss: 4.8433 - val_mse: 4.8433\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1124 - mse: 10.1124 - val_loss: 4.7851 - val_mse: 4.7851\n",
      "wandb: Agent Finished Run: nztapws7 \n",
      "\n",
      "wandb: Agent Starting Run: bntw2ixy with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 10\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: bntw2ixy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/bntw2ixy\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/bntw2ixy</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 57ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: bntw2ixy \n",
      "\n",
      "wandb: Agent Starting Run: kd2y43w5 with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 10\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: kd2y43w5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/kd2y43w5\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/kd2y43w5</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 59ms/step - loss: 19.9675 - mse: 19.9675 - val_loss: 13.1188 - val_mse: 13.1188\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 18.0023 - mse: 18.0023 - val_loss: 8.0797 - val_mse: 8.0797\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 12.0865 - mse: 12.0865 - val_loss: 4.7552 - val_mse: 4.7552\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0145 - mse: 10.0145 - val_loss: 4.9421 - val_mse: 4.9421\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.2187 - mse: 10.2187 - val_loss: 4.9200 - val_mse: 4.9200\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 10.1586 - mse: 10.1586 - val_loss: 4.8366 - val_mse: 4.8366\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0982 - mse: 10.0982 - val_loss: 4.7781 - val_mse: 4.7781\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 10.0883 - mse: 10.0883 - val_loss: 4.7636 - val_mse: 4.7636\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 10.1308 - mse: 10.1308 - val_loss: 4.7525 - val_mse: 4.7525\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1031 - mse: 10.1031 - val_loss: 4.7585 - val_mse: 4.7585\n",
      "wandb: Agent Finished Run: kd2y43w5 \n",
      "\n",
      "wandb: Agent Starting Run: kkep251k with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 10\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: kkep251k\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/kkep251k\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/kkep251k</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 5s 62ms/step - loss: 19.4897 - mse: 19.4897 - val_loss: 9.0644 - val_mse: 9.0644\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 12.4546 - mse: 12.4546 - val_loss: 5.2468 - val_mse: 5.2468\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.2733 - mse: 10.2733 - val_loss: 4.7633 - val_mse: 4.7633\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 2s 20ms/step - loss: 10.2093 - mse: 10.2093 - val_loss: 4.7671 - val_mse: 4.7671\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.2335 - mse: 10.2335 - val_loss: 4.8255 - val_mse: 4.8255\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.1765 - mse: 10.1765 - val_loss: 4.8180 - val_mse: 4.8180\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.0923 - mse: 10.0923 - val_loss: 4.7815 - val_mse: 4.7815\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.0834 - mse: 10.0834 - val_loss: 4.7525 - val_mse: 4.7525\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.1157 - mse: 10.1157 - val_loss: 4.7531 - val_mse: 4.7531\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.1497 - mse: 10.1497 - val_loss: 4.7551 - val_mse: 4.7551\n",
      "wandb: Agent Finished Run: kkep251k \n",
      "\n",
      "wandb: Agent Starting Run: 3ojazdnl with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 20\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: 3ojazdnl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/3ojazdnl\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/3ojazdnl</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 58ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 3ojazdnl \n",
      "\n",
      "wandb: Agent Starting Run: u80cdt2z with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 20\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: u80cdt2z\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/u80cdt2z\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/u80cdt2z</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 59ms/step - loss: 19.9675 - mse: 19.9675 - val_loss: 13.1007 - val_mse: 13.1007\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 17.9486 - mse: 17.9486 - val_loss: 7.9794 - val_mse: 7.9794\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 12.0194 - mse: 12.0194 - val_loss: 4.7567 - val_mse: 4.7567\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0179 - mse: 10.0179 - val_loss: 4.9418 - val_mse: 4.9418\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.2175 - mse: 10.2175 - val_loss: 4.9166 - val_mse: 4.9166\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1568 - mse: 10.1568 - val_loss: 4.8348 - val_mse: 4.8348\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0976 - mse: 10.0976 - val_loss: 4.7779 - val_mse: 4.7779\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0885 - mse: 10.0885 - val_loss: 4.7640 - val_mse: 4.7640\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1304 - mse: 10.1304 - val_loss: 4.7527 - val_mse: 4.7527\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1028 - mse: 10.1028 - val_loss: 4.7587 - val_mse: 4.7587\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0935 - mse: 10.0935 - val_loss: 4.7717 - val_mse: 4.7717\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1478 - mse: 10.1478 - val_loss: 4.8242 - val_mse: 4.8242\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1170 - mse: 10.1170 - val_loss: 4.8022 - val_mse: 4.8022\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0899 - mse: 10.0899 - val_loss: 4.7897 - val_mse: 4.7897\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0903 - mse: 10.0903 - val_loss: 4.7728 - val_mse: 4.7728\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1070 - mse: 10.1070 - val_loss: 4.7694 - val_mse: 4.7694\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0847 - mse: 10.0847 - val_loss: 4.7797 - val_mse: 4.7797\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0839 - mse: 10.0839 - val_loss: 4.7827 - val_mse: 4.7827\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1050 - mse: 10.1050 - val_loss: 4.7988 - val_mse: 4.7988\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1035 - mse: 10.1035 - val_loss: 4.7929 - val_mse: 4.7929\n",
      "wandb: Agent Finished Run: u80cdt2z \n",
      "\n",
      "wandb: Agent Starting Run: n1ai0339 with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 20\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: n1ai0339\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/n1ai0339\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/n1ai0339</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 5s 63ms/step - loss: 19.4897 - mse: 19.4897 - val_loss: 9.0644 - val_mse: 9.0644\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 12.4546 - mse: 12.4546 - val_loss: 5.2468 - val_mse: 5.2468\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 2s 20ms/step - loss: 10.2733 - mse: 10.2733 - val_loss: 4.7633 - val_mse: 4.7633\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.2093 - mse: 10.2093 - val_loss: 4.7671 - val_mse: 4.7671\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.2335 - mse: 10.2335 - val_loss: 4.8255 - val_mse: 4.8255\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1765 - mse: 10.1765 - val_loss: 4.8180 - val_mse: 4.8180\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0923 - mse: 10.0923 - val_loss: 4.7815 - val_mse: 4.7815\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0834 - mse: 10.0834 - val_loss: 4.7525 - val_mse: 4.7525\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1157 - mse: 10.1157 - val_loss: 4.7531 - val_mse: 4.7531\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1495 - mse: 10.1495 - val_loss: 4.7551 - val_mse: 4.7551\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0872 - mse: 10.0872 - val_loss: 4.7787 - val_mse: 4.7787\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1496 - mse: 10.1496 - val_loss: 4.7998 - val_mse: 4.7998\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1779 - mse: 10.1779 - val_loss: 4.7524 - val_mse: 4.7524\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.1174 - mse: 10.1174 - val_loss: 4.7538 - val_mse: 4.7538\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1480 - mse: 10.1480 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1339 - mse: 10.1339 - val_loss: 4.7849 - val_mse: 4.7849\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0965 - mse: 10.0965 - val_loss: 4.8075 - val_mse: 4.8075\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.1259 - mse: 10.1259 - val_loss: 4.8368 - val_mse: 4.8368\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1343 - mse: 10.1343 - val_loss: 4.7914 - val_mse: 4.7914\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1690 - mse: 10.1690 - val_loss: 4.8030 - val_mse: 4.8030\n",
      "wandb: Agent Finished Run: n1ai0339 \n",
      "\n",
      "wandb: Agent Starting Run: y5mdkoxt with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 40\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: y5mdkoxt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/y5mdkoxt\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/y5mdkoxt</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 58ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: y5mdkoxt \n",
      "\n",
      "wandb: Agent Starting Run: lwfygkkp with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 40\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: lwfygkkp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/lwfygkkp\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/lwfygkkp</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 60ms/step - loss: 19.9046 - mse: 19.9046 - val_loss: 12.8794 - val_mse: 12.8794\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 17.4848 - mse: 17.4848 - val_loss: 7.1823 - val_mse: 7.1823\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 11.5182 - mse: 11.5182 - val_loss: 4.7801 - val_mse: 4.7801\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 10.0497 - mse: 10.0497 - val_loss: 4.9541 - val_mse: 4.9541\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.2176 - mse: 10.2176 - val_loss: 4.8881 - val_mse: 4.8881\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1428 - mse: 10.1428 - val_loss: 4.8144 - val_mse: 4.8144\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0914 - mse: 10.0914 - val_loss: 4.7738 - val_mse: 4.7738\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0913 - mse: 10.0913 - val_loss: 4.7661 - val_mse: 4.7661\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 10.1272 - mse: 10.1272 - val_loss: 4.7542 - val_mse: 4.7542\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 10.1007 - mse: 10.1007 - val_loss: 4.7609 - val_mse: 4.7609\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 10.0941 - mse: 10.0941 - val_loss: 4.7729 - val_mse: 4.7729\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 10.1462 - mse: 10.1462 - val_loss: 4.8228 - val_mse: 4.8228\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1167 - mse: 10.1167 - val_loss: 4.7997 - val_mse: 4.7997\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0890 - mse: 10.0890 - val_loss: 4.7877 - val_mse: 4.7877\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0898 - mse: 10.0898 - val_loss: 4.7719 - val_mse: 4.7719\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1068 - mse: 10.1068 - val_loss: 4.7694 - val_mse: 4.7694\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0848 - mse: 10.0848 - val_loss: 4.7802 - val_mse: 4.7802\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0840 - mse: 10.0840 - val_loss: 4.7834 - val_mse: 4.7834\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1053 - mse: 10.1053 - val_loss: 4.7994 - val_mse: 4.7994\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1034 - mse: 10.1034 - val_loss: 4.7930 - val_mse: 4.7930\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0813 - mse: 10.0813 - val_loss: 4.7725 - val_mse: 4.7725\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0926 - mse: 10.0926 - val_loss: 4.7557 - val_mse: 4.7557\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0841 - mse: 10.0841 - val_loss: 4.7515 - val_mse: 4.7515\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1491 - mse: 10.1491 - val_loss: 4.7567 - val_mse: 4.7567\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.1488 - mse: 10.1488 - val_loss: 4.7525 - val_mse: 4.7525\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1160 - mse: 10.1160 - val_loss: 4.7601 - val_mse: 4.7601\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0860 - mse: 10.0860 - val_loss: 4.7669 - val_mse: 4.7669\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0820 - mse: 10.0820 - val_loss: 4.7713 - val_mse: 4.7713\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0844 - mse: 10.0844 - val_loss: 4.7760 - val_mse: 4.7760\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0961 - mse: 10.0961 - val_loss: 4.7813 - val_mse: 4.7813\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1104 - mse: 10.1104 - val_loss: 4.8081 - val_mse: 4.8081\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0937 - mse: 10.0937 - val_loss: 4.7865 - val_mse: 4.7865\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0873 - mse: 10.0873 - val_loss: 4.7713 - val_mse: 4.7713\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0971 - mse: 10.0971 - val_loss: 4.7577 - val_mse: 4.7577\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0957 - mse: 10.0957 - val_loss: 4.7636 - val_mse: 4.7636\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1001 - mse: 10.1001 - val_loss: 4.7595 - val_mse: 4.7595\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0991 - mse: 10.0991 - val_loss: 4.7541 - val_mse: 4.7541\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0933 - mse: 10.0933 - val_loss: 4.7550 - val_mse: 4.7550\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1168 - mse: 10.1168 - val_loss: 4.7536 - val_mse: 4.7536\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1017 - mse: 10.1017 - val_loss: 4.7626 - val_mse: 4.7626\n",
      "wandb: Agent Finished Run: lwfygkkp \n",
      "\n",
      "wandb: Agent Starting Run: 374j33s1 with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 40\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 16\n",
      "wandb: Agent Started Run: 374j33s1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/374j33s1\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/374j33s1</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 5s 63ms/step - loss: 19.4897 - mse: 19.4897 - val_loss: 9.0644 - val_mse: 9.0644\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 12.4546 - mse: 12.4546 - val_loss: 5.2468 - val_mse: 5.2468\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.2733 - mse: 10.2733 - val_loss: 4.7633 - val_mse: 4.7633\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.2093 - mse: 10.2093 - val_loss: 4.7671 - val_mse: 4.7671\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.2335 - mse: 10.2335 - val_loss: 4.8255 - val_mse: 4.8255\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.1765 - mse: 10.1765 - val_loss: 4.8180 - val_mse: 4.8180\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.0923 - mse: 10.0923 - val_loss: 4.7815 - val_mse: 4.7815\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.0834 - mse: 10.0834 - val_loss: 4.7525 - val_mse: 4.7525\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 10.1157 - mse: 10.1157 - val_loss: 4.7531 - val_mse: 4.7531\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.1495 - mse: 10.1495 - val_loss: 4.7551 - val_mse: 4.7551\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.0872 - mse: 10.0872 - val_loss: 4.7787 - val_mse: 4.7787\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.1496 - mse: 10.1496 - val_loss: 4.7998 - val_mse: 4.7998\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.1779 - mse: 10.1779 - val_loss: 4.7524 - val_mse: 4.7524\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.1174 - mse: 10.1174 - val_loss: 4.7538 - val_mse: 4.7538\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.1480 - mse: 10.1480 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 10.1339 - mse: 10.1339 - val_loss: 4.7849 - val_mse: 4.7849\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.0965 - mse: 10.0965 - val_loss: 4.8075 - val_mse: 4.8075\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 10.1259 - mse: 10.1259 - val_loss: 4.8368 - val_mse: 4.8368\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 2s 20ms/step - loss: 10.1343 - mse: 10.1343 - val_loss: 4.7914 - val_mse: 4.7914\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 2s 20ms/step - loss: 10.1690 - mse: 10.1690 - val_loss: 4.8030 - val_mse: 4.8030\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.1527 - mse: 10.1528 - val_loss: 4.7656 - val_mse: 4.7656\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.0903 - mse: 10.0903 - val_loss: 4.7561 - val_mse: 4.7561\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.1119 - mse: 10.1119 - val_loss: 4.7645 - val_mse: 4.7645\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.0865 - mse: 10.0865 - val_loss: 4.7542 - val_mse: 4.7542\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.1184 - mse: 10.1184 - val_loss: 4.7558 - val_mse: 4.7558\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.1111 - mse: 10.1111 - val_loss: 4.7519 - val_mse: 4.7519\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0855 - mse: 10.0855 - val_loss: 4.7672 - val_mse: 4.7672\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0839 - mse: 10.0839 - val_loss: 4.7882 - val_mse: 4.7882\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0878 - mse: 10.0878 - val_loss: 4.8001 - val_mse: 4.8001\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1165 - mse: 10.1165 - val_loss: 4.8211 - val_mse: 4.8211\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1457 - mse: 10.1457 - val_loss: 4.8277 - val_mse: 4.8277\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1328 - mse: 10.1328 - val_loss: 4.7540 - val_mse: 4.7540\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1161 - mse: 10.1161 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1298 - mse: 10.1298 - val_loss: 4.7524 - val_mse: 4.7524\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1185 - mse: 10.1185 - val_loss: 4.7595 - val_mse: 4.7595\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0889 - mse: 10.0889 - val_loss: 4.7646 - val_mse: 4.7646\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0812 - mse: 10.0812 - val_loss: 4.7774 - val_mse: 4.7774\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.0883 - mse: 10.0883 - val_loss: 4.7969 - val_mse: 4.7969\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 10.1518 - mse: 10.1518 - val_loss: 4.8425 - val_mse: 4.8425\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 10.1129 - mse: 10.1129 - val_loss: 4.7865 - val_mse: 4.7865\n",
      "wandb: Agent Finished Run: 374j33s1 \n",
      "\n",
      "wandb: Agent Starting Run: gf2gyozz with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 10\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: gf2gyozz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/gf2gyozz\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/gf2gyozz</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: gf2gyozz \n",
      "\n",
      "wandb: Agent Starting Run: 02tdvfmf with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 10\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: 02tdvfmf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/02tdvfmf\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/02tdvfmf</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 51ms/step - loss: 20.0449 - mse: 20.0449 - val_loss: 13.2938 - val_mse: 13.2938\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 19.2650 - mse: 19.2650 - val_loss: 11.0215 - val_mse: 11.0215\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 16.6081 - mse: 16.6081 - val_loss: 7.6088 - val_mse: 7.6088\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 13.3103 - mse: 13.3103 - val_loss: 5.7677 - val_mse: 5.7677\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 11.2566 - mse: 11.2566 - val_loss: 4.9570 - val_mse: 4.9570\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.2994 - mse: 10.2994 - val_loss: 4.7516 - val_mse: 4.7516\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0698 - mse: 10.0698 - val_loss: 4.8424 - val_mse: 4.8424\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1784 - mse: 10.1784 - val_loss: 4.9523 - val_mse: 4.9523\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1890 - mse: 10.1890 - val_loss: 4.9249 - val_mse: 4.9249\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1647 - mse: 10.1647 - val_loss: 4.8980 - val_mse: 4.8980\n",
      "wandb: Agent Finished Run: 02tdvfmf \n",
      "\n",
      "wandb: Agent Starting Run: 1967angh with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 10\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: 1967angh\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/1967angh\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/1967angh</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 1967angh \n",
      "\n",
      "wandb: Agent Starting Run: 7r8sghac with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 20\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: 7r8sghac\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/7r8sghac\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/7r8sghac</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 7r8sghac \n",
      "\n",
      "wandb: Agent Starting Run: xhp1njnh with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 20\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: xhp1njnh\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/xhp1njnh\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/xhp1njnh</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 20.0449 - mse: 20.0449 - val_loss: 13.2938 - val_mse: 13.2938\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 19.2650 - mse: 19.2650 - val_loss: 11.0215 - val_mse: 11.0215\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 16.6081 - mse: 16.6081 - val_loss: 7.6088 - val_mse: 7.6088\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 13.3103 - mse: 13.3103 - val_loss: 5.7677 - val_mse: 5.7677\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 11.2566 - mse: 11.2566 - val_loss: 4.9570 - val_mse: 4.9570\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2994 - mse: 10.2994 - val_loss: 4.7516 - val_mse: 4.7516\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0698 - mse: 10.0698 - val_loss: 4.8424 - val_mse: 4.8424\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1784 - mse: 10.1784 - val_loss: 4.9523 - val_mse: 4.9523\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1890 - mse: 10.1890 - val_loss: 4.9249 - val_mse: 4.9249\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1647 - mse: 10.1647 - val_loss: 4.8980 - val_mse: 4.8980\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1675 - mse: 10.1675 - val_loss: 4.8545 - val_mse: 4.8545\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1272 - mse: 10.1272 - val_loss: 4.8197 - val_mse: 4.8197\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0973 - mse: 10.0973 - val_loss: 4.8059 - val_mse: 4.8059\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1055 - mse: 10.1055 - val_loss: 4.7901 - val_mse: 4.7901\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0882 - mse: 10.0882 - val_loss: 4.7837 - val_mse: 4.7837\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0863 - mse: 10.0863 - val_loss: 4.7810 - val_mse: 4.7810\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0878 - mse: 10.0878 - val_loss: 4.7707 - val_mse: 4.7707\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0825 - mse: 10.0825 - val_loss: 4.7707 - val_mse: 4.7707\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0835 - mse: 10.0835 - val_loss: 4.7678 - val_mse: 4.7678\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0941 - mse: 10.0941 - val_loss: 4.7607 - val_mse: 4.7607\n",
      "wandb: Agent Finished Run: xhp1njnh \n",
      "\n",
      "wandb: Agent Starting Run: 3j6wn9h4 with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 20\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: 3j6wn9h4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/3j6wn9h4\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/3j6wn9h4</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 3j6wn9h4 \n",
      "\n",
      "wandb: Agent Starting Run: 5ojou1ma with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 40\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: 5ojou1ma\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/5ojou1ma\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/5ojou1ma</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 51ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 5ojou1ma \n",
      "\n",
      "wandb: Agent Starting Run: p07ibacu with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 40\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: p07ibacu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/p07ibacu\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/p07ibacu</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 20.0847 - mse: 20.0847 - val_loss: 13.4710 - val_mse: 13.4710\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 19.4793 - mse: 19.4793 - val_loss: 11.6506 - val_mse: 11.6506\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 17.3180 - mse: 17.3180 - val_loss: 8.2404 - val_mse: 8.2404\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 14.0222 - mse: 14.0222 - val_loss: 6.0519 - val_mse: 6.0519\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 11.5534 - mse: 11.5534 - val_loss: 5.0546 - val_mse: 5.0546\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.3912 - mse: 10.3912 - val_loss: 4.7566 - val_mse: 4.7566\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0781 - mse: 10.0781 - val_loss: 4.8278 - val_mse: 4.8278\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1769 - mse: 10.1769 - val_loss: 4.9502 - val_mse: 4.9502\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1860 - mse: 10.1860 - val_loss: 4.9305 - val_mse: 4.9305\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1679 - mse: 10.1679 - val_loss: 4.9064 - val_mse: 4.9064\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1724 - mse: 10.1724 - val_loss: 4.8617 - val_mse: 4.8617\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1312 - mse: 10.1312 - val_loss: 4.8248 - val_mse: 4.8248\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0995 - mse: 10.0995 - val_loss: 4.8093 - val_mse: 4.8093\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1074 - mse: 10.1074 - val_loss: 4.7917 - val_mse: 4.7917\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0889 - mse: 10.0889 - val_loss: 4.7841 - val_mse: 4.7841\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0860 - mse: 10.0860 - val_loss: 4.7808 - val_mse: 4.7808\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0879 - mse: 10.0879 - val_loss: 4.7701 - val_mse: 4.7701\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0824 - mse: 10.0824 - val_loss: 4.7699 - val_mse: 4.7699\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0835 - mse: 10.0835 - val_loss: 4.7670 - val_mse: 4.7670\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0942 - mse: 10.0942 - val_loss: 4.7602 - val_mse: 4.7602\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0927 - mse: 10.0927 - val_loss: 4.7640 - val_mse: 4.7640\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0865 - mse: 10.0865 - val_loss: 4.7634 - val_mse: 4.7634\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0848 - mse: 10.0848 - val_loss: 4.7612 - val_mse: 4.7612\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0852 - mse: 10.0852 - val_loss: 4.7642 - val_mse: 4.7642\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0818 - mse: 10.0818 - val_loss: 4.7702 - val_mse: 4.7702\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0824 - mse: 10.0824 - val_loss: 4.7796 - val_mse: 4.7796\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0752 - mse: 10.0752 - val_loss: 4.8108 - val_mse: 4.8108\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1060 - mse: 10.1060 - val_loss: 4.8483 - val_mse: 4.8483\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1265 - mse: 10.1265 - val_loss: 4.8459 - val_mse: 4.8459\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1019 - mse: 10.1019 - val_loss: 4.8074 - val_mse: 4.8074\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1101 - mse: 10.1101 - val_loss: 4.7723 - val_mse: 4.7723\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0851 - mse: 10.0851 - val_loss: 4.7603 - val_mse: 4.7603\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0864 - mse: 10.0864 - val_loss: 4.7574 - val_mse: 4.7574\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0902 - mse: 10.0902 - val_loss: 4.7587 - val_mse: 4.7587\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0906 - mse: 10.0906 - val_loss: 4.7644 - val_mse: 4.7644\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0884 - mse: 10.0884 - val_loss: 4.7621 - val_mse: 4.7621\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0794 - mse: 10.0794 - val_loss: 4.7537 - val_mse: 4.7537\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0858 - mse: 10.0858 - val_loss: 4.7515 - val_mse: 4.7515\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0932 - mse: 10.0932 - val_loss: 4.7560 - val_mse: 4.7560\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1395 - mse: 10.1395 - val_loss: 4.7674 - val_mse: 4.7674\n",
      "wandb: Agent Finished Run: p07ibacu \n",
      "\n",
      "wandb: Agent Starting Run: yk8r0b0g with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 40\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: yk8r0b0g\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/yk8r0b0g\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/yk8r0b0g</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: yk8r0b0g \n",
      "\n",
      "wandb: Agent Starting Run: 8no6707p with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 10\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: 8no6707p\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/8no6707p\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/8no6707p</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 51ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 8no6707p \n",
      "\n",
      "wandb: Agent Starting Run: 0yqnvp3f with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 10\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: 0yqnvp3f\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/0yqnvp3f\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/0yqnvp3f</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 20.1116 - mse: 20.1116 - val_loss: 13.6783 - val_mse: 13.6783\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 19.6792 - mse: 19.6792 - val_loss: 13.0070 - val_mse: 13.0070\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 18.7964 - mse: 18.7964 - val_loss: 10.9779 - val_mse: 10.9779\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 16.1035 - mse: 16.1035 - val_loss: 6.3573 - val_mse: 6.3573\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 11.3305 - mse: 11.3305 - val_loss: 4.7548 - val_mse: 4.7548\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0455 - mse: 10.0455 - val_loss: 5.0465 - val_mse: 5.0465\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2773 - mse: 10.2773 - val_loss: 5.1738 - val_mse: 5.1738\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.3401 - mse: 10.3401 - val_loss: 5.0721 - val_mse: 5.0721\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.2530 - mse: 10.2530 - val_loss: 4.8965 - val_mse: 4.8965\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1651 - mse: 10.1651 - val_loss: 4.7930 - val_mse: 4.7930\n",
      "wandb: Agent Finished Run: 0yqnvp3f \n",
      "\n",
      "wandb: Agent Starting Run: 3q9xzmm2 with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 10\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: 3q9xzmm2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/3q9xzmm2\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/3q9xzmm2</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 20.0389 - mse: 20.0389 - val_loss: 12.6912 - val_mse: 12.6912\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 18.0734 - mse: 18.0734 - val_loss: 6.5519 - val_mse: 6.5519\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 11.6421 - mse: 11.6421 - val_loss: 5.8941 - val_mse: 5.8941\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 11.0109 - mse: 11.0109 - val_loss: 5.6677 - val_mse: 5.6677\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.5246 - mse: 10.5246 - val_loss: 4.8639 - val_mse: 4.8639\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0583 - mse: 10.0583 - val_loss: 4.7871 - val_mse: 4.7871\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1929 - mse: 10.1929 - val_loss: 4.8360 - val_mse: 4.8360\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.2753 - mse: 10.2753 - val_loss: 4.8452 - val_mse: 4.8452\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.2760 - mse: 10.2760 - val_loss: 4.7888 - val_mse: 4.7888\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1696 - mse: 10.1696 - val_loss: 4.7515 - val_mse: 4.7515\n",
      "wandb: Agent Finished Run: 3q9xzmm2 \n",
      "\n",
      "wandb: Agent Starting Run: ui9m1xer with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 20\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: ui9m1xer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/ui9m1xer\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/ui9m1xer</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: ui9m1xer \n",
      "\n",
      "wandb: Agent Starting Run: hd3jyotp with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 20\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: hd3jyotp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/hd3jyotp\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/hd3jyotp</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 20.1116 - mse: 20.1116 - val_loss: 13.6783 - val_mse: 13.6783\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 19.6792 - mse: 19.6792 - val_loss: 13.0070 - val_mse: 13.0070\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 18.7964 - mse: 18.7964 - val_loss: 10.9779 - val_mse: 10.9779\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 16.1035 - mse: 16.1035 - val_loss: 6.3573 - val_mse: 6.3573\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 11.3305 - mse: 11.3305 - val_loss: 4.7548 - val_mse: 4.7548\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0455 - mse: 10.0455 - val_loss: 5.0465 - val_mse: 5.0465\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.2773 - mse: 10.2773 - val_loss: 5.1738 - val_mse: 5.1738\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.3401 - mse: 10.3401 - val_loss: 5.0721 - val_mse: 5.0721\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2530 - mse: 10.2530 - val_loss: 4.8965 - val_mse: 4.8965\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1651 - mse: 10.1651 - val_loss: 4.7930 - val_mse: 4.7930\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0989 - mse: 10.0989 - val_loss: 4.7655 - val_mse: 4.7655\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0951 - mse: 10.0951 - val_loss: 4.7712 - val_mse: 4.7712\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0889 - mse: 10.0889 - val_loss: 4.7718 - val_mse: 4.7718\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0821 - mse: 10.0821 - val_loss: 4.7771 - val_mse: 4.7771\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0835 - mse: 10.0835 - val_loss: 4.7846 - val_mse: 4.7846\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0821 - mse: 10.0821 - val_loss: 4.8016 - val_mse: 4.8016\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0892 - mse: 10.0892 - val_loss: 4.8284 - val_mse: 4.8284\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1013 - mse: 10.1013 - val_loss: 4.8463 - val_mse: 4.8463\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1302 - mse: 10.1302 - val_loss: 4.8532 - val_mse: 4.8532\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1075 - mse: 10.1075 - val_loss: 4.8169 - val_mse: 4.8169\n",
      "wandb: Agent Finished Run: hd3jyotp \n",
      "\n",
      "wandb: Agent Starting Run: lswcrshy with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 20\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: lswcrshy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/lswcrshy\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/lswcrshy</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 20.0389 - mse: 20.0389 - val_loss: 12.6912 - val_mse: 12.6912\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 18.0734 - mse: 18.0734 - val_loss: 6.5519 - val_mse: 6.5519\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 11.6421 - mse: 11.6421 - val_loss: 5.8941 - val_mse: 5.8941\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 11.0109 - mse: 11.0109 - val_loss: 5.6677 - val_mse: 5.6677\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.5246 - mse: 10.5246 - val_loss: 4.8639 - val_mse: 4.8639\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0583 - mse: 10.0583 - val_loss: 4.7871 - val_mse: 4.7871\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1929 - mse: 10.1929 - val_loss: 4.8360 - val_mse: 4.8360\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.2753 - mse: 10.2753 - val_loss: 4.8452 - val_mse: 4.8452\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.2760 - mse: 10.2760 - val_loss: 4.7888 - val_mse: 4.7888\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1696 - mse: 10.1696 - val_loss: 4.7515 - val_mse: 4.7515\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0975 - mse: 10.0975 - val_loss: 4.8007 - val_mse: 4.8007\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1440 - mse: 10.1440 - val_loss: 4.8477 - val_mse: 4.8477\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1418 - mse: 10.1418 - val_loss: 4.7834 - val_mse: 4.7834\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0909 - mse: 10.0909 - val_loss: 4.7576 - val_mse: 4.7576\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1223 - mse: 10.1223 - val_loss: 4.7520 - val_mse: 4.7520\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1194 - mse: 10.1194 - val_loss: 4.7519 - val_mse: 4.7519\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0924 - mse: 10.0924 - val_loss: 4.7565 - val_mse: 4.7565\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1095 - mse: 10.1095 - val_loss: 4.7852 - val_mse: 4.7852\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1068 - mse: 10.1068 - val_loss: 4.8187 - val_mse: 4.8187\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1571 - mse: 10.1571 - val_loss: 4.8893 - val_mse: 4.8893\n",
      "wandb: Agent Finished Run: lswcrshy \n",
      "\n",
      "wandb: Agent Starting Run: x4vqxz56 with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 40\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: x4vqxz56\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/x4vqxz56\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/x4vqxz56</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: x4vqxz56 \n",
      "\n",
      "wandb: Agent Starting Run: tegeswn9 with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 40\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: tegeswn9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/tegeswn9\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/tegeswn9</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 20.1308 - mse: 20.1308 - val_loss: 13.7419 - val_mse: 13.7419\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 19.7572 - mse: 19.7572 - val_loss: 13.1198 - val_mse: 13.1198\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 18.9544 - mse: 18.9544 - val_loss: 11.3152 - val_mse: 11.3152\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 16.5397 - mse: 16.5397 - val_loss: 6.8494 - val_mse: 6.8494\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 11.7485 - mse: 11.7485 - val_loss: 4.7562 - val_mse: 4.7562\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0475 - mse: 10.0475 - val_loss: 5.0055 - val_mse: 5.0055\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2501 - mse: 10.2501 - val_loss: 5.1650 - val_mse: 5.1650\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.3389 - mse: 10.3389 - val_loss: 5.0808 - val_mse: 5.0808\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.2591 - mse: 10.2591 - val_loss: 4.9045 - val_mse: 4.9045\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1696 - mse: 10.1696 - val_loss: 4.7960 - val_mse: 4.7960\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1000 - mse: 10.1000 - val_loss: 4.7667 - val_mse: 4.7667\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0953 - mse: 10.0953 - val_loss: 4.7726 - val_mse: 4.7726\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0892 - mse: 10.0892 - val_loss: 4.7727 - val_mse: 4.7727\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0822 - mse: 10.0822 - val_loss: 4.7777 - val_mse: 4.7777\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0837 - mse: 10.0837 - val_loss: 4.7849 - val_mse: 4.7849\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0823 - mse: 10.0823 - val_loss: 4.8017 - val_mse: 4.8017\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0892 - mse: 10.0892 - val_loss: 4.8286 - val_mse: 4.8286\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1014 - mse: 10.1014 - val_loss: 4.8464 - val_mse: 4.8464\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1303 - mse: 10.1303 - val_loss: 4.8530 - val_mse: 4.8530\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1071 - mse: 10.1071 - val_loss: 4.8161 - val_mse: 4.8161\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0742 - mse: 10.0742 - val_loss: 4.7746 - val_mse: 4.7746\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0632 - mse: 10.0632 - val_loss: 4.7529 - val_mse: 4.7529\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0741 - mse: 10.0741 - val_loss: 4.7567 - val_mse: 4.7567\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1612 - mse: 10.1612 - val_loss: 4.7798 - val_mse: 4.7798\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1777 - mse: 10.1777 - val_loss: 4.7835 - val_mse: 4.7835\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1834 - mse: 10.1834 - val_loss: 4.7786 - val_mse: 4.7786\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1610 - mse: 10.1610 - val_loss: 4.7607 - val_mse: 4.7607\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1224 - mse: 10.1224 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0858 - mse: 10.0858 - val_loss: 4.7637 - val_mse: 4.7637\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0666 - mse: 10.0666 - val_loss: 4.8058 - val_mse: 4.8058\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1254 - mse: 10.1254 - val_loss: 4.8556 - val_mse: 4.8556\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1168 - mse: 10.1168 - val_loss: 4.8408 - val_mse: 4.8408\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1052 - mse: 10.1052 - val_loss: 4.8211 - val_mse: 4.8211\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1086 - mse: 10.1086 - val_loss: 4.7937 - val_mse: 4.7937\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0866 - mse: 10.0866 - val_loss: 4.7798 - val_mse: 4.7798\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0848 - mse: 10.0848 - val_loss: 4.7587 - val_mse: 4.7587\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0854 - mse: 10.0854 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1006 - mse: 10.1006 - val_loss: 4.7525 - val_mse: 4.7525\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1355 - mse: 10.1355 - val_loss: 4.7547 - val_mse: 4.7547\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1226 - mse: 10.1226 - val_loss: 4.7531 - val_mse: 4.7531\n",
      "wandb: Agent Finished Run: tegeswn9 \n",
      "\n",
      "wandb: Agent Starting Run: nlg0bs4v with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 40\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: nlg0bs4v\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/nlg0bs4v\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/nlg0bs4v</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 20.0827 - mse: 20.0827 - val_loss: 13.0723 - val_mse: 13.0723\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 18.6732 - mse: 18.6732 - val_loss: 8.0545 - val_mse: 8.0545\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 12.8864 - mse: 12.8864 - val_loss: 5.4004 - val_mse: 5.4004\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.7218 - mse: 10.7218 - val_loss: 5.7724 - val_mse: 5.7724\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.6514 - mse: 10.6514 - val_loss: 4.9717 - val_mse: 4.9717\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0922 - mse: 10.0922 - val_loss: 4.7610 - val_mse: 4.7610\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1327 - mse: 10.1327 - val_loss: 4.8289 - val_mse: 4.8289\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.2717 - mse: 10.2717 - val_loss: 4.8700 - val_mse: 4.8700\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.3092 - mse: 10.3092 - val_loss: 4.8107 - val_mse: 4.8107\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1998 - mse: 10.1998 - val_loss: 4.7540 - val_mse: 4.7540\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1057 - mse: 10.1057 - val_loss: 4.7918 - val_mse: 4.7918\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1440 - mse: 10.1440 - val_loss: 4.8464 - val_mse: 4.8464\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1381 - mse: 10.1381 - val_loss: 4.7880 - val_mse: 4.7880\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0915 - mse: 10.0915 - val_loss: 4.7611 - val_mse: 4.7611\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1183 - mse: 10.1183 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1131 - mse: 10.1131 - val_loss: 4.7527 - val_mse: 4.7527\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0914 - mse: 10.0914 - val_loss: 4.7572 - val_mse: 4.7572\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1075 - mse: 10.1075 - val_loss: 4.7838 - val_mse: 4.7838\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1054 - mse: 10.1054 - val_loss: 4.8140 - val_mse: 4.8140\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1523 - mse: 10.1523 - val_loss: 4.8803 - val_mse: 4.8803\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1412 - mse: 10.1412 - val_loss: 4.8892 - val_mse: 4.8892\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1353 - mse: 10.1353 - val_loss: 4.8588 - val_mse: 4.8588\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1097 - mse: 10.1097 - val_loss: 4.8091 - val_mse: 4.8091\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0883 - mse: 10.0883 - val_loss: 4.7551 - val_mse: 4.7551\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0746 - mse: 10.0746 - val_loss: 4.7565 - val_mse: 4.7565\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1626 - mse: 10.1626 - val_loss: 4.7707 - val_mse: 4.7707\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1512 - mse: 10.1512 - val_loss: 4.7566 - val_mse: 4.7566\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1097 - mse: 10.1097 - val_loss: 4.7522 - val_mse: 4.7522\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0719 - mse: 10.0719 - val_loss: 4.7762 - val_mse: 4.7762\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 15ms/step - loss: 10.0926 - mse: 10.0926 - val_loss: 4.8280 - val_mse: 4.8280\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 10.1629 - mse: 10.1629 - val_loss: 4.8714 - val_mse: 4.8714\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 10.1253 - mse: 10.1253 - val_loss: 4.8222 - val_mse: 4.8222\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 10.1011 - mse: 10.1011 - val_loss: 4.7871 - val_mse: 4.7871\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 10.1090 - mse: 10.1090 - val_loss: 4.7589 - val_mse: 4.7589\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0936 - mse: 10.0936 - val_loss: 4.7525 - val_mse: 4.7525\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1045 - mse: 10.1045 - val_loss: 4.7520 - val_mse: 4.7520\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1098 - mse: 10.1098 - val_loss: 4.7517 - val_mse: 4.7517\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0992 - mse: 10.0992 - val_loss: 4.7534 - val_mse: 4.7534\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1172 - mse: 10.1172 - val_loss: 4.7716 - val_mse: 4.7716\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0875 - mse: 10.0875 - val_loss: 4.7747 - val_mse: 4.7747\n",
      "wandb: Agent Finished Run: nlg0bs4v \n",
      "\n",
      "wandb: Agent Starting Run: 8syiq1os with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 10\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: 8syiq1os\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/8syiq1os\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/8syiq1os</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 5s 67ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 8syiq1os \n",
      "\n",
      "wandb: Agent Starting Run: lvmp9pj8 with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 10\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: lvmp9pj8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/lvmp9pj8\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/lvmp9pj8</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 20.1264 - mse: 20.1264 - val_loss: 13.6996 - val_mse: 13.6996\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 19.6662 - mse: 19.6662 - val_loss: 12.7973 - val_mse: 12.7973\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 18.5726 - mse: 18.5726 - val_loss: 10.1187 - val_mse: 10.1187\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 15.3011 - mse: 15.3011 - val_loss: 5.6486 - val_mse: 5.6486\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.8336 - mse: 10.8336 - val_loss: 4.7539 - val_mse: 4.7539\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0513 - mse: 10.0513 - val_loss: 5.0054 - val_mse: 5.0054\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2531 - mse: 10.2531 - val_loss: 5.1572 - val_mse: 5.1572\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.3373 - mse: 10.3373 - val_loss: 5.0909 - val_mse: 5.0909\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.2676 - mse: 10.2676 - val_loss: 4.9248 - val_mse: 4.9248\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1792 - mse: 10.1792 - val_loss: 4.8089 - val_mse: 4.8089\n",
      "wandb: Agent Finished Run: lvmp9pj8 \n",
      "\n",
      "wandb: Agent Starting Run: e9t16up7 with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 10\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: e9t16up7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/e9t16up7\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/e9t16up7</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 20.0817 - mse: 20.0817 - val_loss: 13.0635 - val_mse: 13.0635\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 18.6703 - mse: 18.6703 - val_loss: 8.0176 - val_mse: 8.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 12.8820 - mse: 12.8820 - val_loss: 5.4071 - val_mse: 5.4071\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.7273 - mse: 10.7273 - val_loss: 5.7666 - val_mse: 5.7666\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.6451 - mse: 10.6451 - val_loss: 4.9653 - val_mse: 4.9653\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0892 - mse: 10.0892 - val_loss: 4.7634 - val_mse: 4.7634\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1399 - mse: 10.1399 - val_loss: 4.8304 - val_mse: 4.8304\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.2735 - mse: 10.2735 - val_loss: 4.8675 - val_mse: 4.8675\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.3058 - mse: 10.3058 - val_loss: 4.8073 - val_mse: 4.8073\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1950 - mse: 10.1950 - val_loss: 4.7533 - val_mse: 4.7533\n",
      "wandb: Agent Finished Run: e9t16up7 \n",
      "\n",
      "wandb: Agent Starting Run: 9ubtzbbe with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 20\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: 9ubtzbbe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/9ubtzbbe\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/9ubtzbbe</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 9ubtzbbe \n",
      "\n",
      "wandb: Agent Starting Run: 9yuq3oz6 with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 20\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: 9yuq3oz6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/9yuq3oz6\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/9yuq3oz6</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 20.1053 - mse: 20.1053 - val_loss: 13.6188 - val_mse: 13.6188\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 19.5362 - mse: 19.5362 - val_loss: 12.4832 - val_mse: 12.4832\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 18.1868 - mse: 18.1868 - val_loss: 9.2291 - val_mse: 9.2291\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 14.3272 - mse: 14.3272 - val_loss: 5.2287 - val_mse: 5.2287\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4683 - mse: 10.4683 - val_loss: 4.7755 - val_mse: 4.7755\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0700 - mse: 10.0700 - val_loss: 5.0469 - val_mse: 5.0469\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.2782 - mse: 10.2782 - val_loss: 5.1546 - val_mse: 5.1546\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.3282 - mse: 10.3282 - val_loss: 5.0684 - val_mse: 5.0684\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.2515 - mse: 10.2515 - val_loss: 4.9071 - val_mse: 4.9071\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1689 - mse: 10.1689 - val_loss: 4.8018 - val_mse: 4.8018\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1014 - mse: 10.1014 - val_loss: 4.7698 - val_mse: 4.7698\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0949 - mse: 10.0949 - val_loss: 4.7751 - val_mse: 4.7751\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0904 - mse: 10.0904 - val_loss: 4.7739 - val_mse: 4.7739\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0823 - mse: 10.0823 - val_loss: 4.7778 - val_mse: 4.7778\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0838 - mse: 10.0838 - val_loss: 4.7841 - val_mse: 4.7841\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0821 - mse: 10.0821 - val_loss: 4.8005 - val_mse: 4.8005\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0886 - mse: 10.0886 - val_loss: 4.8275 - val_mse: 4.8275\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1007 - mse: 10.1007 - val_loss: 4.8456 - val_mse: 4.8456\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1301 - mse: 10.1301 - val_loss: 4.8526 - val_mse: 4.8526\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1070 - mse: 10.1070 - val_loss: 4.8153 - val_mse: 4.8153\n",
      "wandb: Agent Finished Run: 9yuq3oz6 \n",
      "\n",
      "wandb: Agent Starting Run: f5o8whg1 with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 20\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: f5o8whg1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/f5o8whg1\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/f5o8whg1</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 57ms/step - loss: 20.0438 - mse: 20.0438 - val_loss: 12.7933 - val_mse: 12.7933\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 18.2975 - mse: 18.2975 - val_loss: 7.1191 - val_mse: 7.1191\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 12.1297 - mse: 12.1297 - val_loss: 5.6916 - val_mse: 5.6916\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.9015 - mse: 10.9015 - val_loss: 5.7188 - val_mse: 5.7188\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.5797 - mse: 10.5797 - val_loss: 4.9010 - val_mse: 4.9010\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0674 - mse: 10.0674 - val_loss: 4.7779 - val_mse: 4.7779\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1745 - mse: 10.1745 - val_loss: 4.8356 - val_mse: 4.8356\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.2770 - mse: 10.2770 - val_loss: 4.8538 - val_mse: 4.8538\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.2875 - mse: 10.2875 - val_loss: 4.7948 - val_mse: 4.7948\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1778 - mse: 10.1778 - val_loss: 4.7518 - val_mse: 4.7518\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0991 - mse: 10.0991 - val_loss: 4.7991 - val_mse: 4.7991\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1443 - mse: 10.1443 - val_loss: 4.8483 - val_mse: 4.8483\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1418 - mse: 10.1418 - val_loss: 4.7842 - val_mse: 4.7842\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0910 - mse: 10.0910 - val_loss: 4.7582 - val_mse: 4.7582\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1215 - mse: 10.1215 - val_loss: 4.7518 - val_mse: 4.7518\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1183 - mse: 10.1183 - val_loss: 4.7520 - val_mse: 4.7520\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0922 - mse: 10.0922 - val_loss: 4.7566 - val_mse: 4.7566\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1092 - mse: 10.1092 - val_loss: 4.7850 - val_mse: 4.7850\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1064 - mse: 10.1064 - val_loss: 4.8180 - val_mse: 4.8180\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1566 - mse: 10.1566 - val_loss: 4.8881 - val_mse: 4.8881\n",
      "wandb: Agent Finished Run: f5o8whg1 \n",
      "\n",
      "wandb: Agent Starting Run: f7xp1vup with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 40\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: f7xp1vup\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/f7xp1vup\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/f7xp1vup</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: f7xp1vup \n",
      "\n",
      "wandb: Agent Starting Run: bihu3it7 with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 40\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: bihu3it7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/bihu3it7\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/bihu3it7</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 20.1053 - mse: 20.1053 - val_loss: 13.6188 - val_mse: 13.6188\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 19.5362 - mse: 19.5362 - val_loss: 12.4832 - val_mse: 12.4832\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 18.1868 - mse: 18.1868 - val_loss: 9.2291 - val_mse: 9.2291\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 14.3272 - mse: 14.3272 - val_loss: 5.2294 - val_mse: 5.2294\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.4689 - mse: 10.4689 - val_loss: 4.7755 - val_mse: 4.7755\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0698 - mse: 10.0698 - val_loss: 5.0458 - val_mse: 5.0458\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.2774 - mse: 10.2774 - val_loss: 5.1540 - val_mse: 5.1540\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.3280 - mse: 10.3280 - val_loss: 5.0684 - val_mse: 5.0684\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.2516 - mse: 10.2516 - val_loss: 4.9077 - val_mse: 4.9077\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1692 - mse: 10.1692 - val_loss: 4.8021 - val_mse: 4.8021\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1015 - mse: 10.1015 - val_loss: 4.7699 - val_mse: 4.7699\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0949 - mse: 10.0949 - val_loss: 4.7752 - val_mse: 4.7752\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0904 - mse: 10.0904 - val_loss: 4.7739 - val_mse: 4.7739\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0823 - mse: 10.0823 - val_loss: 4.7777 - val_mse: 4.7777\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0838 - mse: 10.0838 - val_loss: 4.7840 - val_mse: 4.7840\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0821 - mse: 10.0821 - val_loss: 4.8004 - val_mse: 4.8004\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0886 - mse: 10.0886 - val_loss: 4.8273 - val_mse: 4.8273\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1006 - mse: 10.1006 - val_loss: 4.8455 - val_mse: 4.8455\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1301 - mse: 10.1301 - val_loss: 4.8526 - val_mse: 4.8526\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1070 - mse: 10.1070 - val_loss: 4.8153 - val_mse: 4.8153\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0734 - mse: 10.0734 - val_loss: 4.7733 - val_mse: 4.7733\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0628 - mse: 10.0628 - val_loss: 4.7524 - val_mse: 4.7524\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0757 - mse: 10.0757 - val_loss: 4.7586 - val_mse: 4.7586\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1690 - mse: 10.1690 - val_loss: 4.7849 - val_mse: 4.7849\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1872 - mse: 10.1872 - val_loss: 4.7864 - val_mse: 4.7864\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1882 - mse: 10.1882 - val_loss: 4.7788 - val_mse: 4.7788\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1592 - mse: 10.1592 - val_loss: 4.7590 - val_mse: 4.7590\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1176 - mse: 10.1176 - val_loss: 4.7516 - val_mse: 4.7516\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0821 - mse: 10.0821 - val_loss: 4.7686 - val_mse: 4.7686\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0671 - mse: 10.0671 - val_loss: 4.8161 - val_mse: 4.8161\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1311 - mse: 10.1311 - val_loss: 4.8678 - val_mse: 4.8678\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1236 - mse: 10.1236 - val_loss: 4.8472 - val_mse: 4.8472\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1073 - mse: 10.1073 - val_loss: 4.8224 - val_mse: 4.8224\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1103 - mse: 10.1103 - val_loss: 4.7918 - val_mse: 4.7918\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0856 - mse: 10.0856 - val_loss: 4.7769 - val_mse: 4.7769\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0847 - mse: 10.0847 - val_loss: 4.7567 - val_mse: 4.7567\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0873 - mse: 10.0873 - val_loss: 4.7515 - val_mse: 4.7515\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1050 - mse: 10.1050 - val_loss: 4.7535 - val_mse: 4.7535\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1399 - mse: 10.1399 - val_loss: 4.7560 - val_mse: 4.7560\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1259 - mse: 10.1259 - val_loss: 4.7537 - val_mse: 4.7537\n",
      "wandb: Agent Finished Run: bihu3it7 \n",
      "\n",
      "wandb: Agent Starting Run: wmh42o2l with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 40\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: wmh42o2l\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/wmh42o2l\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/wmh42o2l</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 20.0373 - mse: 20.0373 - val_loss: 12.6778 - val_mse: 12.6778\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 18.0669 - mse: 18.0669 - val_loss: 6.5097 - val_mse: 6.5097\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 11.6369 - mse: 11.6369 - val_loss: 5.9086 - val_mse: 5.9086\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 11.0187 - mse: 11.0187 - val_loss: 5.6623 - val_mse: 5.6623\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.5202 - mse: 10.5202 - val_loss: 4.8616 - val_mse: 4.8616\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0582 - mse: 10.0582 - val_loss: 4.7899 - val_mse: 4.7899\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1983 - mse: 10.1983 - val_loss: 4.8384 - val_mse: 4.8384\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.2786 - mse: 10.2786 - val_loss: 4.8457 - val_mse: 4.8457\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.2766 - mse: 10.2766 - val_loss: 4.7880 - val_mse: 4.7880\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1684 - mse: 10.1684 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0970 - mse: 10.0970 - val_loss: 4.8022 - val_mse: 4.8022\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1447 - mse: 10.1447 - val_loss: 4.8488 - val_mse: 4.8488\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1428 - mse: 10.1428 - val_loss: 4.7830 - val_mse: 4.7830\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0909 - mse: 10.0909 - val_loss: 4.7572 - val_mse: 4.7572\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1230 - mse: 10.1230 - val_loss: 4.7521 - val_mse: 4.7521\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1204 - mse: 10.1204 - val_loss: 4.7518 - val_mse: 4.7518\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0926 - mse: 10.0926 - val_loss: 4.7563 - val_mse: 4.7563\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1099 - mse: 10.1099 - val_loss: 4.7854 - val_mse: 4.7854\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1070 - mse: 10.1070 - val_loss: 4.8196 - val_mse: 4.8196\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1581 - mse: 10.1581 - val_loss: 4.8911 - val_mse: 4.8911\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1485 - mse: 10.1485 - val_loss: 4.8982 - val_mse: 4.8982\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1404 - mse: 10.1404 - val_loss: 4.8625 - val_mse: 4.8625\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1098 - mse: 10.1098 - val_loss: 4.8082 - val_mse: 4.8082\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0880 - mse: 10.0880 - val_loss: 4.7540 - val_mse: 4.7540\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0765 - mse: 10.0765 - val_loss: 4.7586 - val_mse: 4.7586\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1690 - mse: 10.1690 - val_loss: 4.7742 - val_mse: 4.7742\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1571 - mse: 10.1571 - val_loss: 4.7577 - val_mse: 4.7577\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1112 - mse: 10.1112 - val_loss: 4.7521 - val_mse: 4.7521\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0709 - mse: 10.0709 - val_loss: 4.7777 - val_mse: 4.7777\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0936 - mse: 10.0936 - val_loss: 4.8329 - val_mse: 4.8329\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1674 - mse: 10.1674 - val_loss: 4.8779 - val_mse: 4.8779\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1288 - mse: 10.1288 - val_loss: 4.8245 - val_mse: 4.8245\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1020 - mse: 10.1020 - val_loss: 4.7869 - val_mse: 4.7869\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1100 - mse: 10.1100 - val_loss: 4.7581 - val_mse: 4.7581\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0944 - mse: 10.0944 - val_loss: 4.7521 - val_mse: 4.7521\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1063 - mse: 10.1063 - val_loss: 4.7524 - val_mse: 4.7524\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1118 - mse: 10.1118 - val_loss: 4.7519 - val_mse: 4.7519\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1002 - mse: 10.1002 - val_loss: 4.7532 - val_mse: 4.7532\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1184 - mse: 10.1184 - val_loss: 4.7721 - val_mse: 4.7721\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0876 - mse: 10.0876 - val_loss: 4.7755 - val_mse: 4.7755\n",
      "wandb: Agent Finished Run: wmh42o2l \n",
      "\n",
      "wandb: Agent Starting Run: 0rbma58c with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 10\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: 0rbma58c\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/0rbma58c\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/0rbma58c</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 0rbma58c \n",
      "\n",
      "wandb: Agent Starting Run: uttxkoho with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 10\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: uttxkoho\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/uttxkoho\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/uttxkoho</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 5s 63ms/step - loss: 20.1031 - mse: 20.1031 - val_loss: 13.6389 - val_mse: 13.6389\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 19.5598 - mse: 19.5598 - val_loss: 12.4765 - val_mse: 12.4765\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 18.2106 - mse: 18.2106 - val_loss: 9.1433 - val_mse: 9.1433\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 14.2849 - mse: 14.2849 - val_loss: 5.2418 - val_mse: 5.2418\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.4939 - mse: 10.4939 - val_loss: 4.7738 - val_mse: 4.7738\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0708 - mse: 10.0708 - val_loss: 5.0632 - val_mse: 5.0632\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2901 - mse: 10.2901 - val_loss: 5.1723 - val_mse: 5.1723\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.3394 - mse: 10.3394 - val_loss: 5.0733 - val_mse: 5.0733\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.2540 - mse: 10.2540 - val_loss: 4.9017 - val_mse: 4.9017\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1674 - mse: 10.1674 - val_loss: 4.7956 - val_mse: 4.7956\n",
      "wandb: Agent Finished Run: uttxkoho \n",
      "\n",
      "wandb: Agent Starting Run: c9et4jqs with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 10\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: c9et4jqs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/c9et4jqs\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/c9et4jqs</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 6s 76ms/step - loss: 20.0372 - mse: 20.0372 - val_loss: 12.6542 - val_mse: 12.6542\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 18.0345 - mse: 18.0345 - val_loss: 6.5574 - val_mse: 6.5574\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 11.6856 - mse: 11.6856 - val_loss: 5.9026 - val_mse: 5.9026\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 11.0219 - mse: 11.0219 - val_loss: 5.6890 - val_mse: 5.6890\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.5406 - mse: 10.5406 - val_loss: 4.8720 - val_mse: 4.8720\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0597 - mse: 10.0597 - val_loss: 4.7869 - val_mse: 4.7869\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.1926 - mse: 10.1926 - val_loss: 4.8388 - val_mse: 4.8388\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.2797 - mse: 10.2797 - val_loss: 4.8490 - val_mse: 4.8490\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.2810 - mse: 10.2810 - val_loss: 4.7907 - val_mse: 4.7907\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1722 - mse: 10.1722 - val_loss: 4.7516 - val_mse: 4.7516\n",
      "wandb: Agent Finished Run: c9et4jqs \n",
      "\n",
      "wandb: Agent Starting Run: vqryt7fu with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 20\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: vqryt7fu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/vqryt7fu\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/vqryt7fu</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: vqryt7fu \n",
      "\n",
      "wandb: Agent Starting Run: m1h26caa with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 20\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: m1h26caa\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/m1h26caa\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/m1h26caa</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 20.1000 - mse: 20.1000 - val_loss: 13.5836 - val_mse: 13.5836\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 19.4788 - mse: 19.4788 - val_loss: 12.2526 - val_mse: 12.2526\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 17.9375 - mse: 17.9375 - val_loss: 8.5708 - val_mse: 8.5708\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 13.6752 - mse: 13.6752 - val_loss: 5.0702 - val_mse: 5.0702\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.3435 - mse: 10.3435 - val_loss: 4.7970 - val_mse: 4.7970\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0887 - mse: 10.0887 - val_loss: 5.0904 - val_mse: 5.0904\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.3068 - mse: 10.3068 - val_loss: 5.1730 - val_mse: 5.1730\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.3361 - mse: 10.3361 - val_loss: 5.0635 - val_mse: 5.0635\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.2472 - mse: 10.2472 - val_loss: 4.8949 - val_mse: 4.8949\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1634 - mse: 10.1634 - val_loss: 4.7932 - val_mse: 4.7932\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0987 - mse: 10.0987 - val_loss: 4.7656 - val_mse: 4.7656\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0952 - mse: 10.0952 - val_loss: 4.7713 - val_mse: 4.7713\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0890 - mse: 10.0890 - val_loss: 4.7718 - val_mse: 4.7718\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0821 - mse: 10.0821 - val_loss: 4.7771 - val_mse: 4.7771\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0834 - mse: 10.0834 - val_loss: 4.7847 - val_mse: 4.7847\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0821 - mse: 10.0821 - val_loss: 4.8020 - val_mse: 4.8020\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0893 - mse: 10.0893 - val_loss: 4.8292 - val_mse: 4.8292\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1017 - mse: 10.1017 - val_loss: 4.8472 - val_mse: 4.8472\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1308 - mse: 10.1308 - val_loss: 4.8538 - val_mse: 4.8538\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1074 - mse: 10.1074 - val_loss: 4.8163 - val_mse: 4.8163\n",
      "wandb: Agent Finished Run: m1h26caa \n",
      "\n",
      "wandb: Agent Starting Run: uaei49dj with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 20\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: uaei49dj\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/uaei49dj\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/uaei49dj</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 20.0372 - mse: 20.0372 - val_loss: 12.6542 - val_mse: 12.6542\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 18.0345 - mse: 18.0345 - val_loss: 6.4855 - val_mse: 6.4855\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 11.6311 - mse: 11.6311 - val_loss: 5.9140 - val_mse: 5.9140\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 11.0253 - mse: 11.0253 - val_loss: 5.6784 - val_mse: 5.6784\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.5317 - mse: 10.5317 - val_loss: 4.8671 - val_mse: 4.8671\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0589 - mse: 10.0589 - val_loss: 4.7884 - val_mse: 4.7884\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1956 - mse: 10.1956 - val_loss: 4.8386 - val_mse: 4.8386\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.2792 - mse: 10.2792 - val_loss: 4.8473 - val_mse: 4.8473\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.2788 - mse: 10.2788 - val_loss: 4.7894 - val_mse: 4.7894\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1704 - mse: 10.1704 - val_loss: 4.7515 - val_mse: 4.7515\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0976 - mse: 10.0976 - val_loss: 4.8005 - val_mse: 4.8005\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1439 - mse: 10.1439 - val_loss: 4.8475 - val_mse: 4.8475\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1416 - mse: 10.1416 - val_loss: 4.7834 - val_mse: 4.7834\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0909 - mse: 10.0909 - val_loss: 4.7577 - val_mse: 4.7577\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1221 - mse: 10.1221 - val_loss: 4.7519 - val_mse: 4.7519\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1192 - mse: 10.1192 - val_loss: 4.7519 - val_mse: 4.7519\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0924 - mse: 10.0924 - val_loss: 4.7564 - val_mse: 4.7564\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1094 - mse: 10.1094 - val_loss: 4.7849 - val_mse: 4.7849\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1066 - mse: 10.1066 - val_loss: 4.8184 - val_mse: 4.8184\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1569 - mse: 10.1569 - val_loss: 4.8889 - val_mse: 4.8889\n",
      "wandb: Agent Finished Run: uaei49dj \n",
      "\n",
      "wandb: Agent Starting Run: 7e48790x with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 40\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: 7e48790x\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/7e48790x\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/7e48790x</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 7e48790x \n",
      "\n",
      "wandb: Agent Starting Run: 563xciag with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 40\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: 563xciag\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/563xciag\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/563xciag</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 20.1000 - mse: 20.1000 - val_loss: 13.5836 - val_mse: 13.5836\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 19.4788 - mse: 19.4788 - val_loss: 12.2526 - val_mse: 12.2526\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 17.9375 - mse: 17.9375 - val_loss: 8.5708 - val_mse: 8.5708\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 13.6752 - mse: 13.6752 - val_loss: 5.0702 - val_mse: 5.0702\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.3435 - mse: 10.3435 - val_loss: 4.7970 - val_mse: 4.7970\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0887 - mse: 10.0887 - val_loss: 5.0904 - val_mse: 5.0904\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.3068 - mse: 10.3068 - val_loss: 5.1730 - val_mse: 5.1730\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.3361 - mse: 10.3361 - val_loss: 5.0635 - val_mse: 5.0635\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2472 - mse: 10.2472 - val_loss: 4.8949 - val_mse: 4.8949\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1634 - mse: 10.1634 - val_loss: 4.7932 - val_mse: 4.7932\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0987 - mse: 10.0987 - val_loss: 4.7656 - val_mse: 4.7656\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0952 - mse: 10.0952 - val_loss: 4.7713 - val_mse: 4.7713\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 10.0890 - mse: 10.0890 - val_loss: 4.7718 - val_mse: 4.7718\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0821 - mse: 10.0821 - val_loss: 4.7771 - val_mse: 4.7771\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0834 - mse: 10.0834 - val_loss: 4.7847 - val_mse: 4.7847\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0821 - mse: 10.0821 - val_loss: 4.8020 - val_mse: 4.8020\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0893 - mse: 10.0893 - val_loss: 4.8292 - val_mse: 4.8292\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1017 - mse: 10.1017 - val_loss: 4.8472 - val_mse: 4.8472\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1308 - mse: 10.1308 - val_loss: 4.8538 - val_mse: 4.8538\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1074 - mse: 10.1074 - val_loss: 4.8163 - val_mse: 4.8163\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0738 - mse: 10.0738 - val_loss: 4.7742 - val_mse: 4.7742\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0630 - mse: 10.0630 - val_loss: 4.7527 - val_mse: 4.7527\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0745 - mse: 10.0745 - val_loss: 4.7573 - val_mse: 4.7573\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1637 - mse: 10.1637 - val_loss: 4.7815 - val_mse: 4.7815\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1809 - mse: 10.1809 - val_loss: 4.7844 - val_mse: 4.7844\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1849 - mse: 10.1849 - val_loss: 4.7786 - val_mse: 4.7786\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1604 - mse: 10.1604 - val_loss: 4.7602 - val_mse: 4.7602\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1210 - mse: 10.1210 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0850 - mse: 10.0850 - val_loss: 4.7645 - val_mse: 4.7645\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0668 - mse: 10.0668 - val_loss: 4.8072 - val_mse: 4.8072\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1262 - mse: 10.1262 - val_loss: 4.8569 - val_mse: 4.8569\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.1176 - mse: 10.1176 - val_loss: 4.8411 - val_mse: 4.8411\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1052 - mse: 10.1052 - val_loss: 4.8206 - val_mse: 4.8206\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1085 - mse: 10.1085 - val_loss: 4.7929 - val_mse: 4.7929\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0864 - mse: 10.0864 - val_loss: 4.7790 - val_mse: 4.7790\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0847 - mse: 10.0847 - val_loss: 4.7583 - val_mse: 4.7583\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0857 - mse: 10.0857 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1013 - mse: 10.1013 - val_loss: 4.7526 - val_mse: 4.7526\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1361 - mse: 10.1361 - val_loss: 4.7549 - val_mse: 4.7549\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1230 - mse: 10.1230 - val_loss: 4.7532 - val_mse: 4.7532\n",
      "wandb: Agent Finished Run: 563xciag \n",
      "\n",
      "wandb: Agent Starting Run: fpta231f with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 40\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 32\n",
      "wandb: Agent Started Run: fpta231f\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/fpta231f\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/fpta231f</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 20.0372 - mse: 20.0372 - val_loss: 12.6542 - val_mse: 12.6542\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 18.0471 - mse: 18.0471 - val_loss: 6.6476 - val_mse: 6.6476\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 11.7551 - mse: 11.7551 - val_loss: 5.8725 - val_mse: 5.8725\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 11.0052 - mse: 11.0052 - val_loss: 5.6959 - val_mse: 5.6959\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.5482 - mse: 10.5482 - val_loss: 4.8774 - val_mse: 4.8774\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0609 - mse: 10.0609 - val_loss: 4.7849 - val_mse: 4.7849\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1888 - mse: 10.1888 - val_loss: 4.8381 - val_mse: 4.8381\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.2792 - mse: 10.2792 - val_loss: 4.8501 - val_mse: 4.8501\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.2825 - mse: 10.2825 - val_loss: 4.7917 - val_mse: 4.7917\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1737 - mse: 10.1737 - val_loss: 4.7517 - val_mse: 4.7517\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0985 - mse: 10.0985 - val_loss: 4.7992 - val_mse: 4.7992\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1436 - mse: 10.1436 - val_loss: 4.8470 - val_mse: 4.8470\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1409 - mse: 10.1409 - val_loss: 4.7840 - val_mse: 4.7840\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0910 - mse: 10.0910 - val_loss: 4.7582 - val_mse: 4.7582\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1214 - mse: 10.1214 - val_loss: 4.7518 - val_mse: 4.7518\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1181 - mse: 10.1181 - val_loss: 4.7520 - val_mse: 4.7520\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0923 - mse: 10.0923 - val_loss: 4.7565 - val_mse: 4.7565\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1093 - mse: 10.1093 - val_loss: 4.7848 - val_mse: 4.7848\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1064 - mse: 10.1064 - val_loss: 4.8176 - val_mse: 4.8176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1561 - mse: 10.1561 - val_loss: 4.8874 - val_mse: 4.8874\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1460 - mse: 10.1460 - val_loss: 4.8953 - val_mse: 4.8953\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1387 - mse: 10.1387 - val_loss: 4.8613 - val_mse: 4.8613\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1099 - mse: 10.1099 - val_loss: 4.8086 - val_mse: 4.8086\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0881 - mse: 10.0881 - val_loss: 4.7544 - val_mse: 4.7544\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 14ms/step - loss: 10.0758 - mse: 10.0758 - val_loss: 4.7578 - val_mse: 4.7578\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1667 - mse: 10.1667 - val_loss: 4.7730 - val_mse: 4.7730\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1551 - mse: 10.1551 - val_loss: 4.7574 - val_mse: 4.7574\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1109 - mse: 10.1109 - val_loss: 4.7521 - val_mse: 4.7521\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0714 - mse: 10.0714 - val_loss: 4.7769 - val_mse: 4.7769\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0931 - mse: 10.0931 - val_loss: 4.8308 - val_mse: 4.8308\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1658 - mse: 10.1658 - val_loss: 4.8754 - val_mse: 4.8754\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1273 - mse: 10.1273 - val_loss: 4.8234 - val_mse: 4.8234\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1015 - mse: 10.1015 - val_loss: 4.7867 - val_mse: 4.7867\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.1096 - mse: 10.1096 - val_loss: 4.7583 - val_mse: 4.7583\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 13ms/step - loss: 10.0942 - mse: 10.0942 - val_loss: 4.7522 - val_mse: 4.7522\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 10.1058 - mse: 10.1058 - val_loss: 4.7522 - val_mse: 4.7522\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 10.1112 - mse: 10.1112 - val_loss: 4.7518 - val_mse: 4.7518\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0999 - mse: 10.0999 - val_loss: 4.7533 - val_mse: 4.7533\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 12ms/step - loss: 10.1181 - mse: 10.1181 - val_loss: 4.7720 - val_mse: 4.7720\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0876 - mse: 10.0876 - val_loss: 4.7753 - val_mse: 4.7753\n",
      "wandb: Agent Finished Run: fpta231f \n",
      "\n",
      "wandb: Agent Starting Run: bwoh29od with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 10\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: bwoh29od\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/bwoh29od\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/bwoh29od</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 47ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: bwoh29od \n",
      "\n",
      "wandb: Agent Starting Run: 7cj9zm2c with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 10\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: 7cj9zm2c\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/7cj9zm2c\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/7cj9zm2c</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 20.1596 - mse: 20.1596 - val_loss: 13.6220 - val_mse: 13.6220\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 19.7972 - mse: 19.7972 - val_loss: 12.7444 - val_mse: 12.7444\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 18.9811 - mse: 18.9811 - val_loss: 11.0144 - val_mse: 11.0144\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 17.4706 - mse: 17.4706 - val_loss: 8.6083 - val_mse: 8.6083\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 14.9280 - mse: 14.9280 - val_loss: 6.7968 - val_mse: 6.7968\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 12.5393 - mse: 12.5393 - val_loss: 5.7050 - val_mse: 5.7050\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 11.3054 - mse: 11.3054 - val_loss: 5.0802 - val_mse: 5.0802\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.5983 - mse: 10.5983 - val_loss: 4.8113 - val_mse: 4.8113\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1791 - mse: 10.1791 - val_loss: 4.7518 - val_mse: 4.7518\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0835 - mse: 10.0835 - val_loss: 4.8154 - val_mse: 4.8154\n",
      "wandb: Agent Finished Run: 7cj9zm2c \n",
      "\n",
      "wandb: Agent Starting Run: pof9gdcu with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 10\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: pof9gdcu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/pof9gdcu\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/pof9gdcu</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: pof9gdcu \n",
      "\n",
      "wandb: Agent Starting Run: 5jizrzqa with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 20\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: 5jizrzqa\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/5jizrzqa\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/5jizrzqa</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 5jizrzqa \n",
      "\n",
      "wandb: Agent Starting Run: unoidw71 with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 20\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: unoidw71\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/unoidw71\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/unoidw71</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 49ms/step - loss: 20.1596 - mse: 20.1596 - val_loss: 13.6220 - val_mse: 13.6220\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 19.7972 - mse: 19.7972 - val_loss: 12.7444 - val_mse: 12.7444\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 18.9811 - mse: 18.9811 - val_loss: 11.0144 - val_mse: 11.0144\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 17.4706 - mse: 17.4706 - val_loss: 8.6083 - val_mse: 8.6083\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 14.9280 - mse: 14.9280 - val_loss: 6.7968 - val_mse: 6.7968\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 12.5393 - mse: 12.5393 - val_loss: 5.7050 - val_mse: 5.7050\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 11.3054 - mse: 11.3054 - val_loss: 5.0802 - val_mse: 5.0802\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.5983 - mse: 10.5983 - val_loss: 4.8113 - val_mse: 4.8113\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1791 - mse: 10.1791 - val_loss: 4.7518 - val_mse: 4.7518\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0835 - mse: 10.0835 - val_loss: 4.8154 - val_mse: 4.8154\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1026 - mse: 10.1026 - val_loss: 4.9279 - val_mse: 4.9279\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1578 - mse: 10.1578 - val_loss: 5.0218 - val_mse: 5.0218\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2300 - mse: 10.2300 - val_loss: 5.0826 - val_mse: 5.0826\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2759 - mse: 10.2759 - val_loss: 5.0864 - val_mse: 5.0864\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.2784 - mse: 10.2784 - val_loss: 5.0459 - val_mse: 5.0459\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2397 - mse: 10.2397 - val_loss: 4.9691 - val_mse: 4.9691\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1998 - mse: 10.1998 - val_loss: 4.8919 - val_mse: 4.8919\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1371 - mse: 10.1371 - val_loss: 4.8403 - val_mse: 4.8403\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0994 - mse: 10.0994 - val_loss: 4.7957 - val_mse: 4.7957\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0951 - mse: 10.0951 - val_loss: 4.7680 - val_mse: 4.7680\n",
      "wandb: Agent Finished Run: unoidw71 \n",
      "\n",
      "wandb: Agent Starting Run: o5ak06sl with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 20\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: o5ak06sl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/o5ak06sl\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/o5ak06sl</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: o5ak06sl \n",
      "\n",
      "wandb: Agent Starting Run: v1yswh9x with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 40\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: v1yswh9x\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/v1yswh9x\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/v1yswh9x</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 48ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: v1yswh9x \n",
      "\n",
      "wandb: Agent Starting Run: b2zhllvz with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 40\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: b2zhllvz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/b2zhllvz\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/b2zhllvz</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 49ms/step - loss: 20.1684 - mse: 20.1684 - val_loss: 13.7258 - val_mse: 13.7258\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 19.8964 - mse: 19.8964 - val_loss: 13.0471 - val_mse: 13.0471\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 19.2610 - mse: 19.2610 - val_loss: 11.6177 - val_mse: 11.6177\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 18.0297 - mse: 18.0297 - val_loss: 9.3102 - val_mse: 9.3102\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 15.7308 - mse: 15.7308 - val_loss: 7.2402 - val_mse: 7.2402\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 13.0950 - mse: 13.0950 - val_loss: 5.9725 - val_mse: 5.9725\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 11.6194 - mse: 11.6194 - val_loss: 5.2270 - val_mse: 5.2270\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.7736 - mse: 10.7736 - val_loss: 4.8669 - val_mse: 4.8669\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2529 - mse: 10.2529 - val_loss: 4.7540 - val_mse: 4.7540\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1019 - mse: 10.1019 - val_loss: 4.7918 - val_mse: 4.7918\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0926 - mse: 10.0926 - val_loss: 4.9044 - val_mse: 4.9044\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1406 - mse: 10.1406 - val_loss: 5.0130 - val_mse: 5.0130\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2235 - mse: 10.2235 - val_loss: 5.0903 - val_mse: 5.0903\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2820 - mse: 10.2820 - val_loss: 5.1052 - val_mse: 5.1052\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2929 - mse: 10.2929 - val_loss: 5.0683 - val_mse: 5.0683\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2567 - mse: 10.2567 - val_loss: 4.9888 - val_mse: 4.9888\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2136 - mse: 10.2136 - val_loss: 4.9066 - val_mse: 4.9066\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1463 - mse: 10.1463 - val_loss: 4.8501 - val_mse: 4.8501\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1042 - mse: 10.1042 - val_loss: 4.8011 - val_mse: 4.8011\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0971 - mse: 10.0971 - val_loss: 4.7704 - val_mse: 4.7704\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0833 - mse: 10.0833 - val_loss: 4.7582 - val_mse: 4.7582\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0822 - mse: 10.0822 - val_loss: 4.7526 - val_mse: 4.7526\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0941 - mse: 10.0941 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1052 - mse: 10.1052 - val_loss: 4.7516 - val_mse: 4.7516\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1041 - mse: 10.1041 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0950 - mse: 10.0950 - val_loss: 4.7541 - val_mse: 4.7541\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0777 - mse: 10.0777 - val_loss: 4.7695 - val_mse: 4.7695\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0971 - mse: 10.0971 - val_loss: 4.7944 - val_mse: 4.7944\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0942 - mse: 10.0942 - val_loss: 4.8046 - val_mse: 4.8046\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0917 - mse: 10.0917 - val_loss: 4.7990 - val_mse: 4.7990\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0900 - mse: 10.0900 - val_loss: 4.7892 - val_mse: 4.7892\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0870 - mse: 10.0870 - val_loss: 4.7841 - val_mse: 4.7841\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0849 - mse: 10.0849 - val_loss: 4.7835 - val_mse: 4.7835\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0845 - mse: 10.0845 - val_loss: 4.7890 - val_mse: 4.7890\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0929 - mse: 10.0929 - val_loss: 4.7913 - val_mse: 4.7913\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0833 - mse: 10.0833 - val_loss: 4.7769 - val_mse: 4.7769\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0767 - mse: 10.0767 - val_loss: 4.7603 - val_mse: 4.7603\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0729 - mse: 10.0729 - val_loss: 4.7517 - val_mse: 4.7517\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0835 - mse: 10.0835 - val_loss: 4.7559 - val_mse: 4.7559\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1280 - mse: 10.1280 - val_loss: 4.7712 - val_mse: 4.7712\n",
      "wandb: Agent Finished Run: b2zhllvz \n",
      "\n",
      "wandb: Agent Starting Run: 3w0uzrzd with config:\n",
      "\tdout_rate: 0\n",
      "\tepochs: 40\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: 3w0uzrzd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/3w0uzrzd\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/3w0uzrzd</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 3w0uzrzd \n",
      "\n",
      "wandb: Agent Starting Run: rhwphy2x with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 10\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: rhwphy2x\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/rhwphy2x\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/rhwphy2x</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 49ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: rhwphy2x \n",
      "\n",
      "wandb: Agent Starting Run: jt0ksjl8 with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 10\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: jt0ksjl8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/jt0ksjl8\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/jt0ksjl8</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 5s 63ms/step - loss: 20.1766 - mse: 20.1766 - val_loss: 13.8077 - val_mse: 13.8077\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 19.9417 - mse: 19.9417 - val_loss: 13.5023 - val_mse: 13.5023\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 19.6245 - mse: 19.6245 - val_loss: 12.9354 - val_mse: 12.9354\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 18.9866 - mse: 18.9866 - val_loss: 11.7341 - val_mse: 11.7341\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 17.6612 - mse: 17.6612 - val_loss: 9.4328 - val_mse: 9.4328\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 15.4340 - mse: 15.4340 - val_loss: 6.1770 - val_mse: 6.1770\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 11.9430 - mse: 11.9430 - val_loss: 4.8010 - val_mse: 4.8010\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1662 - mse: 10.1662 - val_loss: 4.8716 - val_mse: 4.8716\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1028 - mse: 10.1028 - val_loss: 5.1696 - val_mse: 5.1696\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.3359 - mse: 10.3359 - val_loss: 5.3007 - val_mse: 5.3007\n",
      "wandb: Agent Finished Run: jt0ksjl8 \n",
      "\n",
      "wandb: Agent Starting Run: 4fclye7v with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 10\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: 4fclye7v\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/4fclye7v\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/4fclye7v</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 20.1675 - mse: 20.1675 - val_loss: 13.4745 - val_mse: 13.4745\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 19.6300 - mse: 19.6300 - val_loss: 11.8383 - val_mse: 11.8383\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 18.0485 - mse: 18.0485 - val_loss: 7.4135 - val_mse: 7.4135\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 13.4092 - mse: 13.4092 - val_loss: 4.7809 - val_mse: 4.7809\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1837 - mse: 10.1837 - val_loss: 5.6087 - val_mse: 5.6087\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.6556 - mse: 10.6556 - val_loss: 5.4140 - val_mse: 5.4140\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4674 - mse: 10.4674 - val_loss: 4.9602 - val_mse: 4.9602\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1212 - mse: 10.1212 - val_loss: 4.7518 - val_mse: 4.7518\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1030 - mse: 10.1030 - val_loss: 4.9048 - val_mse: 4.9048\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4096 - mse: 10.4096 - val_loss: 4.9834 - val_mse: 4.9834\n",
      "wandb: Agent Finished Run: 4fclye7v \n",
      "\n",
      "wandb: Agent Starting Run: yhcdm5t6 with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 20\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: yhcdm5t6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/yhcdm5t6\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/yhcdm5t6</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: yhcdm5t6 \n",
      "\n",
      "wandb: Agent Starting Run: s8vo236a with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 20\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: s8vo236a\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/s8vo236a\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/s8vo236a</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 20.1766 - mse: 20.1766 - val_loss: 13.8077 - val_mse: 13.8077\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 19.9417 - mse: 19.9417 - val_loss: 13.5023 - val_mse: 13.5023\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 19.6245 - mse: 19.6245 - val_loss: 12.9354 - val_mse: 12.9354\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 18.9866 - mse: 18.9866 - val_loss: 11.7341 - val_mse: 11.7341\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 17.6612 - mse: 17.6612 - val_loss: 9.4328 - val_mse: 9.4328\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 15.4340 - mse: 15.4340 - val_loss: 6.1770 - val_mse: 6.1770\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 11.9430 - mse: 11.9430 - val_loss: 4.8010 - val_mse: 4.8010\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1662 - mse: 10.1662 - val_loss: 4.8716 - val_mse: 4.8716\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1028 - mse: 10.1028 - val_loss: 5.1696 - val_mse: 5.1696\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.3359 - mse: 10.3359 - val_loss: 5.3007 - val_mse: 5.3007\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.4474 - mse: 10.4474 - val_loss: 5.3117 - val_mse: 5.3117\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.4558 - mse: 10.4558 - val_loss: 5.2637 - val_mse: 5.2637\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.4131 - mse: 10.4131 - val_loss: 5.1321 - val_mse: 5.1321\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.3082 - mse: 10.3082 - val_loss: 4.9759 - val_mse: 4.9759\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.2085 - mse: 10.2085 - val_loss: 4.8675 - val_mse: 4.8675\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1361 - mse: 10.1361 - val_loss: 4.8207 - val_mse: 4.8207\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1015 - mse: 10.1015 - val_loss: 4.8043 - val_mse: 4.8043\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0925 - mse: 10.0925 - val_loss: 4.7987 - val_mse: 4.7987\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0896 - mse: 10.0896 - val_loss: 4.7882 - val_mse: 4.7882\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0803 - mse: 10.0803 - val_loss: 4.7670 - val_mse: 4.7670\n",
      "wandb: Agent Finished Run: s8vo236a \n",
      "\n",
      "wandb: Agent Starting Run: 5meutyn8 with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 20\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: 5meutyn8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/5meutyn8\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/5meutyn8</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 20.1675 - mse: 20.1675 - val_loss: 13.4745 - val_mse: 13.4745\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 19.6300 - mse: 19.6300 - val_loss: 11.7458 - val_mse: 11.7458\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 17.9488 - mse: 17.9488 - val_loss: 7.1764 - val_mse: 7.1764\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 13.1227 - mse: 13.1227 - val_loss: 4.8210 - val_mse: 4.8210\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1800 - mse: 10.1800 - val_loss: 5.6617 - val_mse: 5.6617\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.7056 - mse: 10.7056 - val_loss: 5.4162 - val_mse: 5.4162\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4661 - mse: 10.4661 - val_loss: 4.9514 - val_mse: 4.9514\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1153 - mse: 10.1153 - val_loss: 4.7524 - val_mse: 4.7524\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1075 - mse: 10.1075 - val_loss: 4.9112 - val_mse: 4.9112\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4176 - mse: 10.4176 - val_loss: 4.9892 - val_mse: 4.9892\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4635 - mse: 10.4635 - val_loss: 4.8859 - val_mse: 4.8859\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.3855 - mse: 10.3855 - val_loss: 4.8274 - val_mse: 4.8274\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.2498 - mse: 10.2498 - val_loss: 4.8067 - val_mse: 4.8067\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2348 - mse: 10.2348 - val_loss: 4.7850 - val_mse: 4.7850\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1815 - mse: 10.1815 - val_loss: 4.7659 - val_mse: 4.7659\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1441 - mse: 10.1441 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0839 - mse: 10.0839 - val_loss: 4.7697 - val_mse: 4.7697\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0908 - mse: 10.0908 - val_loss: 4.8233 - val_mse: 4.8233\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0849 - mse: 10.0849 - val_loss: 4.8997 - val_mse: 4.8997\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1356 - mse: 10.1356 - val_loss: 5.0191 - val_mse: 5.0191\n",
      "wandb: Agent Finished Run: 5meutyn8 \n",
      "\n",
      "wandb: Agent Starting Run: rb75k0ug with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 40\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: rb75k0ug\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/rb75k0ug\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/rb75k0ug</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 49ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: rb75k0ug \n",
      "\n",
      "wandb: Agent Starting Run: 169o6837 with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 40\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: 169o6837\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/169o6837\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/169o6837</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 20.1803 - mse: 20.1803 - val_loss: 13.8528 - val_mse: 13.8528\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 19.9931 - mse: 19.9931 - val_loss: 13.5883 - val_mse: 13.5883\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 19.7199 - mse: 19.7199 - val_loss: 13.1316 - val_mse: 13.1316\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 19.1904 - mse: 19.1904 - val_loss: 12.1365 - val_mse: 12.1365\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 18.0852 - mse: 18.0852 - val_loss: 10.1798 - val_mse: 10.1798\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 16.1773 - mse: 16.1773 - val_loss: 6.9094 - val_mse: 6.9094\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 12.7495 - mse: 12.7495 - val_loss: 4.9160 - val_mse: 4.9160\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.3436 - mse: 10.3436 - val_loss: 4.8105 - val_mse: 4.8105\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0634 - mse: 10.0634 - val_loss: 5.1118 - val_mse: 5.1118\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2887 - mse: 10.2887 - val_loss: 5.2924 - val_mse: 5.2924\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.4377 - mse: 10.4377 - val_loss: 5.3367 - val_mse: 5.3367\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.4765 - mse: 10.4765 - val_loss: 5.3046 - val_mse: 5.3046\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.4464 - mse: 10.4464 - val_loss: 5.1729 - val_mse: 5.1729\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.3397 - mse: 10.3397 - val_loss: 5.0063 - val_mse: 5.0063\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.2300 - mse: 10.2300 - val_loss: 4.8857 - val_mse: 4.8857\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1477 - mse: 10.1477 - val_loss: 4.8305 - val_mse: 4.8305\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1067 - mse: 10.1067 - val_loss: 4.8089 - val_mse: 4.8089\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0947 - mse: 10.0947 - val_loss: 4.7995 - val_mse: 4.7995\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0890 - mse: 10.0890 - val_loss: 4.7865 - val_mse: 4.7865\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0792 - mse: 10.0792 - val_loss: 4.7648 - val_mse: 4.7648\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0774 - mse: 10.0774 - val_loss: 4.7521 - val_mse: 4.7521\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0834 - mse: 10.0834 - val_loss: 4.7559 - val_mse: 4.7559\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1114 - mse: 10.1114 - val_loss: 4.7812 - val_mse: 4.7812\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1854 - mse: 10.1854 - val_loss: 4.8138 - val_mse: 4.8138\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.2283 - mse: 10.2283 - val_loss: 4.8361 - val_mse: 4.8361\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2714 - mse: 10.2714 - val_loss: 4.8425 - val_mse: 4.8425\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.2709 - mse: 10.2709 - val_loss: 4.8166 - val_mse: 4.8166\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2286 - mse: 10.2286 - val_loss: 4.7834 - val_mse: 4.7834\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1648 - mse: 10.1648 - val_loss: 4.7565 - val_mse: 4.7565\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1031 - mse: 10.1031 - val_loss: 4.7554 - val_mse: 4.7554\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1333 - mse: 10.1333 - val_loss: 4.7780 - val_mse: 4.7780\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0824 - mse: 10.0824 - val_loss: 4.7920 - val_mse: 4.7920\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0895 - mse: 10.0895 - val_loss: 4.8016 - val_mse: 4.8016\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0901 - mse: 10.0901 - val_loss: 4.8080 - val_mse: 4.8080\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0993 - mse: 10.0993 - val_loss: 4.8074 - val_mse: 4.8074\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0867 - mse: 10.0867 - val_loss: 4.7859 - val_mse: 4.7859\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0912 - mse: 10.0912 - val_loss: 4.7664 - val_mse: 4.7664\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0803 - mse: 10.0803 - val_loss: 4.7573 - val_mse: 4.7573\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0882 - mse: 10.0882 - val_loss: 4.7531 - val_mse: 4.7531\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0907 - mse: 10.0907 - val_loss: 4.7518 - val_mse: 4.7518\n",
      "wandb: Agent Finished Run: 169o6837 \n",
      "\n",
      "wandb: Agent Starting Run: fsdqnpzs with config:\n",
      "\tdout_rate: 0.2\n",
      "\tepochs: 40\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: fsdqnpzs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/fsdqnpzs\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/fsdqnpzs</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 20.1675 - mse: 20.1675 - val_loss: 13.4070 - val_mse: 13.4070\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 19.5575 - mse: 19.5575 - val_loss: 11.4247 - val_mse: 11.4247\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 17.6428 - mse: 17.6428 - val_loss: 6.5939 - val_mse: 6.5939\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 12.4248 - mse: 12.4248 - val_loss: 4.9460 - val_mse: 4.9460\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2342 - mse: 10.2342 - val_loss: 5.7715 - val_mse: 5.7715\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.8095 - mse: 10.8095 - val_loss: 5.4328 - val_mse: 5.4328\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4749 - mse: 10.4749 - val_loss: 4.9432 - val_mse: 4.9432\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1103 - mse: 10.1103 - val_loss: 4.7531 - val_mse: 4.7531\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1111 - mse: 10.1111 - val_loss: 4.9169 - val_mse: 4.9169\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.4250 - mse: 10.4250 - val_loss: 4.9977 - val_mse: 4.9977\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4746 - mse: 10.4746 - val_loss: 4.8963 - val_mse: 4.8963\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.3978 - mse: 10.3978 - val_loss: 4.8363 - val_mse: 4.8363\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2627 - mse: 10.2627 - val_loss: 4.8134 - val_mse: 4.8134\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2452 - mse: 10.2452 - val_loss: 4.7893 - val_mse: 4.7893\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1878 - mse: 10.1878 - val_loss: 4.7679 - val_mse: 4.7679\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1482 - mse: 10.1482 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0854 - mse: 10.0854 - val_loss: 4.7682 - val_mse: 4.7682\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0906 - mse: 10.0906 - val_loss: 4.8202 - val_mse: 4.8202\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0829 - mse: 10.0829 - val_loss: 4.8954 - val_mse: 4.8954\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1328 - mse: 10.1328 - val_loss: 5.0128 - val_mse: 5.0128\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2379 - mse: 10.2379 - val_loss: 5.0917 - val_mse: 5.0917\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2830 - mse: 10.2830 - val_loss: 5.0776 - val_mse: 5.0776\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2624 - mse: 10.2624 - val_loss: 4.9754 - val_mse: 4.9754\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1758 - mse: 10.1758 - val_loss: 4.8399 - val_mse: 4.8399\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0960 - mse: 10.0960 - val_loss: 4.7659 - val_mse: 4.7659\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1166 - mse: 10.1166 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1024 - mse: 10.1024 - val_loss: 4.7544 - val_mse: 4.7544\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1260 - mse: 10.1260 - val_loss: 4.7550 - val_mse: 4.7550\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1195 - mse: 10.1195 - val_loss: 4.7519 - val_mse: 4.7519\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1012 - mse: 10.1012 - val_loss: 4.7530 - val_mse: 4.7530\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1232 - mse: 10.1232 - val_loss: 4.7577 - val_mse: 4.7577\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 10.0878 - mse: 10.0878 - val_loss: 4.7572 - val_mse: 4.7572\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0900 - mse: 10.0900 - val_loss: 4.7567 - val_mse: 4.7567\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0893 - mse: 10.0893 - val_loss: 4.7552 - val_mse: 4.7552\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0912 - mse: 10.0912 - val_loss: 4.7539 - val_mse: 4.7539\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0919 - mse: 10.0919 - val_loss: 4.7521 - val_mse: 4.7521\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0950 - mse: 10.0950 - val_loss: 4.7525 - val_mse: 4.7525\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0900 - mse: 10.0900 - val_loss: 4.7572 - val_mse: 4.7572\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0955 - mse: 10.0955 - val_loss: 4.7661 - val_mse: 4.7661\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0868 - mse: 10.0868 - val_loss: 4.7657 - val_mse: 4.7657\n",
      "wandb: Agent Finished Run: fsdqnpzs \n",
      "\n",
      "wandb: Agent Starting Run: ig28nmp0 with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 10\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: ig28nmp0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/ig28nmp0\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/ig28nmp0</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 49ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: ig28nmp0 \n",
      "\n",
      "wandb: Agent Starting Run: r717jgt0 with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 10\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: r717jgt0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/r717jgt0\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/r717jgt0</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 60ms/step - loss: 20.1800 - mse: 20.1800 - val_loss: 13.8305 - val_mse: 13.8305\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 19.9567 - mse: 19.9567 - val_loss: 13.4742 - val_mse: 13.4742\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 19.5788 - mse: 19.5788 - val_loss: 12.6937 - val_mse: 12.6937\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 18.7904 - mse: 18.7904 - val_loss: 11.0935 - val_mse: 11.0935\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 17.1257 - mse: 17.1257 - val_loss: 8.2903 - val_mse: 8.2903\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 14.4408 - mse: 14.4408 - val_loss: 5.5527 - val_mse: 5.5527\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 11.2060 - mse: 11.2060 - val_loss: 4.7738 - val_mse: 4.7738\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1179 - mse: 10.1179 - val_loss: 4.8662 - val_mse: 4.8662\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1114 - mse: 10.1114 - val_loss: 5.1445 - val_mse: 5.1445\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.3221 - mse: 10.3221 - val_loss: 5.2820 - val_mse: 5.2820\n",
      "wandb: Agent Finished Run: r717jgt0 \n",
      "\n",
      "wandb: Agent Starting Run: o5xkytyi with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 10\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: o5xkytyi\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/o5xkytyi\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/o5xkytyi</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 20.1740 - mse: 20.1740 - val_loss: 13.5800 - val_mse: 13.5800\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 19.7330 - mse: 19.7330 - val_loss: 12.1734 - val_mse: 12.1734\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 18.3663 - mse: 18.3663 - val_loss: 8.1268 - val_mse: 8.1268\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 14.2115 - mse: 14.2115 - val_loss: 4.7592 - val_mse: 4.7592\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2892 - mse: 10.2892 - val_loss: 5.4672 - val_mse: 5.4672\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.5221 - mse: 10.5221 - val_loss: 5.3845 - val_mse: 5.3845\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4462 - mse: 10.4462 - val_loss: 4.9451 - val_mse: 4.9451\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1089 - mse: 10.1089 - val_loss: 4.7553 - val_mse: 4.7553\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1201 - mse: 10.1201 - val_loss: 4.9352 - val_mse: 4.9352\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.4432 - mse: 10.4432 - val_loss: 4.9726 - val_mse: 4.9726\n",
      "wandb: Agent Finished Run: o5xkytyi \n",
      "\n",
      "wandb: Agent Starting Run: vyjkxj7d with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 20\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: vyjkxj7d\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/vyjkxj7d\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/vyjkxj7d</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: vyjkxj7d \n",
      "\n",
      "wandb: Agent Starting Run: 7n0kve03 with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 20\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: 7n0kve03\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/7n0kve03\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/7n0kve03</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 51ms/step - loss: 20.1760 - mse: 20.1760 - val_loss: 13.7799 - val_mse: 13.7799\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 19.8959 - mse: 19.8959 - val_loss: 13.3453 - val_mse: 13.3453\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 19.4524 - mse: 19.4524 - val_loss: 12.4135 - val_mse: 12.4135\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 18.5189 - mse: 18.5189 - val_loss: 10.5038 - val_mse: 10.5038\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 16.5288 - mse: 16.5288 - val_loss: 7.4608 - val_mse: 7.4608\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 13.5589 - mse: 13.5589 - val_loss: 5.2404 - val_mse: 5.2404\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.7845 - mse: 10.7845 - val_loss: 4.7538 - val_mse: 4.7538\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0755 - mse: 10.0755 - val_loss: 4.9111 - val_mse: 4.9111\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1437 - mse: 10.1437 - val_loss: 5.1715 - val_mse: 5.1715\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.3440 - mse: 10.3440 - val_loss: 5.2760 - val_mse: 5.2760\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.4307 - mse: 10.4307 - val_loss: 5.2817 - val_mse: 5.2817\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.4327 - mse: 10.4327 - val_loss: 5.2400 - val_mse: 5.2400\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.3950 - mse: 10.3950 - val_loss: 5.1237 - val_mse: 5.1237\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.3023 - mse: 10.3023 - val_loss: 4.9816 - val_mse: 4.9816\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.2113 - mse: 10.2113 - val_loss: 4.8782 - val_mse: 4.8782\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1418 - mse: 10.1418 - val_loss: 4.8307 - val_mse: 4.8307\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1063 - mse: 10.1063 - val_loss: 4.8127 - val_mse: 4.8127\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0960 - mse: 10.0960 - val_loss: 4.8052 - val_mse: 4.8052\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0916 - mse: 10.0916 - val_loss: 4.7922 - val_mse: 4.7922\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0808 - mse: 10.0808 - val_loss: 4.7680 - val_mse: 4.7680\n",
      "wandb: Agent Finished Run: 7n0kve03 \n",
      "\n",
      "wandb: Agent Starting Run: q5vd4wwt with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 20\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: q5vd4wwt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/q5vd4wwt\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/q5vd4wwt</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 20.1672 - mse: 20.1672 - val_loss: 13.4042 - val_mse: 13.4042\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 19.5590 - mse: 19.5590 - val_loss: 11.4161 - val_mse: 11.4161\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 17.6415 - mse: 17.6415 - val_loss: 6.6135 - val_mse: 6.6135\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 12.4620 - mse: 12.4620 - val_loss: 4.9360 - val_mse: 4.9360\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2304 - mse: 10.2304 - val_loss: 5.7735 - val_mse: 5.7735\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.8103 - mse: 10.8103 - val_loss: 5.4362 - val_mse: 5.4362\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4773 - mse: 10.4773 - val_loss: 4.9429 - val_mse: 4.9429\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1102 - mse: 10.1102 - val_loss: 4.7530 - val_mse: 4.7530\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1108 - mse: 10.1108 - val_loss: 4.9122 - val_mse: 4.9122\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4179 - mse: 10.4179 - val_loss: 4.9907 - val_mse: 4.9907\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4657 - mse: 10.4657 - val_loss: 4.8918 - val_mse: 4.8918\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.3914 - mse: 10.3914 - val_loss: 4.8338 - val_mse: 4.8338\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2592 - mse: 10.2592 - val_loss: 4.8122 - val_mse: 4.8122\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2431 - mse: 10.2431 - val_loss: 4.7891 - val_mse: 4.7891\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1878 - mse: 10.1878 - val_loss: 4.7681 - val_mse: 4.7681\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1487 - mse: 10.1487 - val_loss: 4.7515 - val_mse: 4.7515\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0859 - mse: 10.0859 - val_loss: 4.7677 - val_mse: 4.7677\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0906 - mse: 10.0906 - val_loss: 4.8196 - val_mse: 4.8196\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0825 - mse: 10.0825 - val_loss: 4.8950 - val_mse: 4.8950\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1324 - mse: 10.1324 - val_loss: 5.0138 - val_mse: 5.0138\n",
      "wandb: Agent Finished Run: q5vd4wwt \n",
      "\n",
      "wandb: Agent Starting Run: 4kymudp5 with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 40\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: 4kymudp5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/4kymudp5\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/4kymudp5</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 48ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 4kymudp5 \n",
      "\n",
      "wandb: Agent Starting Run: ky2kwsqf with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 40\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: ky2kwsqf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/ky2kwsqf\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/ky2kwsqf</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 20.1800 - mse: 20.1800 - val_loss: 13.8114 - val_mse: 13.8114\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 19.9384 - mse: 19.9384 - val_loss: 13.4456 - val_mse: 13.4456\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 19.5500 - mse: 19.5500 - val_loss: 12.6199 - val_mse: 12.6199\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 18.7175 - mse: 18.7175 - val_loss: 10.9342 - val_mse: 10.9342\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 16.9641 - mse: 16.9641 - val_loss: 8.0625 - val_mse: 8.0625\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 14.2022 - mse: 14.2022 - val_loss: 5.4543 - val_mse: 5.4543\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 11.0759 - mse: 11.0759 - val_loss: 4.7657 - val_mse: 4.7657\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1026 - mse: 10.1026 - val_loss: 4.8793 - val_mse: 4.8793\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1204 - mse: 10.1204 - val_loss: 5.1555 - val_mse: 5.1555\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.3309 - mse: 10.3309 - val_loss: 5.2839 - val_mse: 5.2839\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.4359 - mse: 10.4359 - val_loss: 5.3021 - val_mse: 5.3021\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.4492 - mse: 10.4492 - val_loss: 5.2637 - val_mse: 5.2637\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.4140 - mse: 10.4140 - val_loss: 5.1430 - val_mse: 5.1430\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.3170 - mse: 10.3170 - val_loss: 4.9938 - val_mse: 4.9938\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2201 - mse: 10.2201 - val_loss: 4.8845 - val_mse: 4.8845\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1461 - mse: 10.1461 - val_loss: 4.8332 - val_mse: 4.8332\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1078 - mse: 10.1078 - val_loss: 4.8130 - val_mse: 4.8130\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0964 - mse: 10.0964 - val_loss: 4.8040 - val_mse: 4.8040\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0908 - mse: 10.0908 - val_loss: 4.7904 - val_mse: 4.7904\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0801 - mse: 10.0801 - val_loss: 4.7666 - val_mse: 4.7666\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0766 - mse: 10.0766 - val_loss: 4.7523 - val_mse: 4.7523\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0820 - mse: 10.0820 - val_loss: 4.7562 - val_mse: 4.7562\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1118 - mse: 10.1118 - val_loss: 4.7839 - val_mse: 4.7839\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1908 - mse: 10.1908 - val_loss: 4.8197 - val_mse: 4.8197\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2374 - mse: 10.2374 - val_loss: 4.8431 - val_mse: 4.8431\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.2817 - mse: 10.2817 - val_loss: 4.8484 - val_mse: 4.8484\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2791 - mse: 10.2791 - val_loss: 4.8188 - val_mse: 4.8188\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2313 - mse: 10.2313 - val_loss: 4.7829 - val_mse: 4.7829\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1629 - mse: 10.1629 - val_loss: 4.7555 - val_mse: 4.7555\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0994 - mse: 10.0994 - val_loss: 4.7571 - val_mse: 4.7571\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1332 - mse: 10.1332 - val_loss: 4.7828 - val_mse: 4.7828\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0834 - mse: 10.0834 - val_loss: 4.7977 - val_mse: 4.7977\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0915 - mse: 10.0915 - val_loss: 4.8071 - val_mse: 4.8071\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0926 - mse: 10.0926 - val_loss: 4.8127 - val_mse: 4.8127\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1010 - mse: 10.1010 - val_loss: 4.8107 - val_mse: 4.8107\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0873 - mse: 10.0873 - val_loss: 4.7870 - val_mse: 4.7870\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0918 - mse: 10.0918 - val_loss: 4.7662 - val_mse: 4.7662\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0802 - mse: 10.0802 - val_loss: 4.7567 - val_mse: 4.7567\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0888 - mse: 10.0888 - val_loss: 4.7526 - val_mse: 4.7526\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0918 - mse: 10.0918 - val_loss: 4.7515 - val_mse: 4.7515\n",
      "wandb: Agent Finished Run: ky2kwsqf \n",
      "\n",
      "wandb: Agent Starting Run: uuoy506v with config:\n",
      "\tdout_rate: 0.4\n",
      "\tepochs: 40\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: uuoy506v\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/uuoy506v\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/uuoy506v</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 20.1672 - mse: 20.1672 - val_loss: 13.4042 - val_mse: 13.4042\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 19.5590 - mse: 19.5590 - val_loss: 11.4161 - val_mse: 11.4161\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 17.6415 - mse: 17.6415 - val_loss: 6.6135 - val_mse: 6.6135\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 12.4620 - mse: 12.4620 - val_loss: 4.9360 - val_mse: 4.9360\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2304 - mse: 10.2304 - val_loss: 5.7735 - val_mse: 5.7735\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.8103 - mse: 10.8103 - val_loss: 5.4362 - val_mse: 5.4362\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4773 - mse: 10.4773 - val_loss: 4.9429 - val_mse: 4.9429\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1102 - mse: 10.1102 - val_loss: 4.7530 - val_mse: 4.7530\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1108 - mse: 10.1108 - val_loss: 4.9122 - val_mse: 4.9122\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4179 - mse: 10.4179 - val_loss: 4.9907 - val_mse: 4.9907\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4657 - mse: 10.4657 - val_loss: 4.8918 - val_mse: 4.8918\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.3914 - mse: 10.3914 - val_loss: 4.8338 - val_mse: 4.8338\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2592 - mse: 10.2592 - val_loss: 4.8122 - val_mse: 4.8122\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2431 - mse: 10.2431 - val_loss: 4.7891 - val_mse: 4.7891\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1878 - mse: 10.1878 - val_loss: 4.7681 - val_mse: 4.7681\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1487 - mse: 10.1487 - val_loss: 4.7515 - val_mse: 4.7515\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0859 - mse: 10.0859 - val_loss: 4.7677 - val_mse: 4.7677\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0906 - mse: 10.0906 - val_loss: 4.8196 - val_mse: 4.8196\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0825 - mse: 10.0825 - val_loss: 4.8950 - val_mse: 4.8950\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1324 - mse: 10.1324 - val_loss: 5.0138 - val_mse: 5.0138\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.2389 - mse: 10.2389 - val_loss: 5.0933 - val_mse: 5.0933\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2842 - mse: 10.2842 - val_loss: 5.0777 - val_mse: 5.0777\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2619 - mse: 10.2619 - val_loss: 4.9735 - val_mse: 4.9735\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1742 - mse: 10.1742 - val_loss: 4.8375 - val_mse: 4.8375\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0947 - mse: 10.0947 - val_loss: 4.7648 - val_mse: 4.7648\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1169 - mse: 10.1169 - val_loss: 4.7514 - val_mse: 4.7514\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1037 - mse: 10.1037 - val_loss: 4.7548 - val_mse: 4.7548\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1274 - mse: 10.1274 - val_loss: 4.7554 - val_mse: 4.7554\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1207 - mse: 10.1207 - val_loss: 4.7520 - val_mse: 4.7520\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1020 - mse: 10.1020 - val_loss: 4.7529 - val_mse: 4.7529\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1237 - mse: 10.1237 - val_loss: 4.7575 - val_mse: 4.7575\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0879 - mse: 10.0879 - val_loss: 4.7571 - val_mse: 4.7571\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0902 - mse: 10.0902 - val_loss: 4.7568 - val_mse: 4.7568\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0892 - mse: 10.0892 - val_loss: 4.7553 - val_mse: 4.7553\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0911 - mse: 10.0911 - val_loss: 4.7541 - val_mse: 4.7541\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0916 - mse: 10.0916 - val_loss: 4.7522 - val_mse: 4.7522\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0946 - mse: 10.0946 - val_loss: 4.7526 - val_mse: 4.7526\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0897 - mse: 10.0897 - val_loss: 4.7574 - val_mse: 4.7574\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0952 - mse: 10.0952 - val_loss: 4.7663 - val_mse: 4.7663\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0867 - mse: 10.0867 - val_loss: 4.7658 - val_mse: 4.7658\n",
      "wandb: Agent Finished Run: uuoy506v \n",
      "\n",
      "wandb: Agent Starting Run: o07vceyv with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 10\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: o07vceyv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/o07vceyv\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/o07vceyv</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 4s 49ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: o07vceyv \n",
      "\n",
      "wandb: Agent Starting Run: 125dtln4 with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 10\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: 125dtln4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/125dtln4\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/125dtln4</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 5s 63ms/step - loss: 20.1749 - mse: 20.1749 - val_loss: 13.7789 - val_mse: 13.7789\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 19.8806 - mse: 19.8806 - val_loss: 13.2956 - val_mse: 13.2956\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 19.4032 - mse: 19.4032 - val_loss: 12.2454 - val_mse: 12.2454\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 18.3692 - mse: 18.3692 - val_loss: 10.1013 - val_mse: 10.1013\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 16.1539 - mse: 16.1539 - val_loss: 7.0187 - val_mse: 7.0187\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 13.0803 - mse: 13.0803 - val_loss: 5.1325 - val_mse: 5.1325\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.6355 - mse: 10.6355 - val_loss: 4.7521 - val_mse: 4.7521\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0638 - mse: 10.0638 - val_loss: 4.9532 - val_mse: 4.9532\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1724 - mse: 10.1724 - val_loss: 5.2191 - val_mse: 5.2191\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.3789 - mse: 10.3789 - val_loss: 5.3019 - val_mse: 5.3019\n",
      "wandb: Agent Finished Run: 125dtln4 \n",
      "\n",
      "wandb: Agent Starting Run: 322x1361 with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 10\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: 322x1361\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/322x1361\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/322x1361</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 5s 66ms/step - loss: 20.1672 - mse: 20.1672 - val_loss: 13.3973 - val_mse: 13.3973\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 19.5556 - mse: 19.5556 - val_loss: 11.4068 - val_mse: 11.4068\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 17.6362 - mse: 17.6362 - val_loss: 6.6129 - val_mse: 6.6129\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 12.4590 - mse: 12.4590 - val_loss: 4.9100 - val_mse: 4.9100\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2056 - mse: 10.2056 - val_loss: 5.7363 - val_mse: 5.7363\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.7788 - mse: 10.7788 - val_loss: 5.3871 - val_mse: 5.3871\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4315 - mse: 10.4315 - val_loss: 4.8923 - val_mse: 4.8923\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0785 - mse: 10.0785 - val_loss: 4.7650 - val_mse: 4.7650\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1470 - mse: 10.1470 - val_loss: 4.9529 - val_mse: 4.9529\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4613 - mse: 10.4613 - val_loss: 4.9843 - val_mse: 4.9843\n",
      "wandb: Agent Finished Run: 322x1361 \n",
      "\n",
      "wandb: Agent Starting Run: 6uvdc0bg with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 20\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: 6uvdc0bg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/6uvdc0bg\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/6uvdc0bg</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 48ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 6uvdc0bg \n",
      "\n",
      "wandb: Agent Starting Run: 6svj63n9 with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 20\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: 6svj63n9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/6svj63n9\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/6svj63n9</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 51ms/step - loss: 20.1791 - mse: 20.1791 - val_loss: 13.8302 - val_mse: 13.8302\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 19.9499 - mse: 19.9499 - val_loss: 13.4596 - val_mse: 13.4596\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 19.5647 - mse: 19.5647 - val_loss: 12.6322 - val_mse: 12.6322\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 18.7442 - mse: 18.7442 - val_loss: 10.9155 - val_mse: 10.9155\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 16.9875 - mse: 16.9875 - val_loss: 8.0008 - val_mse: 8.0008\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 14.1943 - mse: 14.1943 - val_loss: 5.4585 - val_mse: 5.4585\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 11.0910 - mse: 11.0910 - val_loss: 4.7684 - val_mse: 4.7684\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1111 - mse: 10.1111 - val_loss: 4.8683 - val_mse: 4.8683\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1146 - mse: 10.1146 - val_loss: 5.1406 - val_mse: 5.1406\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.3191 - mse: 10.3191 - val_loss: 5.2755 - val_mse: 5.2755\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.4286 - mse: 10.4286 - val_loss: 5.3000 - val_mse: 5.3000\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.4477 - mse: 10.4477 - val_loss: 5.2666 - val_mse: 5.2666\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.4165 - mse: 10.4165 - val_loss: 5.1500 - val_mse: 5.1500\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.3226 - mse: 10.3226 - val_loss: 5.0024 - val_mse: 5.0024\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.2261 - mse: 10.2261 - val_loss: 4.8924 - val_mse: 4.8924\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1508 - mse: 10.1508 - val_loss: 4.8394 - val_mse: 4.8394\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1111 - mse: 10.1111 - val_loss: 4.8171 - val_mse: 4.8171\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0984 - mse: 10.0984 - val_loss: 4.8064 - val_mse: 4.8064\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0914 - mse: 10.0914 - val_loss: 4.7912 - val_mse: 4.7912\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0800 - mse: 10.0800 - val_loss: 4.7665 - val_mse: 4.7665\n",
      "wandb: Agent Finished Run: 6svj63n9 \n",
      "\n",
      "wandb: Agent Starting Run: keibsi7w with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 20\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: keibsi7w\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/keibsi7w\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/keibsi7w</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 20.1738 - mse: 20.1738 - val_loss: 13.5744 - val_mse: 13.5744\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 19.7301 - mse: 19.7301 - val_loss: 12.1671 - val_mse: 12.1671\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 18.3639 - mse: 18.3639 - val_loss: 8.1157 - val_mse: 8.1157\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 14.2019 - mse: 14.2019 - val_loss: 4.7598 - val_mse: 4.7598\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2718 - mse: 10.2718 - val_loss: 5.4633 - val_mse: 5.4633\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.5203 - mse: 10.5203 - val_loss: 5.3861 - val_mse: 5.3861\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.4475 - mse: 10.4475 - val_loss: 4.9454 - val_mse: 4.9454\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1090 - mse: 10.1090 - val_loss: 4.7552 - val_mse: 4.7552\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1197 - mse: 10.1197 - val_loss: 4.9319 - val_mse: 4.9319\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4383 - mse: 10.4383 - val_loss: 4.9673 - val_mse: 4.9673\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4321 - mse: 10.4321 - val_loss: 4.8351 - val_mse: 4.8351\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.3274 - mse: 10.3274 - val_loss: 4.7822 - val_mse: 4.7822\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1780 - mse: 10.1780 - val_loss: 4.7711 - val_mse: 4.7711\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1731 - mse: 10.1731 - val_loss: 4.7624 - val_mse: 4.7624\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1433 - mse: 10.1433 - val_loss: 4.7554 - val_mse: 4.7554\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1195 - mse: 10.1195 - val_loss: 4.7529 - val_mse: 4.7529\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0783 - mse: 10.0783 - val_loss: 4.7764 - val_mse: 4.7764\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0908 - mse: 10.0908 - val_loss: 4.8275 - val_mse: 4.8275\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0903 - mse: 10.0903 - val_loss: 4.8960 - val_mse: 4.8960\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1337 - mse: 10.1337 - val_loss: 5.0075 - val_mse: 5.0075\n",
      "wandb: Agent Finished Run: keibsi7w \n",
      "\n",
      "wandb: Agent Starting Run: 7gherfni with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 40\n",
      "\tnn_units: 16\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: 7gherfni\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/7gherfni\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/7gherfni</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 20.1910 - mse: 20.1910 - val_loss: 14.0176 - val_mse: 14.0176\n",
      "wandb: Agent Finished Run: 7gherfni \n",
      "\n",
      "wandb: Agent Starting Run: cogkh340 with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 40\n",
      "\tnn_units: 32\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: cogkh340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/cogkh340\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/cogkh340</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 20.1749 - mse: 20.1749 - val_loss: 13.7789 - val_mse: 13.7789\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 19.8806 - mse: 19.8806 - val_loss: 13.2956 - val_mse: 13.2956\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 19.4032 - mse: 19.4032 - val_loss: 12.2454 - val_mse: 12.2454\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 18.3692 - mse: 18.3692 - val_loss: 10.1013 - val_mse: 10.1013\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 16.1539 - mse: 16.1539 - val_loss: 7.0187 - val_mse: 7.0187\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 13.0803 - mse: 13.0803 - val_loss: 5.1325 - val_mse: 5.1325\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.6355 - mse: 10.6355 - val_loss: 4.7521 - val_mse: 4.7521\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0638 - mse: 10.0638 - val_loss: 4.9532 - val_mse: 4.9532\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1724 - mse: 10.1724 - val_loss: 5.2178 - val_mse: 5.2178\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.3779 - mse: 10.3779 - val_loss: 5.3011 - val_mse: 5.3011\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.4506 - mse: 10.4506 - val_loss: 5.2858 - val_mse: 5.2858\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.4349 - mse: 10.4349 - val_loss: 5.2282 - val_mse: 5.2282\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.3846 - mse: 10.3846 - val_loss: 5.1025 - val_mse: 5.1025\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2858 - mse: 10.2858 - val_loss: 4.9581 - val_mse: 4.9581\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1957 - mse: 10.1957 - val_loss: 4.8587 - val_mse: 4.8587\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1304 - mse: 10.1304 - val_loss: 4.8172 - val_mse: 4.8172\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0994 - mse: 10.0994 - val_loss: 4.8042 - val_mse: 4.8042\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0921 - mse: 10.0921 - val_loss: 4.8011 - val_mse: 4.8011\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0911 - mse: 10.0911 - val_loss: 4.7916 - val_mse: 4.7916\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0813 - mse: 10.0813 - val_loss: 4.7688 - val_mse: 4.7688\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0768 - mse: 10.0768 - val_loss: 4.7531 - val_mse: 4.7531\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0799 - mse: 10.0799 - val_loss: 4.7550 - val_mse: 4.7550\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1072 - mse: 10.1072 - val_loss: 4.7822 - val_mse: 4.7822\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1881 - mse: 10.1881 - val_loss: 4.8190 - val_mse: 4.8190\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2365 - mse: 10.2365 - val_loss: 4.8427 - val_mse: 4.8427\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.2814 - mse: 10.2814 - val_loss: 4.8473 - val_mse: 4.8473\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.2771 - mse: 10.2771 - val_loss: 4.8155 - val_mse: 4.8155\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.2260 - mse: 10.2260 - val_loss: 4.7793 - val_mse: 4.7793\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1564 - mse: 10.1564 - val_loss: 4.7541 - val_mse: 4.7541\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0949 - mse: 10.0949 - val_loss: 4.7592 - val_mse: 4.7592\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.1322 - mse: 10.1322 - val_loss: 4.7872 - val_mse: 4.7872\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0847 - mse: 10.0847 - val_loss: 4.8020 - val_mse: 4.8020\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0930 - mse: 10.0930 - val_loss: 4.8105 - val_mse: 4.8105\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0944 - mse: 10.0944 - val_loss: 4.8150 - val_mse: 4.8150\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.1015 - mse: 10.1015 - val_loss: 4.8117 - val_mse: 4.8117\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0872 - mse: 10.0872 - val_loss: 4.7870 - val_mse: 4.7870\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0920 - mse: 10.0920 - val_loss: 4.7659 - val_mse: 4.7659\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0802 - mse: 10.0802 - val_loss: 4.7564 - val_mse: 4.7564\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 10.0891 - mse: 10.0891 - val_loss: 4.7525 - val_mse: 4.7525\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 10.0923 - mse: 10.0923 - val_loss: 4.7515 - val_mse: 4.7515\n",
      "wandb: Agent Finished Run: cogkh340 \n",
      "\n",
      "wandb: Agent Starting Run: 1a5qm0rm with config:\n",
      "\tdout_rate: 0.6\n",
      "\tepochs: 40\n",
      "\tnn_units: 64\n",
      "\tbatch_size: 64\n",
      "wandb: Agent Started Run: 1a5qm0rm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/mdvu8oxq</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/1a5qm0rm\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/1a5qm0rm</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 20.1672 - mse: 20.1672 - val_loss: 13.3973 - val_mse: 13.3973\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 19.5556 - mse: 19.5556 - val_loss: 11.4068 - val_mse: 11.4068\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 17.6362 - mse: 17.6362 - val_loss: 6.6129 - val_mse: 6.6129\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 12.4590 - mse: 12.4590 - val_loss: 4.9100 - val_mse: 4.9100\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2056 - mse: 10.2056 - val_loss: 5.7363 - val_mse: 5.7363\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.7788 - mse: 10.7788 - val_loss: 5.3871 - val_mse: 5.3871\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.4315 - mse: 10.4315 - val_loss: 4.8923 - val_mse: 4.8923\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0785 - mse: 10.0785 - val_loss: 4.7650 - val_mse: 4.7650\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1470 - mse: 10.1470 - val_loss: 4.9529 - val_mse: 4.9529\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.4613 - mse: 10.4613 - val_loss: 4.9843 - val_mse: 4.9843\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.4540 - mse: 10.4540 - val_loss: 4.8512 - val_mse: 4.8512\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.3475 - mse: 10.3475 - val_loss: 4.7923 - val_mse: 4.7923\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1946 - mse: 10.1946 - val_loss: 4.7768 - val_mse: 4.7768\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1845 - mse: 10.1845 - val_loss: 4.7648 - val_mse: 4.7648\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.1463 - mse: 10.1463 - val_loss: 4.7558 - val_mse: 4.7558\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1208 - mse: 10.1208 - val_loss: 4.7528 - val_mse: 4.7528\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0783 - mse: 10.0783 - val_loss: 4.7759 - val_mse: 4.7759\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0907 - mse: 10.0907 - val_loss: 4.8262 - val_mse: 4.8262\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0895 - mse: 10.0895 - val_loss: 4.8937 - val_mse: 4.8937\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1323 - mse: 10.1323 - val_loss: 5.0021 - val_mse: 5.0021\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2291 - mse: 10.2291 - val_loss: 5.0714 - val_mse: 5.0714\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2672 - mse: 10.2672 - val_loss: 5.0501 - val_mse: 5.0501\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.2400 - mse: 10.2400 - val_loss: 4.9470 - val_mse: 4.9470\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1557 - mse: 10.1557 - val_loss: 4.8194 - val_mse: 4.8194\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0861 - mse: 10.0861 - val_loss: 4.7580 - val_mse: 4.7580\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1202 - mse: 10.1202 - val_loss: 4.7528 - val_mse: 4.7528\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1138 - mse: 10.1138 - val_loss: 4.7584 - val_mse: 4.7584\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1364 - mse: 10.1364 - val_loss: 4.7577 - val_mse: 4.7577\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1269 - mse: 10.1269 - val_loss: 4.7523 - val_mse: 4.7523\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1022 - mse: 10.1022 - val_loss: 4.7533 - val_mse: 4.7533\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.1262 - mse: 10.1262 - val_loss: 4.7595 - val_mse: 4.7595\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0863 - mse: 10.0863 - val_loss: 4.7597 - val_mse: 4.7597\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0891 - mse: 10.0891 - val_loss: 4.7595 - val_mse: 4.7595\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0875 - mse: 10.0875 - val_loss: 4.7575 - val_mse: 4.7575\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0889 - mse: 10.0889 - val_loss: 4.7557 - val_mse: 4.7557\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0896 - mse: 10.0896 - val_loss: 4.7529 - val_mse: 4.7529\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0927 - mse: 10.0927 - val_loss: 4.7532 - val_mse: 4.7532\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 10.0886 - mse: 10.0886 - val_loss: 4.7583 - val_mse: 4.7583\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0945 - mse: 10.0945 - val_loss: 4.7674 - val_mse: 4.7674\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 10.0862 - mse: 10.0862 - val_loss: 4.7662 - val_mse: 4.7662\n",
      "wandb: Agent Finished Run: 1a5qm0rm \n",
      "\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweepid, function=train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
