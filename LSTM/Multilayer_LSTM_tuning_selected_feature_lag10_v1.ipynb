{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* Country: United States, France, Germany, Japan, United Kingdom, Italy, Canada\n",
    "* Time period: 1950-2018, 69 years\n",
    "* Target variable: `ngdp_rpch` for annual data, `ngdp_r_sa_pcha` and `ngdp_r_sa_pchy` (respectively) for quarterly data\n",
    "* Train-test split: 1950-2009 (train, ≤ x  years, depends on data availability), x - y (test, z years)   \n",
    "  _Need further discussion. Here I divide the dataset by x/y just as the working paper did. Now for the ML model family we do not need to do such split._\n",
    "  \n",
    "* LSTM Model\n",
    "* CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Module 1: Importing the libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "keras = tf.keras\n",
    "\n",
    "# Print all outputs in a code block\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# from tf.random import set_seed\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# from keras.callbacks import ResetStatesCallback()\n",
    "\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten,Dense\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # Use the %tensorflow_version magic if in colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import random\n",
    "# from tensorflow.random import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "\n",
    "seed_global = 42\n",
    "\n",
    "# Source: https://machinelearningmastery.com/reproducible-results-neural-networks-keras/\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(seed_global)\n",
    "\n",
    "#  Giving an eror \n",
    "# from tensorflow import set_random_seed\n",
    "# set_random_seed(seed_global)\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/58638701/importerror-cannot-import-name-set-random-seed-from-tensorflow-c-users-po\n",
    "\n",
    "tf.random.set_seed(seed_global)\n",
    "\n",
    "# Copy paste this code snippet in every model code chunk \n",
    "seed(seed_global)\n",
    "tf.random.set_seed(seed_global)\n",
    "\n",
    "# --Ignore--\n",
    "# tf.random.set_seed(seed)\n",
    "# # This is giving me an error\n",
    "\n",
    "# #  Global Seed\n",
    "# # random.seed (2019) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery gdp_quarterly_q\n",
    "\n",
    "SELECT *\n",
    "FROM `deep-nexus.temp_for_imf_data.WEO_G7_Quarterly`\n",
    "ORDER BY time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "gdp_quarterly_q.year = (gdp_quarterly_q.time+40)//4 + 1950\n",
    "gdp_quarterly_q.quarter = (gdp_quarterly_q.time+40)%4 + 1\n",
    "gdp_quarterly_q.time = gdp_quarterly_q.year.astype('str') + 'Q' + gdp_quarterly_q.quarter.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>ifscode</th>\n",
       "      <th>time</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>111</td>\n",
       "      <td>1950Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>112</td>\n",
       "      <td>1950Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.776393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.776393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>France</td>\n",
       "      <td>132</td>\n",
       "      <td>1950Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.016824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.016824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Germany</td>\n",
       "      <td>134</td>\n",
       "      <td>1950Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Italy</td>\n",
       "      <td>136</td>\n",
       "      <td>1950Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          country  ifscode    time  gdpwgt  lc  le       llf  lulcm  lur  \\\n",
       "0   United States      111  1950Q1     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "1  United Kingdom      112  1950Q1     NaN NaN NaN       NaN    NaN  NaN   \n",
       "2          France      132  1950Q1     NaN NaN NaN       NaN    NaN  NaN   \n",
       "3         Germany      134  1950Q1     NaN NaN NaN       NaN    NaN  NaN   \n",
       "4           Italy      136  1950Q1     NaN NaN NaN       NaN    NaN  NaN   \n",
       "\n",
       "   ncg_r  ...  pcpi_sa  pcpi_sa_pcha  pcpi_sa_pchy      pppgdp  pppsh  \\\n",
       "0    NaN  ...      NaN           NaN           NaN  301.782705    NaN   \n",
       "1    NaN  ...      NaN           NaN           NaN   72.776393    NaN   \n",
       "2    NaN  ...      NaN           NaN           NaN   47.016824    NaN   \n",
       "3    NaN  ...      NaN           NaN           NaN         NaN    NaN   \n",
       "4    NaN  ...      NaN           NaN           NaN         NaN    NaN   \n",
       "\n",
       "       pppwgt  tmgwgt  tmwgt  txgwgt  txwgt  \n",
       "0  301.782705     NaN    NaN     NaN    NaN  \n",
       "1   72.776393     NaN    NaN     NaN    NaN  \n",
       "2   47.016824     NaN    NaN     NaN    NaN  \n",
       "3         NaN     NaN    NaN     NaN    NaN  \n",
       "4         NaN     NaN    NaN     NaN    NaN  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdp_quarterly_q = pd.DataFrame(gdp_quarterly_q)\n",
    "gdp_quarterly_q.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['United States', 'United Kingdom', 'France', 'Germany', 'Italy',\n",
       "       'Canada', 'Japan'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Countries: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['United States']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting a subset of countries\n",
    "gdp_quarterly_q.country.unique()\n",
    "\n",
    "selected_countries = list(gdp_quarterly_q.country.unique())[0:1]\n",
    "print(\"\\nSelected Countries: \\n\")\n",
    "selected_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_2 = gdp_quarterly_q\n",
    "\n",
    "# for i in selected_countries:\n",
    "#    dataset_2[i] = gdp_quarterly_q[gdp_quarterly_q['country'] == i]\n",
    "\n",
    "# https://stackoverflow.com/questions/51583888/concatenate-dataframe-name-with-variable-value-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest Regressor\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # random forest model creation\n",
    "\n",
    "# # Set Seed\n",
    "# seed(seed_global)\n",
    "# tensorflow.random.set_seed(seed_global)\n",
    "\n",
    "\n",
    "# rfr = RandomForestRegressor(n_estimators = 1000)\n",
    "\n",
    "# rfr.fit(X_train, y_train)\n",
    "\n",
    "# # predictions\n",
    "# y_pred = rfr.predict(X_test)\n",
    "\n",
    "# metrics_mse[\"random_forest\"] =  mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# print(\"Random Forest Test MSE: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Variable Importance\n",
    "\n",
    "# # Top i factors by importance\n",
    "\n",
    "# i = 20\n",
    "# importances = rf_reg.feature_importances_\n",
    "# indices = np.argsort(importances)[-(i-1):]\n",
    "# features = X.columns\n",
    "\n",
    "# plt.figure(figsize=(6,6))\n",
    "# plt.title('Feature Importances - Random Forest Regressor')\n",
    "# plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "# plt.yticks(range(len(indices)), features[indices])\n",
    "# plt.xlabel('Relative Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>ifscode</th>\n",
       "      <th>time</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>111</td>\n",
       "      <td>1950Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>United States</td>\n",
       "      <td>111</td>\n",
       "      <td>1950Q2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>United States</td>\n",
       "      <td>111</td>\n",
       "      <td>1950Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>United States</td>\n",
       "      <td>111</td>\n",
       "      <td>1950Q4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.782705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>United States</td>\n",
       "      <td>111</td>\n",
       "      <td>1951Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>348.993057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>348.993057</td>\n",
       "      <td>8.709073</td>\n",
       "      <td>11.508033</td>\n",
       "      <td>10.159067</td>\n",
       "      <td>12.875869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          country  ifscode    time  gdpwgt  lc  le       llf  lulcm  lur  \\\n",
       "0   United States      111  1950Q1     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "7   United States      111  1950Q2     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "14  United States      111  1950Q3     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "21  United States      111  1950Q4     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "28  United States      111  1951Q1     NaN NaN NaN  0.062002    NaN  NaN   \n",
       "\n",
       "    ncg_r  ...  pcpi_sa  pcpi_sa_pcha  pcpi_sa_pchy      pppgdp  pppsh  \\\n",
       "0     NaN  ...      NaN           NaN           NaN  301.782705    NaN   \n",
       "7     NaN  ...      NaN           NaN           NaN  301.782705    NaN   \n",
       "14    NaN  ...      NaN           NaN           NaN  301.782705    NaN   \n",
       "21    NaN  ...      NaN           NaN           NaN  301.782705    NaN   \n",
       "28    NaN  ...      NaN           NaN           NaN  348.993057    NaN   \n",
       "\n",
       "        pppwgt    tmgwgt      tmwgt     txgwgt      txwgt  \n",
       "0   301.782705       NaN        NaN        NaN        NaN  \n",
       "7   301.782705       NaN        NaN        NaN        NaN  \n",
       "14  301.782705       NaN        NaN        NaN        NaN  \n",
       "21  301.782705       NaN        NaN        NaN        NaN  \n",
       "28  348.993057  8.709073  11.508033  10.159067  12.875869  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter data by country\n",
    "\n",
    "dataset = gdp_quarterly_q[gdp_quarterly_q['country'].isin(selected_countries)]\n",
    "\n",
    "dataset = pd.DataFrame(dataset)\n",
    "# print (\"#\", \"column name\", \"missing values\")\n",
    "# for i in range(len(dataset.columns)):\n",
    "#     print(i, dataset.columns[i], \" \", dataset.iloc[i].isnull().count())\n",
    "\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10628.0</td>\n",
       "      <td>153.952333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.571</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>2565.6</td>\n",
       "      <td>0.469925</td>\n",
       "      <td>12729.7</td>\n",
       "      <td>1.139334</td>\n",
       "      <td>...</td>\n",
       "      <td>247.273333</td>\n",
       "      <td>3.141889</td>\n",
       "      <td>2.109865</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>15.284951</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>2221.075</td>\n",
       "      <td>2739.425</td>\n",
       "      <td>1444.025</td>\n",
       "      <td>2220.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10786.0</td>\n",
       "      <td>154.951667</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>111.839</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>2578.3</td>\n",
       "      <td>0.495011</td>\n",
       "      <td>12782.9</td>\n",
       "      <td>0.417920</td>\n",
       "      <td>...</td>\n",
       "      <td>249.250333</td>\n",
       "      <td>3.236639</td>\n",
       "      <td>2.222997</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10876.1</td>\n",
       "      <td>155.449000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.132</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>0.531358</td>\n",
       "      <td>12909.2</td>\n",
       "      <td>0.988039</td>\n",
       "      <td>...</td>\n",
       "      <td>250.578667</td>\n",
       "      <td>2.148827</td>\n",
       "      <td>2.668825</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10994.3</td>\n",
       "      <td>155.879000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.681</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.540123</td>\n",
       "      <td>13019.8</td>\n",
       "      <td>0.856753</td>\n",
       "      <td>...</td>\n",
       "      <td>251.828667</td>\n",
       "      <td>2.010362</td>\n",
       "      <td>2.632912</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>11057.4</td>\n",
       "      <td>156.776667</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>111.370</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>2605.7</td>\n",
       "      <td>-0.011512</td>\n",
       "      <td>13066.3</td>\n",
       "      <td>0.357148</td>\n",
       "      <td>...</td>\n",
       "      <td>252.759000</td>\n",
       "      <td>1.485933</td>\n",
       "      <td>2.218463</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gdpwgt       lc          le       llf    lulcm       lur   ncg_r  \\\n",
       "1897  18155.700000  10628.0  153.952333  0.160311  110.571  4.133333  2565.6   \n",
       "1904  18819.741667  10786.0  154.951667  0.162071  111.839  4.066667  2578.3   \n",
       "1911  18819.741667  10876.1  155.449000  0.162071  110.132  3.900000  2592.0   \n",
       "1918  18819.741667  10994.3  155.879000  0.162071  110.681  3.800000  2606.0   \n",
       "1925  18819.741667  11057.4  156.776667  0.162071  111.370  3.800000  2605.7   \n",
       "\n",
       "      ncg_rpch    ncp_r  ncp_rpch  ...     pcpi_sa  pcpi_sa_pcha  \\\n",
       "1897  0.469925  12729.7  1.139334  ...  247.273333      3.141889   \n",
       "1904  0.495011  12782.9  0.417920  ...  249.250333      3.236639   \n",
       "1911  0.531358  12909.2  0.988039  ...  250.578667      2.148827   \n",
       "1918  0.540123  13019.8  0.856753  ...  251.828667      2.010362   \n",
       "1925 -0.011512  13066.3  0.357148  ...  252.759000      1.485933   \n",
       "\n",
       "      pcpi_sa_pchy    pppgdp      pppsh    pppwgt    tmgwgt     tmwgt  \\\n",
       "1897      2.109865  19519.40  15.284951  19519.40  2221.075  2739.425   \n",
       "1904      2.222997  20580.25  15.195560  20580.25  2379.800  2932.075   \n",
       "1911      2.668825  20580.25  15.195560  20580.25  2379.800  2932.075   \n",
       "1918      2.632912  20580.25  15.195560  20580.25  2379.800  2932.075   \n",
       "1925      2.218463  20580.25  15.195560  20580.25  2379.800  2932.075   \n",
       "\n",
       "        txgwgt     txwgt  \n",
       "1897  1444.025  2220.625  \n",
       "1904  1538.375  2356.725  \n",
       "1911  1538.375  2356.725  \n",
       "1918  1538.375  2356.725  \n",
       "1925  1538.375  2356.725  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_input = dataset\n",
    "dataset_input = dataset_input.drop(columns = ['country', 'ifscode', 'time', 'ngdp_r_sa_pcha', 'ngdp_r_sa_pchy', 'ngdp_dpchy'])\n",
    "dataset_input.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "      <th>time</th>\n",
       "      <th>ngdp_r_sa_pcha</th>\n",
       "      <th>1_step_ahead_ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10456.7</td>\n",
       "      <td>153.815333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.185</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2553.6</td>\n",
       "      <td>0.145104</td>\n",
       "      <td>12586.3</td>\n",
       "      <td>0.586595</td>\n",
       "      <td>...</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>15.284951</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>2221.075</td>\n",
       "      <td>2739.425</td>\n",
       "      <td>1444.025</td>\n",
       "      <td>2220.625</td>\n",
       "      <td>2017Q3</td>\n",
       "      <td>3.202964</td>\n",
       "      <td>3.545494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10628.0</td>\n",
       "      <td>153.952333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.571</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>2565.6</td>\n",
       "      <td>0.469925</td>\n",
       "      <td>12729.7</td>\n",
       "      <td>1.139334</td>\n",
       "      <td>...</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>15.284951</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>2221.075</td>\n",
       "      <td>2739.425</td>\n",
       "      <td>1444.025</td>\n",
       "      <td>2220.625</td>\n",
       "      <td>2017Q4</td>\n",
       "      <td>3.545494</td>\n",
       "      <td>2.552107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10786.0</td>\n",
       "      <td>154.951667</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>111.839</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>2578.3</td>\n",
       "      <td>0.495011</td>\n",
       "      <td>12782.9</td>\n",
       "      <td>0.417920</td>\n",
       "      <td>...</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>2018Q1</td>\n",
       "      <td>2.552107</td>\n",
       "      <td>3.512025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10876.1</td>\n",
       "      <td>155.449000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.132</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>0.531358</td>\n",
       "      <td>12909.2</td>\n",
       "      <td>0.988039</td>\n",
       "      <td>...</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>2018Q2</td>\n",
       "      <td>3.512025</td>\n",
       "      <td>2.926498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10994.3</td>\n",
       "      <td>155.879000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.681</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.540123</td>\n",
       "      <td>13019.8</td>\n",
       "      <td>0.856753</td>\n",
       "      <td>...</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>2018Q3</td>\n",
       "      <td>2.926498</td>\n",
       "      <td>1.089155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gdpwgt       lc          le       llf    lulcm       lur   ncg_r  \\\n",
       "1890  18155.700000  10456.7  153.815333  0.160311  110.185  4.300000  2553.6   \n",
       "1897  18155.700000  10628.0  153.952333  0.160311  110.571  4.133333  2565.6   \n",
       "1904  18819.741667  10786.0  154.951667  0.162071  111.839  4.066667  2578.3   \n",
       "1911  18819.741667  10876.1  155.449000  0.162071  110.132  3.900000  2592.0   \n",
       "1918  18819.741667  10994.3  155.879000  0.162071  110.681  3.800000  2606.0   \n",
       "\n",
       "      ncg_rpch    ncp_r  ncp_rpch  ...    pppgdp      pppsh    pppwgt  \\\n",
       "1890  0.145104  12586.3  0.586595  ...  19519.40  15.284951  19519.40   \n",
       "1897  0.469925  12729.7  1.139334  ...  19519.40  15.284951  19519.40   \n",
       "1904  0.495011  12782.9  0.417920  ...  20580.25  15.195560  20580.25   \n",
       "1911  0.531358  12909.2  0.988039  ...  20580.25  15.195560  20580.25   \n",
       "1918  0.540123  13019.8  0.856753  ...  20580.25  15.195560  20580.25   \n",
       "\n",
       "        tmgwgt     tmwgt    txgwgt     txwgt    time  ngdp_r_sa_pcha  \\\n",
       "1890  2221.075  2739.425  1444.025  2220.625  2017Q3        3.202964   \n",
       "1897  2221.075  2739.425  1444.025  2220.625  2017Q4        3.545494   \n",
       "1904  2379.800  2932.075  1538.375  2356.725  2018Q1        2.552107   \n",
       "1911  2379.800  2932.075  1538.375  2356.725  2018Q2        3.512025   \n",
       "1918  2379.800  2932.075  1538.375  2356.725  2018Q3        2.926498   \n",
       "\n",
       "      1_step_ahead_ngdp_r_sa_pcha  \n",
       "1890                     3.545494  \n",
       "1897                     2.552107  \n",
       "1904                     3.512025  \n",
       "1911                     2.926498  \n",
       "1918                     1.089155  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outcome vaiable (Column Name) = ngdp_r_sa_pcha\n",
    "\n",
    "outcome_variable = \"ngdp_r_sa_pcha\"\n",
    "predicted_variable = \"1_step_ahead_\" + outcome_variable\n",
    "\n",
    "dataset_1 = dataset_input\n",
    "dataset_1[\"time\"] = dataset[\"time\"]\n",
    "# dataset_1[num_cols] = dataset[num_cols]\n",
    "dataset_1[outcome_variable] = dataset[outcome_variable]\n",
    "\n",
    "dataset_1[predicted_variable] = dataset_1[outcome_variable].shift(-1)\n",
    "\n",
    "# # Source: https://stackoverflow.com/questions/20095673/shift-column-in-pandas-dataframe-up-by-one\n",
    "\n",
    "dataset_1 = dataset_1[:-1] \n",
    "\n",
    "dataset_1.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>1_step_ahead_ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>2017Q3</td>\n",
       "      <td>3.545494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>2017Q4</td>\n",
       "      <td>2.552107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>3.512025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>2018Q2</td>\n",
       "      <td>2.926498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>2018Q3</td>\n",
       "      <td>1.089155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        time  1_step_ahead_ngdp_r_sa_pcha\n",
       "1890  2017Q3                     3.545494\n",
       "1897  2017Q4                     2.552107\n",
       "1904  2018Q1                     3.512025\n",
       "1911  2018Q2                     2.926498\n",
       "1918  2018Q3                     1.089155"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output Columns\n",
    "\n",
    "# ngdp_r_sa_pcha: WEO: Gross domestic product, constant prices, seasonally adjusted, quarter-over-quarter percent change, annualized (Percent, Units).\n",
    "\n",
    "dataset_Y = dataset_1[[\"time\", predicted_variable]]\n",
    "dataset_Y.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding the lagged variables to the input dataset: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>pcpi_sa_pcha</th>\n",
       "      <th>pcpi_sa_pchy</th>\n",
       "      <th>pppgdp</th>\n",
       "      <th>pppsh</th>\n",
       "      <th>pppwgt</th>\n",
       "      <th>tmgwgt</th>\n",
       "      <th>tmwgt</th>\n",
       "      <th>txgwgt</th>\n",
       "      <th>txwgt</th>\n",
       "      <th>ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10456.7</td>\n",
       "      <td>153.815333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.185</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2553.6</td>\n",
       "      <td>0.145104</td>\n",
       "      <td>12586.3</td>\n",
       "      <td>0.586595</td>\n",
       "      <td>...</td>\n",
       "      <td>2.153214</td>\n",
       "      <td>1.981427</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>15.284951</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>2221.075</td>\n",
       "      <td>2739.425</td>\n",
       "      <td>1444.025</td>\n",
       "      <td>2220.625</td>\n",
       "      <td>3.202964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10628.0</td>\n",
       "      <td>153.952333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.571</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>2565.6</td>\n",
       "      <td>0.469925</td>\n",
       "      <td>12729.7</td>\n",
       "      <td>1.139334</td>\n",
       "      <td>...</td>\n",
       "      <td>3.141889</td>\n",
       "      <td>2.109865</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>15.284951</td>\n",
       "      <td>19519.40</td>\n",
       "      <td>2221.075</td>\n",
       "      <td>2739.425</td>\n",
       "      <td>1444.025</td>\n",
       "      <td>2220.625</td>\n",
       "      <td>3.545494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10786.0</td>\n",
       "      <td>154.951667</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>111.839</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>2578.3</td>\n",
       "      <td>0.495011</td>\n",
       "      <td>12782.9</td>\n",
       "      <td>0.417920</td>\n",
       "      <td>...</td>\n",
       "      <td>3.236639</td>\n",
       "      <td>2.222997</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>2.552107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10876.1</td>\n",
       "      <td>155.449000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.132</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>0.531358</td>\n",
       "      <td>12909.2</td>\n",
       "      <td>0.988039</td>\n",
       "      <td>...</td>\n",
       "      <td>2.148827</td>\n",
       "      <td>2.668825</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>3.512025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10994.3</td>\n",
       "      <td>155.879000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.681</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.540123</td>\n",
       "      <td>13019.8</td>\n",
       "      <td>0.856753</td>\n",
       "      <td>...</td>\n",
       "      <td>2.010362</td>\n",
       "      <td>2.632912</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>15.195560</td>\n",
       "      <td>20580.25</td>\n",
       "      <td>2379.800</td>\n",
       "      <td>2932.075</td>\n",
       "      <td>1538.375</td>\n",
       "      <td>2356.725</td>\n",
       "      <td>2.926498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gdpwgt       lc          le       llf    lulcm       lur   ncg_r  \\\n",
       "1890  18155.700000  10456.7  153.815333  0.160311  110.185  4.300000  2553.6   \n",
       "1897  18155.700000  10628.0  153.952333  0.160311  110.571  4.133333  2565.6   \n",
       "1904  18819.741667  10786.0  154.951667  0.162071  111.839  4.066667  2578.3   \n",
       "1911  18819.741667  10876.1  155.449000  0.162071  110.132  3.900000  2592.0   \n",
       "1918  18819.741667  10994.3  155.879000  0.162071  110.681  3.800000  2606.0   \n",
       "\n",
       "      ncg_rpch    ncp_r  ncp_rpch  ...  pcpi_sa_pcha  pcpi_sa_pchy    pppgdp  \\\n",
       "1890  0.145104  12586.3  0.586595  ...      2.153214      1.981427  19519.40   \n",
       "1897  0.469925  12729.7  1.139334  ...      3.141889      2.109865  19519.40   \n",
       "1904  0.495011  12782.9  0.417920  ...      3.236639      2.222997  20580.25   \n",
       "1911  0.531358  12909.2  0.988039  ...      2.148827      2.668825  20580.25   \n",
       "1918  0.540123  13019.8  0.856753  ...      2.010362      2.632912  20580.25   \n",
       "\n",
       "          pppsh    pppwgt    tmgwgt     tmwgt    txgwgt     txwgt  \\\n",
       "1890  15.284951  19519.40  2221.075  2739.425  1444.025  2220.625   \n",
       "1897  15.284951  19519.40  2221.075  2739.425  1444.025  2220.625   \n",
       "1904  15.195560  20580.25  2379.800  2932.075  1538.375  2356.725   \n",
       "1911  15.195560  20580.25  2379.800  2932.075  1538.375  2356.725   \n",
       "1918  15.195560  20580.25  2379.800  2932.075  1538.375  2356.725   \n",
       "\n",
       "      ngdp_r_sa_pcha  \n",
       "1890        3.202964  \n",
       "1897        3.545494  \n",
       "1904        2.552107  \n",
       "1911        3.512025  \n",
       "1918        2.926498  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding the lagged variables to the input dataset: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>ngdp_r_sa_pcha-1</th>\n",
       "      <th>ngdp_r_sa_pcha-2</th>\n",
       "      <th>ngdp_r_sa_pcha-3</th>\n",
       "      <th>ngdp_r_sa_pcha-4</th>\n",
       "      <th>ngdp_r_sa_pcha-5</th>\n",
       "      <th>ngdp_r_sa_pcha-6</th>\n",
       "      <th>ngdp_r_sa_pcha-7</th>\n",
       "      <th>ngdp_r_sa_pcha-8</th>\n",
       "      <th>ngdp_r_sa_pcha-9</th>\n",
       "      <th>ngdp_r_sa_pcha-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10456.7</td>\n",
       "      <td>153.815333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.185</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2553.6</td>\n",
       "      <td>0.145104</td>\n",
       "      <td>12586.3</td>\n",
       "      <td>0.586595</td>\n",
       "      <td>...</td>\n",
       "      <td>2.152692</td>\n",
       "      <td>2.288202</td>\n",
       "      <td>2.024769</td>\n",
       "      <td>2.187866</td>\n",
       "      <td>1.895214</td>\n",
       "      <td>2.027962</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>1.327969</td>\n",
       "      <td>2.998835</td>\n",
       "      <td>3.177823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10628.0</td>\n",
       "      <td>153.952333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.571</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>2565.6</td>\n",
       "      <td>0.469925</td>\n",
       "      <td>12729.7</td>\n",
       "      <td>1.139334</td>\n",
       "      <td>...</td>\n",
       "      <td>3.202964</td>\n",
       "      <td>2.152692</td>\n",
       "      <td>2.288202</td>\n",
       "      <td>2.024769</td>\n",
       "      <td>2.187866</td>\n",
       "      <td>1.895214</td>\n",
       "      <td>2.027962</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>1.327969</td>\n",
       "      <td>2.998835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10786.0</td>\n",
       "      <td>154.951667</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>111.839</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>2578.3</td>\n",
       "      <td>0.495011</td>\n",
       "      <td>12782.9</td>\n",
       "      <td>0.417920</td>\n",
       "      <td>...</td>\n",
       "      <td>3.545494</td>\n",
       "      <td>3.202964</td>\n",
       "      <td>2.152692</td>\n",
       "      <td>2.288202</td>\n",
       "      <td>2.024769</td>\n",
       "      <td>2.187866</td>\n",
       "      <td>1.895214</td>\n",
       "      <td>2.027962</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>1.327969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10876.1</td>\n",
       "      <td>155.449000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.132</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>0.531358</td>\n",
       "      <td>12909.2</td>\n",
       "      <td>0.988039</td>\n",
       "      <td>...</td>\n",
       "      <td>2.552107</td>\n",
       "      <td>3.545494</td>\n",
       "      <td>3.202964</td>\n",
       "      <td>2.152692</td>\n",
       "      <td>2.288202</td>\n",
       "      <td>2.024769</td>\n",
       "      <td>2.187866</td>\n",
       "      <td>1.895214</td>\n",
       "      <td>2.027962</td>\n",
       "      <td>0.130624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10994.3</td>\n",
       "      <td>155.879000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.681</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.540123</td>\n",
       "      <td>13019.8</td>\n",
       "      <td>0.856753</td>\n",
       "      <td>...</td>\n",
       "      <td>3.512025</td>\n",
       "      <td>2.552107</td>\n",
       "      <td>3.545494</td>\n",
       "      <td>3.202964</td>\n",
       "      <td>2.152692</td>\n",
       "      <td>2.288202</td>\n",
       "      <td>2.024769</td>\n",
       "      <td>2.187866</td>\n",
       "      <td>1.895214</td>\n",
       "      <td>2.027962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 671 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gdpwgt       lc          le       llf    lulcm       lur   ncg_r  \\\n",
       "1890  18155.700000  10456.7  153.815333  0.160311  110.185  4.300000  2553.6   \n",
       "1897  18155.700000  10628.0  153.952333  0.160311  110.571  4.133333  2565.6   \n",
       "1904  18819.741667  10786.0  154.951667  0.162071  111.839  4.066667  2578.3   \n",
       "1911  18819.741667  10876.1  155.449000  0.162071  110.132  3.900000  2592.0   \n",
       "1918  18819.741667  10994.3  155.879000  0.162071  110.681  3.800000  2606.0   \n",
       "\n",
       "      ncg_rpch    ncp_r  ncp_rpch  ...  ngdp_r_sa_pcha-1  ngdp_r_sa_pcha-2  \\\n",
       "1890  0.145104  12586.3  0.586595  ...          2.152692          2.288202   \n",
       "1897  0.469925  12729.7  1.139334  ...          3.202964          2.152692   \n",
       "1904  0.495011  12782.9  0.417920  ...          3.545494          3.202964   \n",
       "1911  0.531358  12909.2  0.988039  ...          2.552107          3.545494   \n",
       "1918  0.540123  13019.8  0.856753  ...          3.512025          2.552107   \n",
       "\n",
       "      ngdp_r_sa_pcha-3  ngdp_r_sa_pcha-4  ngdp_r_sa_pcha-5  ngdp_r_sa_pcha-6  \\\n",
       "1890          2.024769          2.187866          1.895214          2.027962   \n",
       "1897          2.288202          2.024769          2.187866          1.895214   \n",
       "1904          2.152692          2.288202          2.024769          2.187866   \n",
       "1911          3.202964          2.152692          2.288202          2.024769   \n",
       "1918          3.545494          3.202964          2.152692          2.288202   \n",
       "\n",
       "      ngdp_r_sa_pcha-7  ngdp_r_sa_pcha-8  ngdp_r_sa_pcha-9  ngdp_r_sa_pcha-10  \n",
       "1890          0.130624          1.327969          2.998835           3.177823  \n",
       "1897          2.027962          0.130624          1.327969           2.998835  \n",
       "1904          1.895214          2.027962          0.130624           1.327969  \n",
       "1911          2.187866          1.895214          2.027962           0.130624  \n",
       "1918          2.024769          2.187866          1.895214           2.027962  \n",
       "\n",
       "[5 rows x 671 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Window size and crearting the lagged columns\n",
    "\n",
    "#  Using a lag = 0 for identifying initial variable importance by fitting a randowm forest \n",
    "# window size \n",
    "lag = 10\n",
    "dataset_input_l = dataset_1\n",
    "\n",
    "# Drop the 1) preducted outcome variable and 2) time variable \n",
    "\n",
    "dataset_input_l = dataset_input_l.drop(columns = [\"time\", predicted_variable])\n",
    "\n",
    "print(\"Before adding the lagged variables to the input dataset: \")\n",
    "dataset_input_l.tail(5)\n",
    "\n",
    "# Lagging each column in num_columns by the entire range of lag factors\n",
    "\n",
    "for j in dataset_input_l.columns:\n",
    "    for i in range(1, (lag + 1), 1):\n",
    "        new_col = str(j)+\"-\"+str(i)\n",
    "        dataset_input_l[str(new_col)] = dataset_input_l[str(j)].shift(i)\n",
    "    \n",
    "print(\"After adding the lagged variables to the input dataset: \")\n",
    "dataset_input_l.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>1_step_ahead_ngdp_r_sa_pcha</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>ngdp_r_sa_pcha-1</th>\n",
       "      <th>ngdp_r_sa_pcha-2</th>\n",
       "      <th>ngdp_r_sa_pcha-3</th>\n",
       "      <th>ngdp_r_sa_pcha-4</th>\n",
       "      <th>ngdp_r_sa_pcha-5</th>\n",
       "      <th>ngdp_r_sa_pcha-6</th>\n",
       "      <th>ngdp_r_sa_pcha-7</th>\n",
       "      <th>ngdp_r_sa_pcha-8</th>\n",
       "      <th>ngdp_r_sa_pcha-9</th>\n",
       "      <th>ngdp_r_sa_pcha-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1950Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1950Q2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1950Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1950Q4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1951Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 673 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      time  1_step_ahead_ngdp_r_sa_pcha  gdpwgt  lc  le       llf  lulcm  lur  \\\n",
       "0   1950Q1                          NaN     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "7   1950Q2                          NaN     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "14  1950Q3                          NaN     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "21  1950Q4                          NaN     NaN NaN NaN  0.062128    NaN  NaN   \n",
       "28  1951Q1                          NaN     NaN NaN NaN  0.062002    NaN  NaN   \n",
       "\n",
       "    ncg_r  ncg_rpch  ...  ngdp_r_sa_pcha-1  ngdp_r_sa_pcha-2  \\\n",
       "0     NaN       NaN  ...               NaN               NaN   \n",
       "7     NaN       NaN  ...               NaN               NaN   \n",
       "14    NaN       NaN  ...               NaN               NaN   \n",
       "21    NaN       NaN  ...               NaN               NaN   \n",
       "28    NaN       NaN  ...               NaN               NaN   \n",
       "\n",
       "    ngdp_r_sa_pcha-3  ngdp_r_sa_pcha-4  ngdp_r_sa_pcha-5  ngdp_r_sa_pcha-6  \\\n",
       "0                NaN               NaN               NaN               NaN   \n",
       "7                NaN               NaN               NaN               NaN   \n",
       "14               NaN               NaN               NaN               NaN   \n",
       "21               NaN               NaN               NaN               NaN   \n",
       "28               NaN               NaN               NaN               NaN   \n",
       "\n",
       "    ngdp_r_sa_pcha-7  ngdp_r_sa_pcha-8  ngdp_r_sa_pcha-9  ngdp_r_sa_pcha-10  \n",
       "0                NaN               NaN               NaN                NaN  \n",
       "7                NaN               NaN               NaN                NaN  \n",
       "14               NaN               NaN               NaN                NaN  \n",
       "21               NaN               NaN               NaN                NaN  \n",
       "28               NaN               NaN               NaN                NaN  \n",
       "\n",
       "[5 rows x 673 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(275, 673)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns names:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['time', '1_step_ahead_ngdp_r_sa_pcha', 'gdpwgt', 'lc', 'le', 'llf',\n",
       "       'lulcm', 'lur', 'ncg_r', 'ncg_rpch',\n",
       "       ...\n",
       "       'ngdp_r_sa_pcha-1', 'ngdp_r_sa_pcha-2', 'ngdp_r_sa_pcha-3',\n",
       "       'ngdp_r_sa_pcha-4', 'ngdp_r_sa_pcha-5', 'ngdp_r_sa_pcha-6',\n",
       "       'ngdp_r_sa_pcha-7', 'ngdp_r_sa_pcha-8', 'ngdp_r_sa_pcha-9',\n",
       "       'ngdp_r_sa_pcha-10'],\n",
       "      dtype='object', length=673)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining Input and Output Values\n",
    "\n",
    "# X1 = dataset_input\n",
    "# X = pd.concat([X1, X2, dataset_Y], axis=1)\n",
    "X = pd.concat([dataset_Y, dataset_input_l], axis=1)\n",
    "X.head(5)\n",
    "X.shape\n",
    "\n",
    "print(\"\\nColumns names:\\n\")\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After dropping rows with missing data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(145, 673)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>1_step_ahead_ngdp_r_sa_pcha</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>ngdp_r_sa_pcha-1</th>\n",
       "      <th>ngdp_r_sa_pcha-2</th>\n",
       "      <th>ngdp_r_sa_pcha-3</th>\n",
       "      <th>ngdp_r_sa_pcha-4</th>\n",
       "      <th>ngdp_r_sa_pcha-5</th>\n",
       "      <th>ngdp_r_sa_pcha-6</th>\n",
       "      <th>ngdp_r_sa_pcha-7</th>\n",
       "      <th>ngdp_r_sa_pcha-8</th>\n",
       "      <th>ngdp_r_sa_pcha-9</th>\n",
       "      <th>ngdp_r_sa_pcha-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>1982Q3</td>\n",
       "      <td>0.158923</td>\n",
       "      <td>2897.225</td>\n",
       "      <td>1904.8</td>\n",
       "      <td>99.543333</td>\n",
       "      <td>0.110231</td>\n",
       "      <td>92.663</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>1515.5</td>\n",
       "      <td>1.026598</td>\n",
       "      <td>...</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "      <td>-0.476985</td>\n",
       "      <td>-7.985864</td>\n",
       "      <td>1.261758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>1982Q4</td>\n",
       "      <td>5.373663</td>\n",
       "      <td>2897.225</td>\n",
       "      <td>1918.1</td>\n",
       "      <td>99.119667</td>\n",
       "      <td>0.110231</td>\n",
       "      <td>93.691</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>1532.7</td>\n",
       "      <td>1.134939</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "      <td>-0.476985</td>\n",
       "      <td>-7.985864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>1983Q1</td>\n",
       "      <td>9.421777</td>\n",
       "      <td>3136.050</td>\n",
       "      <td>1947.2</td>\n",
       "      <td>99.143000</td>\n",
       "      <td>0.111528</td>\n",
       "      <td>92.971</td>\n",
       "      <td>10.366667</td>\n",
       "      <td>1549.5</td>\n",
       "      <td>1.096105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158923</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "      <td>-0.476985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>1983Q2</td>\n",
       "      <td>8.238399</td>\n",
       "      <td>3136.050</td>\n",
       "      <td>1986.3</td>\n",
       "      <td>99.945000</td>\n",
       "      <td>0.111528</td>\n",
       "      <td>92.457</td>\n",
       "      <td>10.133333</td>\n",
       "      <td>1561.6</td>\n",
       "      <td>0.780897</td>\n",
       "      <td>...</td>\n",
       "      <td>5.373663</td>\n",
       "      <td>0.158923</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>1983Q3</td>\n",
       "      <td>8.609839</td>\n",
       "      <td>3136.050</td>\n",
       "      <td>2029.6</td>\n",
       "      <td>101.610667</td>\n",
       "      <td>0.111528</td>\n",
       "      <td>91.532</td>\n",
       "      <td>9.366667</td>\n",
       "      <td>1580.2</td>\n",
       "      <td>1.191086</td>\n",
       "      <td>...</td>\n",
       "      <td>9.421777</td>\n",
       "      <td>5.373663</td>\n",
       "      <td>0.158923</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 673 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       time  1_step_ahead_ngdp_r_sa_pcha    gdpwgt      lc          le  \\\n",
       "910  1982Q3                     0.158923  2897.225  1904.8   99.543333   \n",
       "917  1982Q4                     5.373663  2897.225  1918.1   99.119667   \n",
       "924  1983Q1                     9.421777  3136.050  1947.2   99.143000   \n",
       "931  1983Q2                     8.238399  3136.050  1986.3   99.945000   \n",
       "938  1983Q3                     8.609839  3136.050  2029.6  101.610667   \n",
       "\n",
       "          llf   lulcm        lur   ncg_r  ncg_rpch  ...  ngdp_r_sa_pcha-1  \\\n",
       "910  0.110231  92.663   9.900000  1515.5  1.026598  ...          1.837425   \n",
       "917  0.110231  93.691  10.666667  1532.7  1.134939  ...         -1.520719   \n",
       "924  0.111528  92.971  10.366667  1549.5  1.096105  ...          0.158923   \n",
       "931  0.111528  92.457  10.133333  1561.6  0.780897  ...          5.373663   \n",
       "938  0.111528  91.532   9.366667  1580.2  1.191086  ...          9.421777   \n",
       "\n",
       "     ngdp_r_sa_pcha-2  ngdp_r_sa_pcha-3  ngdp_r_sa_pcha-4  ngdp_r_sa_pcha-5  \\\n",
       "910         -6.069358         -4.285831          4.872232         -2.926867   \n",
       "917          1.837425         -6.069358         -4.285831          4.872232   \n",
       "924         -1.520719          1.837425         -6.069358         -4.285831   \n",
       "931          0.158923         -1.520719          1.837425         -6.069358   \n",
       "938          5.373663          0.158923         -1.520719          1.837425   \n",
       "\n",
       "     ngdp_r_sa_pcha-6  ngdp_r_sa_pcha-7  ngdp_r_sa_pcha-8  ngdp_r_sa_pcha-9  \\\n",
       "910          8.070747          7.668385         -0.476985         -7.985864   \n",
       "917         -2.926867          8.070747          7.668385         -0.476985   \n",
       "924          4.872232         -2.926867          8.070747          7.668385   \n",
       "931         -4.285831          4.872232         -2.926867          8.070747   \n",
       "938         -6.069358         -4.285831          4.872232         -2.926867   \n",
       "\n",
       "     ngdp_r_sa_pcha-10  \n",
       "910           1.261758  \n",
       "917          -7.985864  \n",
       "924          -0.476985  \n",
       "931           7.668385  \n",
       "938           8.070747  \n",
       "\n",
       "[5 rows x 673 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>1_step_ahead_ngdp_r_sa_pcha</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>ngdp_r_sa_pcha-1</th>\n",
       "      <th>ngdp_r_sa_pcha-2</th>\n",
       "      <th>ngdp_r_sa_pcha-3</th>\n",
       "      <th>ngdp_r_sa_pcha-4</th>\n",
       "      <th>ngdp_r_sa_pcha-5</th>\n",
       "      <th>ngdp_r_sa_pcha-6</th>\n",
       "      <th>ngdp_r_sa_pcha-7</th>\n",
       "      <th>ngdp_r_sa_pcha-8</th>\n",
       "      <th>ngdp_r_sa_pcha-9</th>\n",
       "      <th>ngdp_r_sa_pcha-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>2017Q3</td>\n",
       "      <td>3.545494</td>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10456.7</td>\n",
       "      <td>153.815333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.185</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2553.6</td>\n",
       "      <td>0.145104</td>\n",
       "      <td>...</td>\n",
       "      <td>2.152692</td>\n",
       "      <td>2.288202</td>\n",
       "      <td>2.024769</td>\n",
       "      <td>2.187866</td>\n",
       "      <td>1.895214</td>\n",
       "      <td>2.027962</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>1.327969</td>\n",
       "      <td>2.998835</td>\n",
       "      <td>3.177823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>2017Q4</td>\n",
       "      <td>2.552107</td>\n",
       "      <td>18155.700000</td>\n",
       "      <td>10628.0</td>\n",
       "      <td>153.952333</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>110.571</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>2565.6</td>\n",
       "      <td>0.469925</td>\n",
       "      <td>...</td>\n",
       "      <td>3.202964</td>\n",
       "      <td>2.152692</td>\n",
       "      <td>2.288202</td>\n",
       "      <td>2.024769</td>\n",
       "      <td>2.187866</td>\n",
       "      <td>1.895214</td>\n",
       "      <td>2.027962</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>1.327969</td>\n",
       "      <td>2.998835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>3.512025</td>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10786.0</td>\n",
       "      <td>154.951667</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>111.839</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>2578.3</td>\n",
       "      <td>0.495011</td>\n",
       "      <td>...</td>\n",
       "      <td>3.545494</td>\n",
       "      <td>3.202964</td>\n",
       "      <td>2.152692</td>\n",
       "      <td>2.288202</td>\n",
       "      <td>2.024769</td>\n",
       "      <td>2.187866</td>\n",
       "      <td>1.895214</td>\n",
       "      <td>2.027962</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>1.327969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>2018Q2</td>\n",
       "      <td>2.926498</td>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10876.1</td>\n",
       "      <td>155.449000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.132</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>0.531358</td>\n",
       "      <td>...</td>\n",
       "      <td>2.552107</td>\n",
       "      <td>3.545494</td>\n",
       "      <td>3.202964</td>\n",
       "      <td>2.152692</td>\n",
       "      <td>2.288202</td>\n",
       "      <td>2.024769</td>\n",
       "      <td>2.187866</td>\n",
       "      <td>1.895214</td>\n",
       "      <td>2.027962</td>\n",
       "      <td>0.130624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>2018Q3</td>\n",
       "      <td>1.089155</td>\n",
       "      <td>18819.741667</td>\n",
       "      <td>10994.3</td>\n",
       "      <td>155.879000</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>110.681</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.540123</td>\n",
       "      <td>...</td>\n",
       "      <td>3.512025</td>\n",
       "      <td>2.552107</td>\n",
       "      <td>3.545494</td>\n",
       "      <td>3.202964</td>\n",
       "      <td>2.152692</td>\n",
       "      <td>2.288202</td>\n",
       "      <td>2.024769</td>\n",
       "      <td>2.187866</td>\n",
       "      <td>1.895214</td>\n",
       "      <td>2.027962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 673 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time  1_step_ahead_ngdp_r_sa_pcha        gdpwgt       lc          le  \\\n",
       "1890  2017Q3                     3.545494  18155.700000  10456.7  153.815333   \n",
       "1897  2017Q4                     2.552107  18155.700000  10628.0  153.952333   \n",
       "1904  2018Q1                     3.512025  18819.741667  10786.0  154.951667   \n",
       "1911  2018Q2                     2.926498  18819.741667  10876.1  155.449000   \n",
       "1918  2018Q3                     1.089155  18819.741667  10994.3  155.879000   \n",
       "\n",
       "           llf    lulcm       lur   ncg_r  ncg_rpch  ...  ngdp_r_sa_pcha-1  \\\n",
       "1890  0.160311  110.185  4.300000  2553.6  0.145104  ...          2.152692   \n",
       "1897  0.160311  110.571  4.133333  2565.6  0.469925  ...          3.202964   \n",
       "1904  0.162071  111.839  4.066667  2578.3  0.495011  ...          3.545494   \n",
       "1911  0.162071  110.132  3.900000  2592.0  0.531358  ...          2.552107   \n",
       "1918  0.162071  110.681  3.800000  2606.0  0.540123  ...          3.512025   \n",
       "\n",
       "      ngdp_r_sa_pcha-2  ngdp_r_sa_pcha-3  ngdp_r_sa_pcha-4  ngdp_r_sa_pcha-5  \\\n",
       "1890          2.288202          2.024769          2.187866          1.895214   \n",
       "1897          2.152692          2.288202          2.024769          2.187866   \n",
       "1904          3.202964          2.152692          2.288202          2.024769   \n",
       "1911          3.545494          3.202964          2.152692          2.288202   \n",
       "1918          2.552107          3.545494          3.202964          2.152692   \n",
       "\n",
       "      ngdp_r_sa_pcha-6  ngdp_r_sa_pcha-7  ngdp_r_sa_pcha-8  ngdp_r_sa_pcha-9  \\\n",
       "1890          2.027962          0.130624          1.327969          2.998835   \n",
       "1897          1.895214          2.027962          0.130624          1.327969   \n",
       "1904          2.187866          1.895214          2.027962          0.130624   \n",
       "1911          2.024769          2.187866          1.895214          2.027962   \n",
       "1918          2.288202          2.024769          2.187866          1.895214   \n",
       "\n",
       "      ngdp_r_sa_pcha-10  \n",
       "1890           3.177823  \n",
       "1897           2.998835  \n",
       "1904           1.327969  \n",
       "1911           0.130624  \n",
       "1918           2.027962  \n",
       "\n",
       "[5 rows x 673 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping all rows with missing data\n",
    "print(\"\\nAfter dropping rows with missing data\")\n",
    "# X = X.iloc[lag:]\n",
    "# X = X.iloc[:-1]\n",
    "X = X.dropna()\n",
    "X.shape\n",
    "X.head(5)\n",
    "X.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Outcome variable dimension (145, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(145, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_step_ahead_ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>0.158923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>5.373663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>9.421777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>8.238399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>8.609839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     1_step_ahead_ngdp_r_sa_pcha\n",
       "910                     0.158923\n",
       "917                     5.373663\n",
       "924                     9.421777\n",
       "931                     8.238399\n",
       "938                     8.609839"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Input matrix: X\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(145, 672)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>...</th>\n",
       "      <th>ngdp_r_sa_pcha-1</th>\n",
       "      <th>ngdp_r_sa_pcha-2</th>\n",
       "      <th>ngdp_r_sa_pcha-3</th>\n",
       "      <th>ngdp_r_sa_pcha-4</th>\n",
       "      <th>ngdp_r_sa_pcha-5</th>\n",
       "      <th>ngdp_r_sa_pcha-6</th>\n",
       "      <th>ngdp_r_sa_pcha-7</th>\n",
       "      <th>ngdp_r_sa_pcha-8</th>\n",
       "      <th>ngdp_r_sa_pcha-9</th>\n",
       "      <th>ngdp_r_sa_pcha-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>1982Q3</td>\n",
       "      <td>2897.225</td>\n",
       "      <td>1904.8</td>\n",
       "      <td>99.543333</td>\n",
       "      <td>0.110231</td>\n",
       "      <td>92.663</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>1515.5</td>\n",
       "      <td>1.026598</td>\n",
       "      <td>4363.3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "      <td>-0.476985</td>\n",
       "      <td>-7.985864</td>\n",
       "      <td>1.261758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>1982Q4</td>\n",
       "      <td>2897.225</td>\n",
       "      <td>1918.1</td>\n",
       "      <td>99.119667</td>\n",
       "      <td>0.110231</td>\n",
       "      <td>93.691</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>1532.7</td>\n",
       "      <td>1.134939</td>\n",
       "      <td>4439.7</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "      <td>-0.476985</td>\n",
       "      <td>-7.985864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>1983Q1</td>\n",
       "      <td>3136.050</td>\n",
       "      <td>1947.2</td>\n",
       "      <td>99.143000</td>\n",
       "      <td>0.111528</td>\n",
       "      <td>92.971</td>\n",
       "      <td>10.366667</td>\n",
       "      <td>1549.5</td>\n",
       "      <td>1.096105</td>\n",
       "      <td>4483.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158923</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "      <td>-0.476985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>1983Q2</td>\n",
       "      <td>3136.050</td>\n",
       "      <td>1986.3</td>\n",
       "      <td>99.945000</td>\n",
       "      <td>0.111528</td>\n",
       "      <td>92.457</td>\n",
       "      <td>10.133333</td>\n",
       "      <td>1561.6</td>\n",
       "      <td>0.780897</td>\n",
       "      <td>4574.9</td>\n",
       "      <td>...</td>\n",
       "      <td>5.373663</td>\n",
       "      <td>0.158923</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>1983Q3</td>\n",
       "      <td>3136.050</td>\n",
       "      <td>2029.6</td>\n",
       "      <td>101.610667</td>\n",
       "      <td>0.111528</td>\n",
       "      <td>91.532</td>\n",
       "      <td>9.366667</td>\n",
       "      <td>1580.2</td>\n",
       "      <td>1.191086</td>\n",
       "      <td>4657.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.421777</td>\n",
       "      <td>5.373663</td>\n",
       "      <td>0.158923</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 672 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       time    gdpwgt      lc          le       llf   lulcm        lur  \\\n",
       "910  1982Q3  2897.225  1904.8   99.543333  0.110231  92.663   9.900000   \n",
       "917  1982Q4  2897.225  1918.1   99.119667  0.110231  93.691  10.666667   \n",
       "924  1983Q1  3136.050  1947.2   99.143000  0.111528  92.971  10.366667   \n",
       "931  1983Q2  3136.050  1986.3   99.945000  0.111528  92.457  10.133333   \n",
       "938  1983Q3  3136.050  2029.6  101.610667  0.111528  91.532   9.366667   \n",
       "\n",
       "      ncg_r  ncg_rpch   ncp_r  ...  ngdp_r_sa_pcha-1  ngdp_r_sa_pcha-2  \\\n",
       "910  1515.5  1.026598  4363.3  ...          1.837425         -6.069358   \n",
       "917  1532.7  1.134939  4439.7  ...         -1.520719          1.837425   \n",
       "924  1549.5  1.096105  4483.6  ...          0.158923         -1.520719   \n",
       "931  1561.6  0.780897  4574.9  ...          5.373663          0.158923   \n",
       "938  1580.2  1.191086  4657.0  ...          9.421777          5.373663   \n",
       "\n",
       "     ngdp_r_sa_pcha-3  ngdp_r_sa_pcha-4  ngdp_r_sa_pcha-5  ngdp_r_sa_pcha-6  \\\n",
       "910         -4.285831          4.872232         -2.926867          8.070747   \n",
       "917         -6.069358         -4.285831          4.872232         -2.926867   \n",
       "924          1.837425         -6.069358         -4.285831          4.872232   \n",
       "931         -1.520719          1.837425         -6.069358         -4.285831   \n",
       "938          0.158923         -1.520719          1.837425         -6.069358   \n",
       "\n",
       "     ngdp_r_sa_pcha-7  ngdp_r_sa_pcha-8  ngdp_r_sa_pcha-9  ngdp_r_sa_pcha-10  \n",
       "910          7.668385         -0.476985         -7.985864           1.261758  \n",
       "917          8.070747          7.668385         -0.476985          -7.985864  \n",
       "924         -2.926867          8.070747          7.668385          -0.476985  \n",
       "931          4.872232         -2.926867          8.070747           7.668385  \n",
       "938         -4.285831          4.872232         -2.926867           8.070747  \n",
       "\n",
       "[5 rows x 672 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " columns in input dataset\n",
      ":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['time', 'gdpwgt', 'lc', 'le', 'llf', 'lulcm', 'lur', 'ncg_r',\n",
       "       'ncg_rpch', 'ncp_r',\n",
       "       ...\n",
       "       'ngdp_r_sa_pcha-1', 'ngdp_r_sa_pcha-2', 'ngdp_r_sa_pcha-3',\n",
       "       'ngdp_r_sa_pcha-4', 'ngdp_r_sa_pcha-5', 'ngdp_r_sa_pcha-6',\n",
       "       'ngdp_r_sa_pcha-7', 'ngdp_r_sa_pcha-8', 'ngdp_r_sa_pcha-9',\n",
       "       'ngdp_r_sa_pcha-10'],\n",
       "      dtype='object', length=672)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating input and output variables\n",
    "\n",
    "X1 = X\n",
    "\n",
    "Y1 = X1[predicted_variable]\n",
    "\n",
    "Y1 = pd.DataFrame(Y1) # very important step, gave me formatting errors, and wasted 2 hour in debugging   \n",
    "\n",
    "print(\"\\n Outcome variable dimension\", Y1.shape)\n",
    "Y1.shape\n",
    "Y1.head(5)\n",
    "\n",
    "# Dropping outcome variable from input matrix\n",
    "X1 = X1.drop(columns = [predicted_variable])\n",
    "print(\"\\n Input matrix: X\")\n",
    "X1.shape\n",
    "X1.head(5)\n",
    "\n",
    "print(\"\\n columns in input dataset\\n:\")\n",
    "X1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# items in training set: 97\n",
      "\n",
      "# items in test set: 48\n",
      "\n",
      " input training set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(97, 672)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>...</th>\n",
       "      <th>ngdp_r_sa_pcha-1</th>\n",
       "      <th>ngdp_r_sa_pcha-2</th>\n",
       "      <th>ngdp_r_sa_pcha-3</th>\n",
       "      <th>ngdp_r_sa_pcha-4</th>\n",
       "      <th>ngdp_r_sa_pcha-5</th>\n",
       "      <th>ngdp_r_sa_pcha-6</th>\n",
       "      <th>ngdp_r_sa_pcha-7</th>\n",
       "      <th>ngdp_r_sa_pcha-8</th>\n",
       "      <th>ngdp_r_sa_pcha-9</th>\n",
       "      <th>ngdp_r_sa_pcha-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>1982Q3</td>\n",
       "      <td>2897.225</td>\n",
       "      <td>1904.8</td>\n",
       "      <td>99.543333</td>\n",
       "      <td>0.110231</td>\n",
       "      <td>92.663</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>1515.5</td>\n",
       "      <td>1.026598</td>\n",
       "      <td>4363.3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "      <td>-0.476985</td>\n",
       "      <td>-7.985864</td>\n",
       "      <td>1.261758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>1982Q4</td>\n",
       "      <td>2897.225</td>\n",
       "      <td>1918.1</td>\n",
       "      <td>99.119667</td>\n",
       "      <td>0.110231</td>\n",
       "      <td>93.691</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>1532.7</td>\n",
       "      <td>1.134939</td>\n",
       "      <td>4439.7</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "      <td>-0.476985</td>\n",
       "      <td>-7.985864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>1983Q1</td>\n",
       "      <td>3136.050</td>\n",
       "      <td>1947.2</td>\n",
       "      <td>99.143000</td>\n",
       "      <td>0.111528</td>\n",
       "      <td>92.971</td>\n",
       "      <td>10.366667</td>\n",
       "      <td>1549.5</td>\n",
       "      <td>1.096105</td>\n",
       "      <td>4483.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158923</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "      <td>-0.476985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>1983Q2</td>\n",
       "      <td>3136.050</td>\n",
       "      <td>1986.3</td>\n",
       "      <td>99.945000</td>\n",
       "      <td>0.111528</td>\n",
       "      <td>92.457</td>\n",
       "      <td>10.133333</td>\n",
       "      <td>1561.6</td>\n",
       "      <td>0.780897</td>\n",
       "      <td>4574.9</td>\n",
       "      <td>...</td>\n",
       "      <td>5.373663</td>\n",
       "      <td>0.158923</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>1983Q3</td>\n",
       "      <td>3136.050</td>\n",
       "      <td>2029.6</td>\n",
       "      <td>101.610667</td>\n",
       "      <td>0.111528</td>\n",
       "      <td>91.532</td>\n",
       "      <td>9.366667</td>\n",
       "      <td>1580.2</td>\n",
       "      <td>1.191086</td>\n",
       "      <td>4657.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.421777</td>\n",
       "      <td>5.373663</td>\n",
       "      <td>0.158923</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 672 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       time    gdpwgt      lc          le       llf   lulcm        lur  \\\n",
       "910  1982Q3  2897.225  1904.8   99.543333  0.110231  92.663   9.900000   \n",
       "917  1982Q4  2897.225  1918.1   99.119667  0.110231  93.691  10.666667   \n",
       "924  1983Q1  3136.050  1947.2   99.143000  0.111528  92.971  10.366667   \n",
       "931  1983Q2  3136.050  1986.3   99.945000  0.111528  92.457  10.133333   \n",
       "938  1983Q3  3136.050  2029.6  101.610667  0.111528  91.532   9.366667   \n",
       "\n",
       "      ncg_r  ncg_rpch   ncp_r  ...  ngdp_r_sa_pcha-1  ngdp_r_sa_pcha-2  \\\n",
       "910  1515.5  1.026598  4363.3  ...          1.837425         -6.069358   \n",
       "917  1532.7  1.134939  4439.7  ...         -1.520719          1.837425   \n",
       "924  1549.5  1.096105  4483.6  ...          0.158923         -1.520719   \n",
       "931  1561.6  0.780897  4574.9  ...          5.373663          0.158923   \n",
       "938  1580.2  1.191086  4657.0  ...          9.421777          5.373663   \n",
       "\n",
       "     ngdp_r_sa_pcha-3  ngdp_r_sa_pcha-4  ngdp_r_sa_pcha-5  ngdp_r_sa_pcha-6  \\\n",
       "910         -4.285831          4.872232         -2.926867          8.070747   \n",
       "917         -6.069358         -4.285831          4.872232         -2.926867   \n",
       "924          1.837425         -6.069358         -4.285831          4.872232   \n",
       "931         -1.520719          1.837425         -6.069358         -4.285831   \n",
       "938          0.158923         -1.520719          1.837425         -6.069358   \n",
       "\n",
       "     ngdp_r_sa_pcha-7  ngdp_r_sa_pcha-8  ngdp_r_sa_pcha-9  ngdp_r_sa_pcha-10  \n",
       "910          7.668385         -0.476985         -7.985864           1.261758  \n",
       "917          8.070747          7.668385         -0.476985          -7.985864  \n",
       "924         -2.926867          8.070747          7.668385          -0.476985  \n",
       "931          4.872232         -2.926867          8.070747           7.668385  \n",
       "938         -4.285831          4.872232         -2.926867           8.070747  \n",
       "\n",
       "[5 rows x 672 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "910    0.158923\n",
       "917    5.373663\n",
       "924    9.421777\n",
       "931    8.238399\n",
       "938    8.609839\n",
       "Name: 1_step_ahead_ngdp_r_sa_pcha, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " input test set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48, 672)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>...</th>\n",
       "      <th>ngdp_r_sa_pcha-1</th>\n",
       "      <th>ngdp_r_sa_pcha-2</th>\n",
       "      <th>ngdp_r_sa_pcha-3</th>\n",
       "      <th>ngdp_r_sa_pcha-4</th>\n",
       "      <th>ngdp_r_sa_pcha-5</th>\n",
       "      <th>ngdp_r_sa_pcha-6</th>\n",
       "      <th>ngdp_r_sa_pcha-7</th>\n",
       "      <th>ngdp_r_sa_pcha-8</th>\n",
       "      <th>ngdp_r_sa_pcha-9</th>\n",
       "      <th>ngdp_r_sa_pcha-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>2006Q4</td>\n",
       "      <td>12236.20</td>\n",
       "      <td>7624.0</td>\n",
       "      <td>145.606000</td>\n",
       "      <td>0.151394</td>\n",
       "      <td>96.534</td>\n",
       "      <td>4.433333</td>\n",
       "      <td>2443.5</td>\n",
       "      <td>0.846059</td>\n",
       "      <td>10504.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620939</td>\n",
       "      <td>0.938637</td>\n",
       "      <td>5.427471</td>\n",
       "      <td>2.548700</td>\n",
       "      <td>3.614058</td>\n",
       "      <td>1.859616</td>\n",
       "      <td>4.501177</td>\n",
       "      <td>4.067524</td>\n",
       "      <td>3.836396</td>\n",
       "      <td>3.084029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>2007Q1</td>\n",
       "      <td>13021.65</td>\n",
       "      <td>7806.8</td>\n",
       "      <td>146.135000</td>\n",
       "      <td>0.153119</td>\n",
       "      <td>96.994</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2444.9</td>\n",
       "      <td>0.057295</td>\n",
       "      <td>10563.3</td>\n",
       "      <td>...</td>\n",
       "      <td>3.449636</td>\n",
       "      <td>0.620939</td>\n",
       "      <td>0.938637</td>\n",
       "      <td>5.427471</td>\n",
       "      <td>2.548700</td>\n",
       "      <td>3.614058</td>\n",
       "      <td>1.859616</td>\n",
       "      <td>4.501177</td>\n",
       "      <td>4.067524</td>\n",
       "      <td>3.836396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>2007Q2</td>\n",
       "      <td>13021.65</td>\n",
       "      <td>7845.4</td>\n",
       "      <td>145.850667</td>\n",
       "      <td>0.153119</td>\n",
       "      <td>95.793</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2460.5</td>\n",
       "      <td>0.638063</td>\n",
       "      <td>10582.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945307</td>\n",
       "      <td>3.449636</td>\n",
       "      <td>0.620939</td>\n",
       "      <td>0.938637</td>\n",
       "      <td>5.427471</td>\n",
       "      <td>2.548700</td>\n",
       "      <td>3.614058</td>\n",
       "      <td>1.859616</td>\n",
       "      <td>4.501177</td>\n",
       "      <td>4.067524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>2007Q3</td>\n",
       "      <td>13021.65</td>\n",
       "      <td>7885.1</td>\n",
       "      <td>145.943667</td>\n",
       "      <td>0.153119</td>\n",
       "      <td>95.084</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>2472.8</td>\n",
       "      <td>0.499898</td>\n",
       "      <td>10642.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.312389</td>\n",
       "      <td>0.945307</td>\n",
       "      <td>3.449636</td>\n",
       "      <td>0.620939</td>\n",
       "      <td>0.938637</td>\n",
       "      <td>5.427471</td>\n",
       "      <td>2.548700</td>\n",
       "      <td>3.614058</td>\n",
       "      <td>1.859616</td>\n",
       "      <td>4.501177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>2007Q4</td>\n",
       "      <td>13021.65</td>\n",
       "      <td>7978.2</td>\n",
       "      <td>146.271333</td>\n",
       "      <td>0.153119</td>\n",
       "      <td>94.925</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>2489.1</td>\n",
       "      <td>0.659172</td>\n",
       "      <td>10672.8</td>\n",
       "      <td>...</td>\n",
       "      <td>2.189473</td>\n",
       "      <td>2.312389</td>\n",
       "      <td>0.945307</td>\n",
       "      <td>3.449636</td>\n",
       "      <td>0.620939</td>\n",
       "      <td>0.938637</td>\n",
       "      <td>5.427471</td>\n",
       "      <td>2.548700</td>\n",
       "      <td>3.614058</td>\n",
       "      <td>1.859616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 672 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time    gdpwgt      lc          le       llf   lulcm       lur  \\\n",
       "1589  2006Q4  12236.20  7624.0  145.606000  0.151394  96.534  4.433333   \n",
       "1596  2007Q1  13021.65  7806.8  146.135000  0.153119  96.994  4.500000   \n",
       "1603  2007Q2  13021.65  7845.4  145.850667  0.153119  95.793  4.500000   \n",
       "1610  2007Q3  13021.65  7885.1  145.943667  0.153119  95.084  4.666667   \n",
       "1617  2007Q4  13021.65  7978.2  146.271333  0.153119  94.925  4.800000   \n",
       "\n",
       "       ncg_r  ncg_rpch    ncp_r  ...  ngdp_r_sa_pcha-1  ngdp_r_sa_pcha-2  \\\n",
       "1589  2443.5  0.846059  10504.5  ...          0.620939          0.938637   \n",
       "1596  2444.9  0.057295  10563.3  ...          3.449636          0.620939   \n",
       "1603  2460.5  0.638063  10582.8  ...          0.945307          3.449636   \n",
       "1610  2472.8  0.499898  10642.5  ...          2.312389          0.945307   \n",
       "1617  2489.1  0.659172  10672.8  ...          2.189473          2.312389   \n",
       "\n",
       "      ngdp_r_sa_pcha-3  ngdp_r_sa_pcha-4  ngdp_r_sa_pcha-5  ngdp_r_sa_pcha-6  \\\n",
       "1589          5.427471          2.548700          3.614058          1.859616   \n",
       "1596          0.938637          5.427471          2.548700          3.614058   \n",
       "1603          0.620939          0.938637          5.427471          2.548700   \n",
       "1610          3.449636          0.620939          0.938637          5.427471   \n",
       "1617          0.945307          3.449636          0.620939          0.938637   \n",
       "\n",
       "      ngdp_r_sa_pcha-7  ngdp_r_sa_pcha-8  ngdp_r_sa_pcha-9  ngdp_r_sa_pcha-10  \n",
       "1589          4.501177          4.067524          3.836396           3.084029  \n",
       "1596          1.859616          4.501177          4.067524           3.836396  \n",
       "1603          3.614058          1.859616          4.501177           4.067524  \n",
       "1610          2.548700          3.614058          1.859616           4.501177  \n",
       "1617          5.427471          2.548700          3.614058           1.859616  \n",
       "\n",
       "[5 rows x 672 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1589    0.945307\n",
       "1596    2.312389\n",
       "1603    2.189473\n",
       "1610    2.455478\n",
       "1617   -2.279453\n",
       "Name: 1_step_ahead_ngdp_r_sa_pcha, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_step_ahead_ngdp_r_sa_pcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>0.945307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>2.312389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>2.189473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>2.455478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>-2.279453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1_step_ahead_ngdp_r_sa_pcha\n",
       "1589                     0.945307\n",
       "1596                     2.312389\n",
       "1603                     2.189473\n",
       "1610                     2.455478\n",
       "1617                    -2.279453"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "# Sequential train-test split\n",
    "train_test_ratio = 0.67\n",
    "\n",
    "training = int(round(X1.shape[0]*train_test_ratio, 0))\n",
    "test = X1.shape[0] - training\n",
    "\n",
    "print(\"# items in training set:\", training)\n",
    "print(\"\\n# items in test set:\", test)\n",
    "\n",
    "X_train = X1.iloc[0:(training),:]\n",
    "y_train = Y1.iloc[0:(training),0]\n",
    "X_test = X1.iloc[training:(X1.shape[0]),:]\n",
    "y_test = Y1.iloc[training:(X1.shape[0]),0]\n",
    "y_test_outcome_value = Y1.iloc[training:(X1.shape[0]),:]\n",
    "\n",
    "print(\"\\n input training set:\")\n",
    "X_train.shape\n",
    "X_train.head(5)\n",
    "\n",
    "y_train.head(5)\n",
    "\n",
    "print(\"\\n input test set:\")\n",
    "X_test.shape\n",
    "X_test.head(5)\n",
    "\n",
    "y_test.head(5)\n",
    "y_test_outcome_value.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py:4117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>ngdp_r_sa_pcha-1</th>\n",
       "      <th>ngdp_r_sa_pcha-2</th>\n",
       "      <th>ngdp_r_sa_pcha-3</th>\n",
       "      <th>ngdp_r_sa_pcha-4</th>\n",
       "      <th>ngdp_r_sa_pcha-5</th>\n",
       "      <th>ngdp_r_sa_pcha-6</th>\n",
       "      <th>ngdp_r_sa_pcha-7</th>\n",
       "      <th>ngdp_r_sa_pcha-8</th>\n",
       "      <th>ngdp_r_sa_pcha-9</th>\n",
       "      <th>ngdp_r_sa_pcha-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>2897.225</td>\n",
       "      <td>1904.8</td>\n",
       "      <td>99.543333</td>\n",
       "      <td>0.110231</td>\n",
       "      <td>92.663</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>1515.5</td>\n",
       "      <td>1.026598</td>\n",
       "      <td>4363.3</td>\n",
       "      <td>0.669082</td>\n",
       "      <td>...</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "      <td>-0.476985</td>\n",
       "      <td>-7.985864</td>\n",
       "      <td>1.261758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>2897.225</td>\n",
       "      <td>1918.1</td>\n",
       "      <td>99.119667</td>\n",
       "      <td>0.110231</td>\n",
       "      <td>93.691</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>1532.7</td>\n",
       "      <td>1.134939</td>\n",
       "      <td>4439.7</td>\n",
       "      <td>1.750968</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "      <td>-0.476985</td>\n",
       "      <td>-7.985864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>3136.050</td>\n",
       "      <td>1947.2</td>\n",
       "      <td>99.143000</td>\n",
       "      <td>0.111528</td>\n",
       "      <td>92.971</td>\n",
       "      <td>10.366667</td>\n",
       "      <td>1549.5</td>\n",
       "      <td>1.096105</td>\n",
       "      <td>4483.6</td>\n",
       "      <td>0.988806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158923</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "      <td>-0.476985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>3136.050</td>\n",
       "      <td>1986.3</td>\n",
       "      <td>99.945000</td>\n",
       "      <td>0.111528</td>\n",
       "      <td>92.457</td>\n",
       "      <td>10.133333</td>\n",
       "      <td>1561.6</td>\n",
       "      <td>0.780897</td>\n",
       "      <td>4574.9</td>\n",
       "      <td>2.036310</td>\n",
       "      <td>...</td>\n",
       "      <td>5.373663</td>\n",
       "      <td>0.158923</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "      <td>7.668385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>3136.050</td>\n",
       "      <td>2029.6</td>\n",
       "      <td>101.610667</td>\n",
       "      <td>0.111528</td>\n",
       "      <td>91.532</td>\n",
       "      <td>9.366667</td>\n",
       "      <td>1580.2</td>\n",
       "      <td>1.191086</td>\n",
       "      <td>4657.0</td>\n",
       "      <td>1.794575</td>\n",
       "      <td>...</td>\n",
       "      <td>9.421777</td>\n",
       "      <td>5.373663</td>\n",
       "      <td>0.158923</td>\n",
       "      <td>-1.520719</td>\n",
       "      <td>1.837425</td>\n",
       "      <td>-6.069358</td>\n",
       "      <td>-4.285831</td>\n",
       "      <td>4.872232</td>\n",
       "      <td>-2.926867</td>\n",
       "      <td>8.070747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 671 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gdpwgt      lc          le       llf   lulcm        lur   ncg_r  \\\n",
       "910  2897.225  1904.8   99.543333  0.110231  92.663   9.900000  1515.5   \n",
       "917  2897.225  1918.1   99.119667  0.110231  93.691  10.666667  1532.7   \n",
       "924  3136.050  1947.2   99.143000  0.111528  92.971  10.366667  1549.5   \n",
       "931  3136.050  1986.3   99.945000  0.111528  92.457  10.133333  1561.6   \n",
       "938  3136.050  2029.6  101.610667  0.111528  91.532   9.366667  1580.2   \n",
       "\n",
       "     ncg_rpch   ncp_r  ncp_rpch  ...  ngdp_r_sa_pcha-1  ngdp_r_sa_pcha-2  \\\n",
       "910  1.026598  4363.3  0.669082  ...          1.837425         -6.069358   \n",
       "917  1.134939  4439.7  1.750968  ...         -1.520719          1.837425   \n",
       "924  1.096105  4483.6  0.988806  ...          0.158923         -1.520719   \n",
       "931  0.780897  4574.9  2.036310  ...          5.373663          0.158923   \n",
       "938  1.191086  4657.0  1.794575  ...          9.421777          5.373663   \n",
       "\n",
       "     ngdp_r_sa_pcha-3  ngdp_r_sa_pcha-4  ngdp_r_sa_pcha-5  ngdp_r_sa_pcha-6  \\\n",
       "910         -4.285831          4.872232         -2.926867          8.070747   \n",
       "917         -6.069358         -4.285831          4.872232         -2.926867   \n",
       "924          1.837425         -6.069358         -4.285831          4.872232   \n",
       "931         -1.520719          1.837425         -6.069358         -4.285831   \n",
       "938          0.158923         -1.520719          1.837425         -6.069358   \n",
       "\n",
       "     ngdp_r_sa_pcha-7  ngdp_r_sa_pcha-8  ngdp_r_sa_pcha-9  ngdp_r_sa_pcha-10  \n",
       "910          7.668385         -0.476985         -7.985864           1.261758  \n",
       "917          8.070747          7.668385         -0.476985          -7.985864  \n",
       "924         -2.926867          8.070747          7.668385          -0.476985  \n",
       "931          4.872232         -2.926867          8.070747           7.668385  \n",
       "938         -4.285831          4.872232         -2.926867           8.070747  \n",
       "\n",
       "[5 rows x 671 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling the training & test sets \n",
    "\n",
    "# Dropping the \"time\" column\n",
    "\n",
    "X_train.drop(columns = ['time'], inplace = True)\n",
    "X_test.drop(columns = ['time'], inplace = True)\n",
    "\n",
    "train_columns = list(X_train.columns)\n",
    "# train_columns\n",
    "\n",
    "# X_test  = X_test.drop(columns = [\"time\"], inplace = True)\n",
    "\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled training input dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>ngdp_r_sa_pcha-1</th>\n",
       "      <th>ngdp_r_sa_pcha-2</th>\n",
       "      <th>ngdp_r_sa_pcha-3</th>\n",
       "      <th>ngdp_r_sa_pcha-4</th>\n",
       "      <th>ngdp_r_sa_pcha-5</th>\n",
       "      <th>ngdp_r_sa_pcha-6</th>\n",
       "      <th>ngdp_r_sa_pcha-7</th>\n",
       "      <th>ngdp_r_sa_pcha-8</th>\n",
       "      <th>ngdp_r_sa_pcha-9</th>\n",
       "      <th>ngdp_r_sa_pcha-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091959</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.761216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.511333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115132</td>\n",
       "      <td>0.706313</td>\n",
       "      <td>0.202857</td>\n",
       "      <td>0.912787</td>\n",
       "      <td>0.886813</td>\n",
       "      <td>0.361005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.168567</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018891</td>\n",
       "      <td>0.783991</td>\n",
       "      <td>0.012648</td>\n",
       "      <td>0.898015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159268</td>\n",
       "      <td>0.510407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115132</td>\n",
       "      <td>0.706313</td>\n",
       "      <td>0.202857</td>\n",
       "      <td>0.912787</td>\n",
       "      <td>0.886813</td>\n",
       "      <td>0.431355</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.007595</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.031524</td>\n",
       "      <td>0.114912</td>\n",
       "      <td>0.955665</td>\n",
       "      <td>0.037342</td>\n",
       "      <td>0.775827</td>\n",
       "      <td>0.019916</td>\n",
       "      <td>0.625607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288318</td>\n",
       "      <td>0.293629</td>\n",
       "      <td>0.510407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115132</td>\n",
       "      <td>0.706313</td>\n",
       "      <td>0.202857</td>\n",
       "      <td>0.912787</td>\n",
       "      <td>0.899275</td>\n",
       "      <td>0.431355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.014599</td>\n",
       "      <td>0.018168</td>\n",
       "      <td>0.031524</td>\n",
       "      <td>0.076608</td>\n",
       "      <td>0.921182</td>\n",
       "      <td>0.050632</td>\n",
       "      <td>0.709567</td>\n",
       "      <td>0.035030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688976</td>\n",
       "      <td>0.402055</td>\n",
       "      <td>0.293629</td>\n",
       "      <td>0.510407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115132</td>\n",
       "      <td>0.706313</td>\n",
       "      <td>0.202857</td>\n",
       "      <td>0.922389</td>\n",
       "      <td>0.899275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.022356</td>\n",
       "      <td>0.054834</td>\n",
       "      <td>0.031524</td>\n",
       "      <td>0.007676</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.071060</td>\n",
       "      <td>0.795793</td>\n",
       "      <td>0.048622</td>\n",
       "      <td>0.913600</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.738682</td>\n",
       "      <td>0.402055</td>\n",
       "      <td>0.293629</td>\n",
       "      <td>0.510407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115132</td>\n",
       "      <td>0.706313</td>\n",
       "      <td>0.290619</td>\n",
       "      <td>0.922389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 671 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gdpwgt        lc        le       llf     lulcm       lur     ncg_r  \\\n",
       "0  0.000000  0.000000  0.009326  0.000000  0.091959  0.886700  0.000000   \n",
       "1  0.000000  0.002382  0.000000  0.000000  0.168567  1.000000  0.018891   \n",
       "2  0.025573  0.007595  0.000514  0.031524  0.114912  0.955665  0.037342   \n",
       "3  0.025573  0.014599  0.018168  0.031524  0.076608  0.921182  0.050632   \n",
       "4  0.025573  0.022356  0.054834  0.031524  0.007676  0.807882  0.071060   \n",
       "\n",
       "   ncg_rpch     ncp_r  ncp_rpch  ...  ngdp_r_sa_pcha-1  ngdp_r_sa_pcha-2  \\\n",
       "0  0.761216  0.000000  0.511333  ...          0.417280          0.000000   \n",
       "1  0.783991  0.012648  0.898015  ...          0.159268          0.510407   \n",
       "2  0.775827  0.019916  0.625607  ...          0.288318          0.293629   \n",
       "3  0.709567  0.035030  1.000000  ...          0.688976          0.402055   \n",
       "4  0.795793  0.048622  0.913600  ...          1.000000          0.738682   \n",
       "\n",
       "   ngdp_r_sa_pcha-3  ngdp_r_sa_pcha-4  ngdp_r_sa_pcha-5  ngdp_r_sa_pcha-6  \\\n",
       "0          0.115132          0.706313          0.202857          0.912787   \n",
       "1          0.000000          0.115132          0.706313          0.202857   \n",
       "2          0.510407          0.000000          0.115132          0.706313   \n",
       "3          0.293629          0.510407          0.000000          0.115132   \n",
       "4          0.402055          0.293629          0.510407          0.000000   \n",
       "\n",
       "   ngdp_r_sa_pcha-7  ngdp_r_sa_pcha-8  ngdp_r_sa_pcha-9  ngdp_r_sa_pcha-10  \n",
       "0          0.886813          0.361005          0.000000           0.531239  \n",
       "1          0.912787          0.886813          0.431355           0.000000  \n",
       "2          0.202857          0.912787          0.899275           0.431355  \n",
       "3          0.706313          0.202857          0.922389           0.899275  \n",
       "4          0.115132          0.706313          0.290619           0.922389  \n",
       "\n",
       "[5 rows x 671 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled test input dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>ngdp_r_sa_pcha-1</th>\n",
       "      <th>ngdp_r_sa_pcha-2</th>\n",
       "      <th>ngdp_r_sa_pcha-3</th>\n",
       "      <th>ngdp_r_sa_pcha-4</th>\n",
       "      <th>ngdp_r_sa_pcha-5</th>\n",
       "      <th>ngdp_r_sa_pcha-6</th>\n",
       "      <th>ngdp_r_sa_pcha-7</th>\n",
       "      <th>ngdp_r_sa_pcha-8</th>\n",
       "      <th>ngdp_r_sa_pcha-9</th>\n",
       "      <th>ngdp_r_sa_pcha-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095128</td>\n",
       "      <td>0.103261</td>\n",
       "      <td>0.019358</td>\n",
       "      <td>0.782466</td>\n",
       "      <td>0.033506</td>\n",
       "      <td>0.885535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647165</td>\n",
       "      <td>0.670012</td>\n",
       "      <td>0.992820</td>\n",
       "      <td>0.785797</td>\n",
       "      <td>0.862411</td>\n",
       "      <td>0.736243</td>\n",
       "      <td>0.926207</td>\n",
       "      <td>0.895021</td>\n",
       "      <td>0.878400</td>\n",
       "      <td>0.824295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.119305</td>\n",
       "      <td>0.054238</td>\n",
       "      <td>0.443550</td>\n",
       "      <td>0.161546</td>\n",
       "      <td>0.122325</td>\n",
       "      <td>0.114130</td>\n",
       "      <td>0.025968</td>\n",
       "      <td>0.525182</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.695181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850587</td>\n",
       "      <td>0.647165</td>\n",
       "      <td>0.670012</td>\n",
       "      <td>0.992820</td>\n",
       "      <td>0.785797</td>\n",
       "      <td>0.862411</td>\n",
       "      <td>0.736243</td>\n",
       "      <td>0.926207</td>\n",
       "      <td>0.895021</td>\n",
       "      <td>0.878400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.119305</td>\n",
       "      <td>0.065691</td>\n",
       "      <td>0.427312</td>\n",
       "      <td>0.161546</td>\n",
       "      <td>0.051318</td>\n",
       "      <td>0.114130</td>\n",
       "      <td>0.099622</td>\n",
       "      <td>0.714620</td>\n",
       "      <td>0.063593</td>\n",
       "      <td>0.520215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670491</td>\n",
       "      <td>0.850587</td>\n",
       "      <td>0.647165</td>\n",
       "      <td>0.670012</td>\n",
       "      <td>0.992820</td>\n",
       "      <td>0.785797</td>\n",
       "      <td>0.862411</td>\n",
       "      <td>0.736243</td>\n",
       "      <td>0.926207</td>\n",
       "      <td>0.895021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119305</td>\n",
       "      <td>0.077471</td>\n",
       "      <td>0.432623</td>\n",
       "      <td>0.161546</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.157696</td>\n",
       "      <td>0.669553</td>\n",
       "      <td>0.086532</td>\n",
       "      <td>0.697215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768803</td>\n",
       "      <td>0.670491</td>\n",
       "      <td>0.850587</td>\n",
       "      <td>0.647165</td>\n",
       "      <td>0.670012</td>\n",
       "      <td>0.992820</td>\n",
       "      <td>0.785797</td>\n",
       "      <td>0.862411</td>\n",
       "      <td>0.736243</td>\n",
       "      <td>0.926207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.119305</td>\n",
       "      <td>0.105095</td>\n",
       "      <td>0.451335</td>\n",
       "      <td>0.161546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163043</td>\n",
       "      <td>0.234655</td>\n",
       "      <td>0.721506</td>\n",
       "      <td>0.098175</td>\n",
       "      <td>0.566902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.759964</td>\n",
       "      <td>0.768803</td>\n",
       "      <td>0.670491</td>\n",
       "      <td>0.850587</td>\n",
       "      <td>0.647165</td>\n",
       "      <td>0.670012</td>\n",
       "      <td>0.992820</td>\n",
       "      <td>0.785797</td>\n",
       "      <td>0.862411</td>\n",
       "      <td>0.736243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 671 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gdpwgt        lc        le       llf     lulcm       lur     ncg_r  \\\n",
       "0  0.000000  0.000000  0.413340  0.000000  0.095128  0.103261  0.019358   \n",
       "1  0.119305  0.054238  0.443550  0.161546  0.122325  0.114130  0.025968   \n",
       "2  0.119305  0.065691  0.427312  0.161546  0.051318  0.114130  0.099622   \n",
       "3  0.119305  0.077471  0.432623  0.161546  0.009400  0.141304  0.157696   \n",
       "4  0.119305  0.105095  0.451335  0.161546  0.000000  0.163043  0.234655   \n",
       "\n",
       "   ncg_rpch     ncp_r  ncp_rpch  ...  ngdp_r_sa_pcha-1  ngdp_r_sa_pcha-2  \\\n",
       "0  0.782466  0.033506  0.885535  ...          0.647165          0.670012   \n",
       "1  0.525182  0.056100  0.695181  ...          0.850587          0.647165   \n",
       "2  0.714620  0.063593  0.520215  ...          0.670491          0.850587   \n",
       "3  0.669553  0.086532  0.697215  ...          0.768803          0.670491   \n",
       "4  0.721506  0.098175  0.566902  ...          0.759964          0.768803   \n",
       "\n",
       "   ngdp_r_sa_pcha-3  ngdp_r_sa_pcha-4  ngdp_r_sa_pcha-5  ngdp_r_sa_pcha-6  \\\n",
       "0          0.992820          0.785797          0.862411          0.736243   \n",
       "1          0.670012          0.992820          0.785797          0.862411   \n",
       "2          0.647165          0.670012          0.992820          0.785797   \n",
       "3          0.850587          0.647165          0.670012          0.992820   \n",
       "4          0.670491          0.850587          0.647165          0.670012   \n",
       "\n",
       "   ngdp_r_sa_pcha-7  ngdp_r_sa_pcha-8  ngdp_r_sa_pcha-9  ngdp_r_sa_pcha-10  \n",
       "0          0.926207          0.895021          0.878400           0.824295  \n",
       "1          0.736243          0.926207          0.895021           0.878400  \n",
       "2          0.862411          0.736243          0.926207           0.895021  \n",
       "3          0.785797          0.862411          0.736243           0.926207  \n",
       "4          0.992820          0.785797          0.862411           0.736243  \n",
       "\n",
       "[5 rows x 671 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling all the numerical variables\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# train_columns = list(X_train.columns) # removing 'time' column for feature scaling\n",
    "# train_columns\n",
    "\n",
    "\n",
    "print(\"\\nScaled training input dataset:\")\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns = train_columns)\n",
    "\n",
    "X_train.head(5)\n",
    "\n",
    "print(\"\\nScaled test input dataset:\")\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns = train_columns)\n",
    "\n",
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does not exist\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdpwgt</th>\n",
       "      <th>lc</th>\n",
       "      <th>le</th>\n",
       "      <th>llf</th>\n",
       "      <th>lulcm</th>\n",
       "      <th>lur</th>\n",
       "      <th>ncg_r</th>\n",
       "      <th>ncg_rpch</th>\n",
       "      <th>ncp_r</th>\n",
       "      <th>ncp_rpch</th>\n",
       "      <th>...</th>\n",
       "      <th>ngdp_r_sa_pcha-1</th>\n",
       "      <th>ngdp_r_sa_pcha-2</th>\n",
       "      <th>ngdp_r_sa_pcha-3</th>\n",
       "      <th>ngdp_r_sa_pcha-4</th>\n",
       "      <th>ngdp_r_sa_pcha-5</th>\n",
       "      <th>ngdp_r_sa_pcha-6</th>\n",
       "      <th>ngdp_r_sa_pcha-7</th>\n",
       "      <th>ngdp_r_sa_pcha-8</th>\n",
       "      <th>ngdp_r_sa_pcha-9</th>\n",
       "      <th>ngdp_r_sa_pcha-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091959</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.761216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.511333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115132</td>\n",
       "      <td>0.706313</td>\n",
       "      <td>0.202857</td>\n",
       "      <td>0.912787</td>\n",
       "      <td>0.886813</td>\n",
       "      <td>0.361005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.168567</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018891</td>\n",
       "      <td>0.783991</td>\n",
       "      <td>0.012648</td>\n",
       "      <td>0.898015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159268</td>\n",
       "      <td>0.510407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115132</td>\n",
       "      <td>0.706313</td>\n",
       "      <td>0.202857</td>\n",
       "      <td>0.912787</td>\n",
       "      <td>0.886813</td>\n",
       "      <td>0.431355</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.007595</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.031524</td>\n",
       "      <td>0.114912</td>\n",
       "      <td>0.955665</td>\n",
       "      <td>0.037342</td>\n",
       "      <td>0.775827</td>\n",
       "      <td>0.019916</td>\n",
       "      <td>0.625607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288318</td>\n",
       "      <td>0.293629</td>\n",
       "      <td>0.510407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115132</td>\n",
       "      <td>0.706313</td>\n",
       "      <td>0.202857</td>\n",
       "      <td>0.912787</td>\n",
       "      <td>0.899275</td>\n",
       "      <td>0.431355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.014599</td>\n",
       "      <td>0.018168</td>\n",
       "      <td>0.031524</td>\n",
       "      <td>0.076608</td>\n",
       "      <td>0.921182</td>\n",
       "      <td>0.050632</td>\n",
       "      <td>0.709567</td>\n",
       "      <td>0.035030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688976</td>\n",
       "      <td>0.402055</td>\n",
       "      <td>0.293629</td>\n",
       "      <td>0.510407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115132</td>\n",
       "      <td>0.706313</td>\n",
       "      <td>0.202857</td>\n",
       "      <td>0.922389</td>\n",
       "      <td>0.899275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.022356</td>\n",
       "      <td>0.054834</td>\n",
       "      <td>0.031524</td>\n",
       "      <td>0.007676</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.071060</td>\n",
       "      <td>0.795793</td>\n",
       "      <td>0.048622</td>\n",
       "      <td>0.913600</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.738682</td>\n",
       "      <td>0.402055</td>\n",
       "      <td>0.293629</td>\n",
       "      <td>0.510407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115132</td>\n",
       "      <td>0.706313</td>\n",
       "      <td>0.290619</td>\n",
       "      <td>0.922389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.925039</td>\n",
       "      <td>0.934741</td>\n",
       "      <td>0.950243</td>\n",
       "      <td>0.949109</td>\n",
       "      <td>0.316715</td>\n",
       "      <td>0.157635</td>\n",
       "      <td>0.974080</td>\n",
       "      <td>0.634163</td>\n",
       "      <td>0.957206</td>\n",
       "      <td>0.619015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.418985</td>\n",
       "      <td>0.682360</td>\n",
       "      <td>0.654367</td>\n",
       "      <td>0.639447</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>0.530720</td>\n",
       "      <td>0.693273</td>\n",
       "      <td>0.841731</td>\n",
       "      <td>0.658839</td>\n",
       "      <td>0.587434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.925039</td>\n",
       "      <td>0.950451</td>\n",
       "      <td>0.957126</td>\n",
       "      <td>0.949109</td>\n",
       "      <td>0.271332</td>\n",
       "      <td>0.157635</td>\n",
       "      <td>0.974739</td>\n",
       "      <td>0.550664</td>\n",
       "      <td>0.962189</td>\n",
       "      <td>0.378235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553782</td>\n",
       "      <td>0.511839</td>\n",
       "      <td>0.682360</td>\n",
       "      <td>0.654367</td>\n",
       "      <td>0.639447</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>0.530720</td>\n",
       "      <td>0.693273</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.658839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980833</td>\n",
       "      <td>0.975830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.410314</td>\n",
       "      <td>0.123153</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.746615</td>\n",
       "      <td>0.980978</td>\n",
       "      <td>0.670866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471929</td>\n",
       "      <td>0.625094</td>\n",
       "      <td>0.511839</td>\n",
       "      <td>0.682360</td>\n",
       "      <td>0.654367</td>\n",
       "      <td>0.639447</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>0.530720</td>\n",
       "      <td>0.727043</td>\n",
       "      <td>0.859155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.989441</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.379611</td>\n",
       "      <td>0.108374</td>\n",
       "      <td>0.992641</td>\n",
       "      <td>0.487360</td>\n",
       "      <td>0.989604</td>\n",
       "      <td>0.453178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693110</td>\n",
       "      <td>0.556322</td>\n",
       "      <td>0.625094</td>\n",
       "      <td>0.511839</td>\n",
       "      <td>0.682360</td>\n",
       "      <td>0.654367</td>\n",
       "      <td>0.639447</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>0.582385</td>\n",
       "      <td>0.727043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.330352</td>\n",
       "      <td>0.108374</td>\n",
       "      <td>0.996705</td>\n",
       "      <td>0.577563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.489248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348225</td>\n",
       "      <td>0.742155</td>\n",
       "      <td>0.556322</td>\n",
       "      <td>0.625094</td>\n",
       "      <td>0.511839</td>\n",
       "      <td>0.682360</td>\n",
       "      <td>0.654367</td>\n",
       "      <td>0.639447</td>\n",
       "      <td>0.635922</td>\n",
       "      <td>0.582385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows × 671 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gdpwgt        lc        le       llf     lulcm       lur     ncg_r  \\\n",
       "0   0.000000  0.000000  0.009326  0.000000  0.091959  0.886700  0.000000   \n",
       "1   0.000000  0.002382  0.000000  0.000000  0.168567  1.000000  0.018891   \n",
       "2   0.025573  0.007595  0.000514  0.031524  0.114912  0.955665  0.037342   \n",
       "3   0.025573  0.014599  0.018168  0.031524  0.076608  0.921182  0.050632   \n",
       "4   0.025573  0.022356  0.054834  0.031524  0.007676  0.807882  0.071060   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "92  0.925039  0.934741  0.950243  0.949109  0.316715  0.157635  0.974080   \n",
       "93  0.925039  0.950451  0.957126  0.949109  0.271332  0.157635  0.974739   \n",
       "94  1.000000  0.980833  0.975830  1.000000  0.410314  0.123153  1.000000   \n",
       "95  1.000000  0.989413  0.989441  1.000000  0.379611  0.108374  0.992641   \n",
       "96  1.000000  1.000000  1.000000  1.000000  0.330352  0.108374  0.996705   \n",
       "\n",
       "    ncg_rpch     ncp_r  ncp_rpch  ...  ngdp_r_sa_pcha-1  ngdp_r_sa_pcha-2  \\\n",
       "0   0.761216  0.000000  0.511333  ...          0.417280          0.000000   \n",
       "1   0.783991  0.012648  0.898015  ...          0.159268          0.510407   \n",
       "2   0.775827  0.019916  0.625607  ...          0.288318          0.293629   \n",
       "3   0.709567  0.035030  1.000000  ...          0.688976          0.402055   \n",
       "4   0.795793  0.048622  0.913600  ...          1.000000          0.738682   \n",
       "..       ...       ...       ...  ...               ...               ...   \n",
       "92  0.634163  0.957206  0.619015  ...          0.418985          0.682360   \n",
       "93  0.550664  0.962189  0.378235  ...          0.553782          0.511839   \n",
       "94  0.746615  0.980978  0.670866  ...          0.471929          0.625094   \n",
       "95  0.487360  0.989604  0.453178  ...          0.693110          0.556322   \n",
       "96  0.577563  1.000000  0.489248  ...          0.348225          0.742155   \n",
       "\n",
       "    ngdp_r_sa_pcha-3  ngdp_r_sa_pcha-4  ngdp_r_sa_pcha-5  ngdp_r_sa_pcha-6  \\\n",
       "0           0.115132          0.706313          0.202857          0.912787   \n",
       "1           0.000000          0.115132          0.706313          0.202857   \n",
       "2           0.510407          0.000000          0.115132          0.706313   \n",
       "3           0.293629          0.510407          0.000000          0.115132   \n",
       "4           0.402055          0.293629          0.510407          0.000000   \n",
       "..               ...               ...               ...               ...   \n",
       "92          0.654367          0.639447          0.590879          0.530720   \n",
       "93          0.682360          0.654367          0.639447          0.590879   \n",
       "94          0.511839          0.682360          0.654367          0.639447   \n",
       "95          0.625094          0.511839          0.682360          0.654367   \n",
       "96          0.556322          0.625094          0.511839          0.682360   \n",
       "\n",
       "    ngdp_r_sa_pcha-7  ngdp_r_sa_pcha-8  ngdp_r_sa_pcha-9  ngdp_r_sa_pcha-10  \n",
       "0           0.886813          0.361005          0.000000           0.531239  \n",
       "1           0.912787          0.886813          0.431355           0.000000  \n",
       "2           0.202857          0.912787          0.899275           0.431355  \n",
       "3           0.706313          0.202857          0.922389           0.899275  \n",
       "4           0.115132          0.706313          0.290619           0.922389  \n",
       "..               ...               ...               ...                ...  \n",
       "92          0.693273          0.841731          0.658839           0.587434  \n",
       "93          0.530720          0.693273          0.859155           0.658839  \n",
       "94          0.590879          0.530720          0.727043           0.859155  \n",
       "95          0.639447          0.590879          0.582385           0.727043  \n",
       "96          0.654367          0.639447          0.635922           0.582385  \n",
       "\n",
       "[97 rows x 671 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'time' in X_train.columns:\n",
    "    print(\"Does not exist\")\n",
    "else:\n",
    "    print(\"Does not exist\")\n",
    "    \n",
    "X_train\n",
    "\n",
    "# train_cols = X_train.columns\n",
    "\n",
    "# train_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable selection by Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5000, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test MSE:  8.717457385574578\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# random forest model creation\n",
    "rfr = RandomForestRegressor(n_estimators = 5000)\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred = rfr.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Test MSE: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top variables (> 80% variation):  149 \n",
      " Total variation explained:  0.8007607181773514\n",
      "Top features: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['nm_rpch-1', 'ngdp_d_sa_pchy-7', 'pcpi_pch-2', 'pcpi_sa_pcha-2',\n",
       "       'ncg_rpch', 'ntdd_rpchy-7', 'le-4', 'ngdp_r-4', 'nmg_rpch-7',\n",
       "       'nfi_rpch-5',\n",
       "       ...\n",
       "       'ncp_rpch-1', 'pcpi_pchy', 'ngdp_rpchy-9', 'pcpi_sa_pchy', 'nm_rpch',\n",
       "       'lur-5', 'nfdd_rpch', 'nfbrgdp', 'ncp_rpch', 'nfi_rpch'],\n",
       "      dtype='object', length=149)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x2160 with 0 Axes>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Feature Importances - Random Forest Regressor')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 149 artists>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.YTick at 0x7fb6a9ae1c50>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a9868240>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8189320>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8285c18>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8285710>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82851d0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a827dcc0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a827d860>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82857b8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a827dfd0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8277f28>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8277a90>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8277588>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8277198>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82eeb70>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82ee668>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82ee160>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82e8c18>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82eec18>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8277eb8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82e8d30>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82e8550>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82e82e8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82dfb38>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82df630>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82df128>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82d9be0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82df668>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8285898>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8277710>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82d9438>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82d0eb8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82d0a20>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82d04e0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82caf28>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82caac8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82ca5c0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82d0048>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82d0f98>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82ca2e8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82c2f28>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82c2ac8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82c25c0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82c20b8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82bcb70>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82bc668>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82bc160>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6bc99d320>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a81ad358>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a81896a0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8492668>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8df3358>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a987bb00>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a84e09e8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a84c9978>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a84e0cf8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8d97908>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8e53240>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a84c2780>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a84b9eb8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82bc5c0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82c2a20>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82ca518>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a9403d30>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82ca438>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a84c2860>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a85f5908>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8dd4518>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82b3c88>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82b3748>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82b3198>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a832cbe0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a832c668>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82b3080>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8e53b70>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a832cc88>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8325c50>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a83256d8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8325160>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a831eba8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a831e630>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a831e0b8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a831ec50>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8325898>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8316c88>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8316630>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a83160b8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8310b00>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8310588>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8307f60>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8307a90>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8310a58>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a83166a0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a83072e8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8301ef0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a83019e8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8301470>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82f9e80>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82f9978>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82f9400>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82f98d0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8301668>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8316588>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82f38d0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82f3358>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a836bda0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a836b860>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a836b320>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8364d30>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a836b898>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a82f3828>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8364b38>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8364240>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a835bc88>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a835b748>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a835b1d0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8355c50>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a83556d8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8355208>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8355c88>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8364278>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a838f240>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a838f780>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a838fcc0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8391240>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8391780>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8391cc0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8391828>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a838fb38>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8355cf8>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8394780>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8394cc0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a839a240>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a839a780>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a839acc0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a839c240>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a839a748>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a8394828>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a839c438>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a839ccc0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a839e240>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a839e780>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a839ecc0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a83a2240>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a83a2780>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a83a2cc0>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a83a2208>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a839cc88>,\n",
       "  <matplotlib.axis.YTick at 0x7fb6a83a8240>],\n",
       " <a list of 149 Text yticklabel objects>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Relative Importance')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x2160 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Variable Importance\n",
    "# i = 20\n",
    "\n",
    "importances =  rfr.feature_importances_\n",
    "top_features = []\n",
    "\n",
    "imp_1 = sorted(np.array(importances), reverse = True)\n",
    "sum = 0 \n",
    "for i in range(len(imp_1)):\n",
    "    sum = sum + imp_1[i]\n",
    "    if sum > 0.8:\n",
    "        n = i\n",
    "        print(\"\\n Top variables (> 80% variation): \", n+1, \"\\n Total variation explained: \", sum)\n",
    "        break\n",
    "                       \n",
    "i = n\n",
    "\n",
    "indices = np.argsort(importances)[-(i+1):]\n",
    " \n",
    "print(\"Top features: \")\n",
    "\n",
    "features = X_train.columns\n",
    "\n",
    "top_features = features[indices]\n",
    "top_features\n",
    "\n",
    "plt.figure(figsize=(10,30))\n",
    "plt.title('Feature Importances - Random Forest Regressor')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), features[indices])\n",
    "plt.xlabel('Relative Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Total important variables:  149\n",
      "\n",
      " Important features: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['nm_rpch-1', 'ngdp_d_sa_pchy-7', 'pcpi_pch-2', 'pcpi_sa_pcha-2',\n",
       "       'ncg_rpch', 'ntdd_rpchy-7', 'le-4', 'ngdp_r-4', 'nmg_rpch-7',\n",
       "       'nfi_rpch-5',\n",
       "       ...\n",
       "       'ncp_rpch-1', 'pcpi_pchy', 'ngdp_rpchy-9', 'pcpi_sa_pchy', 'nm_rpch',\n",
       "       'lur-5', 'nfdd_rpch', 'nfbrgdp', 'ncp_rpch', 'nfi_rpch'],\n",
       "      dtype='object', length=149)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Important features\n",
    "\n",
    "print(\"\\n Total important variables: \", len(top_features))\n",
    "print(\"\\n Important features: \\n\")\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 149)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(48, 149)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train & Test set with important features from random forest\n",
    "\n",
    "X_train_random_forest = X_train[top_features]\n",
    "X_train_random_forest.shape\n",
    "\n",
    "X_test_random_forest = X_test[top_features]\n",
    "X_test_random_forest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set reshaped: (97, 149, 1)\n",
      "test set reshaped: (48, 149, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the training and test sets\n",
    "X_train_1 = np.array(X_train_random_forest).reshape(X_train_random_forest.shape[0], X_train_random_forest.shape[1],1)\n",
    "print(\"training set reshaped:\", X_train_1.shape)\n",
    "\n",
    "X_test_1 = np.array(X_test_random_forest).reshape(X_test_random_forest.shape[0], X_test_random_forest.shape[1],1)\n",
    "print(\"test set reshaped:\", X_test_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from keras.layers import LSTM\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.init(project='IMF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "      'name': 'val_loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "                  'epochs': {'values': [10, 20, 40]},\n",
    "                  'batch_size': {'values': [16, 32, 64]},\n",
    "                  'nn_units': {'values': [16, 32, 64]},\n",
    "                  'dout_rate': {'values': [0.0, 0.2, 0.4, 0.6]}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 78fkzlsk\n",
      "Sweep URL: https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\n"
     ]
    }
   ],
   "source": [
    "sweepid = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    # Specify the hyperparameter to be tuned along with\n",
    "    # an initial value\n",
    "    config_defaults = {\n",
    "        'epochs': 10,\n",
    "        'batch_size': 16,\n",
    "        'nn_units': 16,\n",
    "        'dout_rate': 0.0\n",
    "    }\n",
    "    \n",
    "    # Initialize a new wandb run\n",
    "    wandb.init(config=config_defaults)\n",
    "    \n",
    "    # Config is a variable that holds and saves hyperparameters and inputs\n",
    "    config = wandb.config\n",
    "    \n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = config.nn_units, return_sequences = True, input_shape = (X_train_1.shape[1], 1)))\n",
    "    model.add(Dropout(config.dout_rate))\n",
    "    model.add(LSTM(units = config.nn_units, return_sequences = True))\n",
    "    model.add(Dropout(config.dout_rate))\n",
    "    model.add(LSTM(units = config.nn_units, return_sequences = True))\n",
    "    model.add(Dropout(config.dout_rate))\n",
    "    model.add(LSTM(units = config.nn_units, return_sequences = True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units = config.nn_units, return_sequences = True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units = config.nn_units, return_sequences = True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units = config.nn_units))\n",
    "    model.add(Dense(units = 1, activation='relu'))\n",
    "    \n",
    "    # Complie the model\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['mse'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_1, y_train, epochs = config.epochs, batch_size = config.batch_size, \n",
    "              validation_split = 0.3, callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: 08uegvbk with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 32\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: 08uegvbk\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/08uegvbk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/08uegvbk</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 8s 119ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 3s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: 08uegvbk \n",
      "\n",
      "wandb: Agent Starting Run: ncdje6i7 with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 64\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: ncdje6i7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/ncdje6i7\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/ncdje6i7</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 6s 93ms/step - loss: 17.5952 - mse: 17.5952 - val_loss: 6.3653 - val_mse: 6.3653\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 3s 46ms/step - loss: 7.1583 - mse: 7.1583 - val_loss: 6.0974 - val_mse: 6.0974\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.7199 - mse: 5.7199 - val_loss: 6.5350 - val_mse: 6.5350\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3486 - mse: 5.3486 - val_loss: 5.2170 - val_mse: 5.2170\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3152 - mse: 5.3152 - val_loss: 5.1233 - val_mse: 5.1233\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2814 - mse: 5.2814 - val_loss: 5.2572 - val_mse: 5.2572\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2591 - mse: 5.2592 - val_loss: 5.4915 - val_mse: 5.4915\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2709 - mse: 5.2709 - val_loss: 5.3884 - val_mse: 5.3884\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3077 - mse: 5.3077 - val_loss: 5.7324 - val_mse: 5.7324\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2769 - mse: 5.2769 - val_loss: 5.6527 - val_mse: 5.6527\n",
      "wandb: Agent Finished Run: ncdje6i7 \n",
      "\n",
      "wandb: Agent Starting Run: m2brmooo with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 16\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: m2brmooo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/m2brmooo\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/m2brmooo</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 6s 89ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: m2brmooo \n",
      "\n",
      "wandb: Agent Starting Run: c8i1cayi with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 32\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: c8i1cayi\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/c8i1cayi\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/c8i1cayi</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 6s 88ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: c8i1cayi \n",
      "\n",
      "wandb: Agent Starting Run: 9v1yq73g with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 64\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: 9v1yq73g\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/9v1yq73g\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/9v1yq73g</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 6s 91ms/step - loss: 17.8125 - mse: 17.8125 - val_loss: 6.9546 - val_mse: 6.9546\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 7.6474 - mse: 7.6474 - val_loss: 5.9079 - val_mse: 5.9079\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.7698 - mse: 5.7698 - val_loss: 6.7350 - val_mse: 6.7350\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.3874 - mse: 5.3874 - val_loss: 5.2682 - val_mse: 5.2682\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3168 - mse: 5.3168 - val_loss: 5.1102 - val_mse: 5.1102\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2887 - mse: 5.2887 - val_loss: 5.2181 - val_mse: 5.2181\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2634 - mse: 5.2634 - val_loss: 5.4588 - val_mse: 5.4588\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2670 - mse: 5.2670 - val_loss: 5.3901 - val_mse: 5.3901\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3113 - mse: 5.3113 - val_loss: 5.7539 - val_mse: 5.7539\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2796 - mse: 5.2796 - val_loss: 5.6718 - val_mse: 5.6718\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2619 - mse: 5.2619 - val_loss: 5.6894 - val_mse: 5.6894\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2676 - mse: 5.2676 - val_loss: 5.4727 - val_mse: 5.4727\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2525 - mse: 5.2525 - val_loss: 5.0940 - val_mse: 5.0940\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.3099 - mse: 5.3099 - val_loss: 5.0038 - val_mse: 5.0038\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.3470 - mse: 5.3470 - val_loss: 5.0892 - val_mse: 5.0892\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3370 - mse: 5.3370 - val_loss: 5.1142 - val_mse: 5.1142\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.3702 - mse: 5.3702 - val_loss: 5.4430 - val_mse: 5.4430\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2308 - mse: 5.2308 - val_loss: 5.2427 - val_mse: 5.2427\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2571 - mse: 5.2571 - val_loss: 5.1811 - val_mse: 5.1811\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2787 - mse: 5.2787 - val_loss: 5.1381 - val_mse: 5.1381\n",
      "wandb: Agent Finished Run: 9v1yq73g \n",
      "\n",
      "wandb: Agent Starting Run: v5pc00nf with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 16\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: v5pc00nf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/v5pc00nf\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/v5pc00nf</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 6s 87ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: v5pc00nf \n",
      "\n",
      "wandb: Agent Starting Run: tpfndvsw with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 32\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: tpfndvsw\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/tpfndvsw\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/tpfndvsw</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 6s 87ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/40\n",
      "64/67 [===========================>..] - ETA: 0s - loss: 18.8008 - mse: 18.8008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error resolved after 0:00:28.396461, resuming normal operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 3s 42ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: tpfndvsw \n",
      "\n",
      "wandb: Agent Starting Run: go18ltrf with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 64\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: go18ltrf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/go18ltrf\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/go18ltrf</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 6s 89ms/step - loss: 17.5952 - mse: 17.5952 - val_loss: 6.3653 - val_mse: 6.3653\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 7.1583 - mse: 7.1583 - val_loss: 6.0974 - val_mse: 6.0974\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.7199 - mse: 5.7199 - val_loss: 6.5350 - val_mse: 6.5350\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.3486 - mse: 5.3486 - val_loss: 5.2170 - val_mse: 5.2170\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.3152 - mse: 5.3152 - val_loss: 5.1233 - val_mse: 5.1233\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2814 - mse: 5.2814 - val_loss: 5.2572 - val_mse: 5.2572\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2591 - mse: 5.2592 - val_loss: 5.4915 - val_mse: 5.4915\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2709 - mse: 5.2709 - val_loss: 5.3884 - val_mse: 5.3884\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.3077 - mse: 5.3077 - val_loss: 5.7324 - val_mse: 5.7324\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2769 - mse: 5.2769 - val_loss: 5.6527 - val_mse: 5.6527\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2595 - mse: 5.2595 - val_loss: 5.6840 - val_mse: 5.6840\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2669 - mse: 5.2669 - val_loss: 5.4773 - val_mse: 5.4773\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2521 - mse: 5.2521 - val_loss: 5.0983 - val_mse: 5.0983\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.3079 - mse: 5.3079 - val_loss: 5.0062 - val_mse: 5.0062\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.3458 - mse: 5.3458 - val_loss: 5.0895 - val_mse: 5.0895\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3373 - mse: 5.3373 - val_loss: 5.1128 - val_mse: 5.1128\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3699 - mse: 5.3699 - val_loss: 5.4388 - val_mse: 5.4388\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2310 - mse: 5.2310 - val_loss: 5.2418 - val_mse: 5.2418\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2572 - mse: 5.2572 - val_loss: 5.1820 - val_mse: 5.1820\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2783 - mse: 5.2783 - val_loss: 5.1396 - val_mse: 5.1396\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2958 - mse: 5.2958 - val_loss: 5.1163 - val_mse: 5.1163\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2950 - mse: 5.2950 - val_loss: 5.1520 - val_mse: 5.1520\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2944 - mse: 5.2944 - val_loss: 5.0764 - val_mse: 5.0764\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3319 - mse: 5.3319 - val_loss: 5.2061 - val_mse: 5.2061\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2652 - mse: 5.2652 - val_loss: 5.2573 - val_mse: 5.2573\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2533 - mse: 5.2533 - val_loss: 5.3472 - val_mse: 5.3472\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2565 - mse: 5.2565 - val_loss: 5.3134 - val_mse: 5.3134\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2437 - mse: 5.2437 - val_loss: 5.3568 - val_mse: 5.3568\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2508 - mse: 5.2508 - val_loss: 5.3404 - val_mse: 5.3404\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2549 - mse: 5.2549 - val_loss: 5.2672 - val_mse: 5.2672\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 3s 46ms/step - loss: 5.3428 - mse: 5.3428 - val_loss: 5.0319 - val_mse: 5.0319\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 5.3043 - mse: 5.3043 - val_loss: 5.2774 - val_mse: 5.2774\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.1969 - mse: 5.1969 - val_loss: 5.5773 - val_mse: 5.5773\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.3081 - mse: 5.3081 - val_loss: 6.0149 - val_mse: 6.0149\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.3227 - mse: 5.3227 - val_loss: 5.8995 - val_mse: 5.8995\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2703 - mse: 5.2703 - val_loss: 5.5245 - val_mse: 5.5245\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2990 - mse: 5.2990 - val_loss: 5.2491 - val_mse: 5.2491\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2614 - mse: 5.2614 - val_loss: 5.2337 - val_mse: 5.2337\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2576 - mse: 5.2576 - val_loss: 5.3408 - val_mse: 5.3408\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2281 - mse: 5.2281 - val_loss: 5.6287 - val_mse: 5.6287\n",
      "wandb: Agent Finished Run: go18ltrf \n",
      "\n",
      "wandb: Agent Starting Run: 8gmhaah5 with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 16\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: 8gmhaah5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/8gmhaah5\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/8gmhaah5</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 6s 88ms/step - loss: 19.1050 - mse: 19.1050 - val_loss: 12.9760 - val_mse: 12.9760\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 3s 37ms/step - loss: 18.6817 - mse: 18.6817 - val_loss: 12.3221 - val_mse: 12.3221\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 17.5423 - mse: 17.5423 - val_loss: 10.5768 - val_mse: 10.5768\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 14.5932 - mse: 14.5932 - val_loss: 8.1905 - val_mse: 8.1905\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 11.5390 - mse: 11.5390 - val_loss: 6.4787 - val_mse: 6.4787\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 9.1959 - mse: 9.1959 - val_loss: 5.4710 - val_mse: 5.4710\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 7.6652 - mse: 7.6652 - val_loss: 4.9542 - val_mse: 4.9542\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 6.5854 - mse: 6.5854 - val_loss: 4.7727 - val_mse: 4.7727\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 6.0595 - mse: 6.0595 - val_loss: 4.7740 - val_mse: 4.7740\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.5992 - mse: 5.5992 - val_loss: 4.8773 - val_mse: 4.8773\n",
      "wandb: Agent Finished Run: 8gmhaah5 \n",
      "\n",
      "wandb: Agent Starting Run: ozcngrmg with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 32\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: ozcngrmg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/ozcngrmg\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/ozcngrmg</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 6s 89ms/step - loss: 18.6422 - mse: 18.6422 - val_loss: 11.3038 - val_mse: 11.3038\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 14.7226 - mse: 14.7226 - val_loss: 5.9795 - val_mse: 5.9795\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 7.5723 - mse: 7.5723 - val_loss: 4.7959 - val_mse: 4.7959\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 5.4199 - mse: 5.4199 - val_loss: 5.4483 - val_mse: 5.4483\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 5.2092 - mse: 5.2092 - val_loss: 6.0471 - val_mse: 6.0471\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.3409 - mse: 5.3409 - val_loss: 6.0806 - val_mse: 6.0806\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.3638 - mse: 5.3638 - val_loss: 6.0076 - val_mse: 6.0076\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2406 - mse: 5.2406 - val_loss: 5.4894 - val_mse: 5.4894\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2298 - mse: 5.2298 - val_loss: 5.1691 - val_mse: 5.1691\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.3249 - mse: 5.3249 - val_loss: 5.0785 - val_mse: 5.0785\n",
      "wandb: Agent Finished Run: ozcngrmg \n",
      "\n",
      "wandb: Agent Starting Run: s1b60ueu with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 64\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: s1b60ueu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/s1b60ueu\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/s1b60ueu</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 6s 96ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 3s 48ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 3s 47ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 3s 47ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 18.9079 - mse: 18.9079 - val_loss: 11.4360 - val_mse: 11.4360\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 11.2962 - mse: 11.2962 - val_loss: 6.0035 - val_mse: 6.0035\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.8720 - mse: 5.8720 - val_loss: 6.8078 - val_mse: 6.8078\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 5.4051 - mse: 5.4051 - val_loss: 5.1567 - val_mse: 5.1567\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3067 - mse: 5.3067 - val_loss: 5.1061 - val_mse: 5.1061\n",
      "wandb: Agent Finished Run: s1b60ueu \n",
      "\n",
      "wandb: Agent Starting Run: isjswu4g with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 16\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: isjswu4g\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/isjswu4g\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/isjswu4g</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 6s 89ms/step - loss: 19.1050 - mse: 19.1050 - val_loss: 12.9760 - val_mse: 12.9760\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 18.6817 - mse: 18.6817 - val_loss: 12.3221 - val_mse: 12.3221\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 17.5423 - mse: 17.5423 - val_loss: 10.5768 - val_mse: 10.5768\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 14.5932 - mse: 14.5932 - val_loss: 8.1905 - val_mse: 8.1905\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 11.5390 - mse: 11.5390 - val_loss: 6.4787 - val_mse: 6.4787\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 9.1959 - mse: 9.1959 - val_loss: 5.4710 - val_mse: 5.4710\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 7.6652 - mse: 7.6652 - val_loss: 4.9542 - val_mse: 4.9542\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 6.5854 - mse: 6.5854 - val_loss: 4.7727 - val_mse: 4.7727\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 6.0595 - mse: 6.0595 - val_loss: 4.7740 - val_mse: 4.7740\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.5992 - mse: 5.5992 - val_loss: 4.8773 - val_mse: 4.8773\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.4271 - mse: 5.4271 - val_loss: 5.0368 - val_mse: 5.0368\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.3215 - mse: 5.3215 - val_loss: 5.1709 - val_mse: 5.1709\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2504 - mse: 5.2504 - val_loss: 5.2686 - val_mse: 5.2686\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2520 - mse: 5.2520 - val_loss: 5.3559 - val_mse: 5.3559\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2430 - mse: 5.2430 - val_loss: 5.3723 - val_mse: 5.3723\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2371 - mse: 5.2371 - val_loss: 5.4093 - val_mse: 5.4093\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2416 - mse: 5.2416 - val_loss: 5.4291 - val_mse: 5.4291\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2372 - mse: 5.2372 - val_loss: 5.4525 - val_mse: 5.4525\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2448 - mse: 5.2448 - val_loss: 5.5087 - val_mse: 5.5087\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2390 - mse: 5.2390 - val_loss: 5.5339 - val_mse: 5.5339\n",
      "wandb: Agent Finished Run: isjswu4g \n",
      "\n",
      "wandb: Agent Starting Run: eiopno94 with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 32\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: eiopno94\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/eiopno94\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/eiopno94</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 6s 90ms/step - loss: 18.6422 - mse: 18.6422 - val_loss: 11.3038 - val_mse: 11.3038\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 14.7192 - mse: 14.7192 - val_loss: 5.9695 - val_mse: 5.9695\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 7.5615 - mse: 7.5615 - val_loss: 4.7967 - val_mse: 4.7967\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 5.4186 - mse: 5.4186 - val_loss: 5.4490 - val_mse: 5.4490\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2094 - mse: 5.2094 - val_loss: 6.0464 - val_mse: 6.0464\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.3407 - mse: 5.3407 - val_loss: 6.0796 - val_mse: 6.0796\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.3636 - mse: 5.3636 - val_loss: 6.0069 - val_mse: 6.0069\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2405 - mse: 5.2405 - val_loss: 5.4890 - val_mse: 5.4890\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2298 - mse: 5.2298 - val_loss: 5.1689 - val_mse: 5.1689\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.3249 - mse: 5.3249 - val_loss: 5.0785 - val_mse: 5.0785\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.3032 - mse: 5.3032 - val_loss: 5.1298 - val_mse: 5.1298\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2768 - mse: 5.2768 - val_loss: 5.2104 - val_mse: 5.2104\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2611 - mse: 5.2611 - val_loss: 5.4372 - val_mse: 5.4372\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2687 - mse: 5.2687 - val_loss: 5.5891 - val_mse: 5.5891\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2488 - mse: 5.2488 - val_loss: 5.5744 - val_mse: 5.5744\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2350 - mse: 5.2350 - val_loss: 5.4536 - val_mse: 5.4536\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2557 - mse: 5.2557 - val_loss: 5.3573 - val_mse: 5.3573\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2411 - mse: 5.2411 - val_loss: 5.4182 - val_mse: 5.4182\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2721 - mse: 5.2721 - val_loss: 5.5811 - val_mse: 5.5811\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 5.2422 - mse: 5.2422 - val_loss: 5.5148 - val_mse: 5.5148\n",
      "wandb: Agent Finished Run: eiopno94 \n",
      "\n",
      "wandb: Agent Starting Run: 9se6eom1 with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 64\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: 9se6eom1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/9se6eom1\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/9se6eom1</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 6s 94ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 3s 46ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 18.9079 - mse: 18.9079 - val_loss: 11.4360 - val_mse: 11.4360\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 11.2962 - mse: 11.2962 - val_loss: 6.0035 - val_mse: 6.0035\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.8720 - mse: 5.8720 - val_loss: 6.8078 - val_mse: 6.8078\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.4051 - mse: 5.4051 - val_loss: 5.1567 - val_mse: 5.1567\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3067 - mse: 5.3067 - val_loss: 5.1061 - val_mse: 5.1061\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3137 - mse: 5.3137 - val_loss: 5.4257 - val_mse: 5.4257\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3248 - mse: 5.3248 - val_loss: 5.7135 - val_mse: 5.7135\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2397 - mse: 5.2397 - val_loss: 5.3912 - val_mse: 5.3912\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2458 - mse: 5.2458 - val_loss: 5.2036 - val_mse: 5.2036\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2725 - mse: 5.2725 - val_loss: 5.0457 - val_mse: 5.0457\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.4101 - mse: 5.4101 - val_loss: 5.4190 - val_mse: 5.4190\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2446 - mse: 5.2446 - val_loss: 5.7270 - val_mse: 5.7270\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2957 - mse: 5.2957 - val_loss: 6.0954 - val_mse: 6.0954\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3302 - mse: 5.3302 - val_loss: 5.7969 - val_mse: 5.7969\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2750 - mse: 5.2750 - val_loss: 5.5886 - val_mse: 5.5886\n",
      "wandb: Agent Finished Run: 9se6eom1 \n",
      "\n",
      "wandb: Agent Starting Run: l2wvd1ke with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 16\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: l2wvd1ke\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/l2wvd1ke\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/l2wvd1ke</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 6s 89ms/step - loss: 19.1050 - mse: 19.1050 - val_loss: 12.9760 - val_mse: 12.9760\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 18.6817 - mse: 18.6817 - val_loss: 12.3221 - val_mse: 12.3221\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 17.5423 - mse: 17.5423 - val_loss: 10.5768 - val_mse: 10.5768\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 14.5932 - mse: 14.5932 - val_loss: 8.1905 - val_mse: 8.1905\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 11.5390 - mse: 11.5390 - val_loss: 6.4787 - val_mse: 6.4787\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 9.1959 - mse: 9.1959 - val_loss: 5.4710 - val_mse: 5.4710\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 7.6652 - mse: 7.6652 - val_loss: 4.9542 - val_mse: 4.9542\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 6.5854 - mse: 6.5854 - val_loss: 4.7727 - val_mse: 4.7727\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 6.0595 - mse: 6.0595 - val_loss: 4.7740 - val_mse: 4.7740\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.5992 - mse: 5.5992 - val_loss: 4.8773 - val_mse: 4.8773\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.4271 - mse: 5.4271 - val_loss: 5.0368 - val_mse: 5.0368\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.3215 - mse: 5.3215 - val_loss: 5.1709 - val_mse: 5.1709\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2504 - mse: 5.2504 - val_loss: 5.2686 - val_mse: 5.2686\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2520 - mse: 5.2520 - val_loss: 5.3559 - val_mse: 5.3559\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2430 - mse: 5.2430 - val_loss: 5.3723 - val_mse: 5.3723\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2371 - mse: 5.2371 - val_loss: 5.4093 - val_mse: 5.4093\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2416 - mse: 5.2416 - val_loss: 5.4291 - val_mse: 5.4291\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2372 - mse: 5.2372 - val_loss: 5.4525 - val_mse: 5.4525\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2448 - mse: 5.2448 - val_loss: 5.5087 - val_mse: 5.5087\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2390 - mse: 5.2390 - val_loss: 5.5339 - val_mse: 5.5339\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2405 - mse: 5.2405 - val_loss: 5.5141 - val_mse: 5.5141\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2436 - mse: 5.2436 - val_loss: 5.4836 - val_mse: 5.4836\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2433 - mse: 5.2433 - val_loss: 5.5073 - val_mse: 5.5073\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2392 - mse: 5.2392 - val_loss: 5.4790 - val_mse: 5.4790\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2431 - mse: 5.2431 - val_loss: 5.4130 - val_mse: 5.4130\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2392 - mse: 5.2392 - val_loss: 5.3817 - val_mse: 5.3817\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2409 - mse: 5.2409 - val_loss: 5.3675 - val_mse: 5.3675\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2421 - mse: 5.2421 - val_loss: 5.3711 - val_mse: 5.3711\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2577 - mse: 5.2576 - val_loss: 5.4161 - val_mse: 5.4161\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2393 - mse: 5.2393 - val_loss: 5.3501 - val_mse: 5.3501\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2495 - mse: 5.2495 - val_loss: 5.2760 - val_mse: 5.2760\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2533 - mse: 5.2533 - val_loss: 5.2338 - val_mse: 5.2338\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2611 - mse: 5.2611 - val_loss: 5.2087 - val_mse: 5.2087\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2643 - mse: 5.2643 - val_loss: 5.1904 - val_mse: 5.1904\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2690 - mse: 5.2690 - val_loss: 5.2052 - val_mse: 5.2052\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2635 - mse: 5.2635 - val_loss: 5.2092 - val_mse: 5.2092\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2605 - mse: 5.2605 - val_loss: 5.2296 - val_mse: 5.2296\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2581 - mse: 5.2581 - val_loss: 5.2430 - val_mse: 5.2430\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2575 - mse: 5.2575 - val_loss: 5.2590 - val_mse: 5.2590\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2523 - mse: 5.2523 - val_loss: 5.2604 - val_mse: 5.2604\n",
      "wandb: Agent Finished Run: l2wvd1ke \n",
      "\n",
      "wandb: Agent Starting Run: 6dgz707g with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 32\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: 6dgz707g\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/6dgz707g\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/6dgz707g</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 6s 88ms/step - loss: 18.7500 - mse: 18.7500 - val_loss: 11.6912 - val_mse: 11.6912\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 15.5114 - mse: 15.5114 - val_loss: 6.5043 - val_mse: 6.5043\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 8.1381 - mse: 8.1381 - val_loss: 4.7661 - val_mse: 4.7661\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.4945 - mse: 5.4945 - val_loss: 5.4072 - val_mse: 5.4072\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 5.2011 - mse: 5.2011 - val_loss: 6.0803 - val_mse: 6.0803\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.3514 - mse: 5.3514 - val_loss: 6.1227 - val_mse: 6.1227\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.3733 - mse: 5.3733 - val_loss: 6.0325 - val_mse: 6.0325\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 3s 37ms/step - loss: 5.2437 - mse: 5.2437 - val_loss: 5.4992 - val_mse: 5.4992\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2291 - mse: 5.2291 - val_loss: 5.1714 - val_mse: 5.1714\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.3253 - mse: 5.3253 - val_loss: 5.0777 - val_mse: 5.0777\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.3036 - mse: 5.3036 - val_loss: 5.1275 - val_mse: 5.1275\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2778 - mse: 5.2778 - val_loss: 5.2072 - val_mse: 5.2072\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2615 - mse: 5.2615 - val_loss: 5.4324 - val_mse: 5.4324\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2685 - mse: 5.2685 - val_loss: 5.5845 - val_mse: 5.5845\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2485 - mse: 5.2485 - val_loss: 5.5722 - val_mse: 5.5722\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2350 - mse: 5.2350 - val_loss: 5.4540 - val_mse: 5.4540\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2555 - mse: 5.2555 - val_loss: 5.3590 - val_mse: 5.3590\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2410 - mse: 5.2410 - val_loss: 5.4195 - val_mse: 5.4195\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2718 - mse: 5.2718 - val_loss: 5.5812 - val_mse: 5.5812\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2422 - mse: 5.2422 - val_loss: 5.5152 - val_mse: 5.5152\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2416 - mse: 5.2416 - val_loss: 5.4959 - val_mse: 5.4959\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2392 - mse: 5.2392 - val_loss: 5.5212 - val_mse: 5.5212\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2473 - mse: 5.2473 - val_loss: 5.5350 - val_mse: 5.5350\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2506 - mse: 5.2506 - val_loss: 5.3996 - val_mse: 5.3996\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2389 - mse: 5.2389 - val_loss: 5.3668 - val_mse: 5.3668\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2415 - mse: 5.2415 - val_loss: 5.4225 - val_mse: 5.4225\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2377 - mse: 5.2377 - val_loss: 5.4492 - val_mse: 5.4492\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2389 - mse: 5.2389 - val_loss: 5.5153 - val_mse: 5.5153\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2309 - mse: 5.2309 - val_loss: 5.6785 - val_mse: 5.6785\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2560 - mse: 5.2560 - val_loss: 5.6833 - val_mse: 5.6833\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2715 - mse: 5.2715 - val_loss: 5.7726 - val_mse: 5.7726\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2784 - mse: 5.2784 - val_loss: 5.8282 - val_mse: 5.8282\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2808 - mse: 5.2808 - val_loss: 5.7902 - val_mse: 5.7902\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2742 - mse: 5.2742 - val_loss: 5.8032 - val_mse: 5.8032\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2769 - mse: 5.2769 - val_loss: 5.7575 - val_mse: 5.7575\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2677 - mse: 5.2677 - val_loss: 5.5247 - val_mse: 5.5247\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2392 - mse: 5.2392 - val_loss: 5.4538 - val_mse: 5.4538\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2416 - mse: 5.2416 - val_loss: 5.4581 - val_mse: 5.4581\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2383 - mse: 5.2383 - val_loss: 5.4934 - val_mse: 5.4934\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2475 - mse: 5.2475 - val_loss: 5.6003 - val_mse: 5.6003\n",
      "wandb: Agent Finished Run: 6dgz707g \n",
      "\n",
      "wandb: Agent Starting Run: 6p42c9uz with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 64\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: 6p42c9uz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/6p42c9uz\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/6p42c9uz</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 6s 92ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 18.9045 - mse: 18.9045 - val_loss: 11.3918 - val_mse: 11.3918\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 11.1606 - mse: 11.1606 - val_loss: 6.1023 - val_mse: 6.1023\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.9047 - mse: 5.9047 - val_loss: 6.7851 - val_mse: 6.7851\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3961 - mse: 5.3961 - val_loss: 5.1416 - val_mse: 5.1416\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3100 - mse: 5.3100 - val_loss: 5.1100 - val_mse: 5.1100\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3127 - mse: 5.3127 - val_loss: 5.4330 - val_mse: 5.4330\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3242 - mse: 5.3242 - val_loss: 5.7144 - val_mse: 5.7144\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2395 - mse: 5.2395 - val_loss: 5.3898 - val_mse: 5.3898\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2458 - mse: 5.2458 - val_loss: 5.2026 - val_mse: 5.2026\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2727 - mse: 5.2727 - val_loss: 5.0461 - val_mse: 5.0461\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.4091 - mse: 5.4091 - val_loss: 5.4168 - val_mse: 5.4168\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2444 - mse: 5.2444 - val_loss: 5.7248 - val_mse: 5.7248\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2952 - mse: 5.2952 - val_loss: 6.0945 - val_mse: 6.0945\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3301 - mse: 5.3301 - val_loss: 5.7986 - val_mse: 5.7986\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2753 - mse: 5.2753 - val_loss: 5.5909 - val_mse: 5.5909\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2857 - mse: 5.2857 - val_loss: 5.4503 - val_mse: 5.4503\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2423 - mse: 5.2423 - val_loss: 5.7047 - val_mse: 5.7047\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2869 - mse: 5.2869 - val_loss: 5.6755 - val_mse: 5.6755\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2467 - mse: 5.2467 - val_loss: 5.3753 - val_mse: 5.3753\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2377 - mse: 5.2377 - val_loss: 5.3076 - val_mse: 5.3076\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2654 - mse: 5.2654 - val_loss: 5.3107 - val_mse: 5.3107\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3389 - mse: 5.3389 - val_loss: 5.8083 - val_mse: 5.8083\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2962 - mse: 5.2962 - val_loss: 5.7186 - val_mse: 5.7186\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2734 - mse: 5.2734 - val_loss: 5.8146 - val_mse: 5.8146\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2948 - mse: 5.2948 - val_loss: 5.5369 - val_mse: 5.5369\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2604 - mse: 5.2604 - val_loss: 5.5686 - val_mse: 5.5686\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2508 - mse: 5.2508 - val_loss: 5.4951 - val_mse: 5.4951\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2252 - mse: 5.2252 - val_loss: 5.2208 - val_mse: 5.2208\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2823 - mse: 5.2823 - val_loss: 5.1171 - val_mse: 5.1171\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2719 - mse: 5.2719 - val_loss: 5.3359 - val_mse: 5.3359\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2497 - mse: 5.2497 - val_loss: 5.5236 - val_mse: 5.5236\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2429 - mse: 5.2429 - val_loss: 5.5229 - val_mse: 5.5229\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2581 - mse: 5.2581 - val_loss: 5.4135 - val_mse: 5.4135\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2420 - mse: 5.2420 - val_loss: 5.3491 - val_mse: 5.3491\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2823 - mse: 5.2823 - val_loss: 5.2686 - val_mse: 5.2686\n",
      "wandb: Agent Finished Run: 6p42c9uz \n",
      "\n",
      "wandb: Agent Starting Run: qheku42x with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 16\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: qheku42x\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/qheku42x\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/qheku42x</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 6s 88ms/step - loss: 19.0577 - mse: 19.0577 - val_loss: 12.8535 - val_mse: 12.8535\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 18.4476 - mse: 18.4476 - val_loss: 11.9912 - val_mse: 11.9912\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 16.9803 - mse: 16.9803 - val_loss: 9.9146 - val_mse: 9.9146\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 13.7060 - mse: 13.7060 - val_loss: 7.6394 - val_mse: 7.6394\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 10.8512 - mse: 10.8512 - val_loss: 6.1768 - val_mse: 6.1768\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 8.7643 - mse: 8.7643 - val_loss: 5.3151 - val_mse: 5.3151\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 7.3926 - mse: 7.3926 - val_loss: 4.8932 - val_mse: 4.8932\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 6.4322 - mse: 6.4322 - val_loss: 4.7614 - val_mse: 4.7614\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.9700 - mse: 5.9700 - val_loss: 4.7881 - val_mse: 4.7881\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.5489 - mse: 5.5489 - val_loss: 4.9043 - val_mse: 4.9043\n",
      "wandb: Agent Finished Run: qheku42x \n",
      "\n",
      "wandb: Agent Starting Run: c4yeu1uv with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 32\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: c4yeu1uv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/c4yeu1uv\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/c4yeu1uv</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 6s 89ms/step - loss: 18.7657 - mse: 18.7657 - val_loss: 11.7468 - val_mse: 11.7468\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 15.6210 - mse: 15.6210 - val_loss: 6.5425 - val_mse: 6.5425\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 8.1909 - mse: 8.1909 - val_loss: 4.7619 - val_mse: 4.7619\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 5.5133 - mse: 5.5133 - val_loss: 5.4120 - val_mse: 5.4120\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2016 - mse: 5.2016 - val_loss: 6.1162 - val_mse: 6.1162\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.3621 - mse: 5.3621 - val_loss: 6.1398 - val_mse: 6.1398\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.3746 - mse: 5.3746 - val_loss: 6.0282 - val_mse: 6.0282\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2399 - mse: 5.2399 - val_loss: 5.4804 - val_mse: 5.4804\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2303 - mse: 5.2303 - val_loss: 5.1551 - val_mse: 5.1551\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.3308 - mse: 5.3308 - val_loss: 5.0677 - val_mse: 5.0677\n",
      "wandb: Agent Finished Run: c4yeu1uv \n",
      "\n",
      "wandb: Agent Starting Run: qe0bxqe9 with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 64\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: qe0bxqe9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/qe0bxqe9\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/qe0bxqe9</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 6s 95ms/step - loss: 18.0921 - mse: 18.0921 - val_loss: 7.4957 - val_mse: 7.4957\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 3s 48ms/step - loss: 7.2836 - mse: 7.2836 - val_loss: 6.1509 - val_mse: 6.1509\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 3s 46ms/step - loss: 5.4039 - mse: 5.4039 - val_loss: 6.0110 - val_mse: 6.0110\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 5.3514 - mse: 5.3514 - val_loss: 5.8196 - val_mse: 5.8196\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2845 - mse: 5.2845 - val_loss: 5.1016 - val_mse: 5.1016\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3095 - mse: 5.3095 - val_loss: 5.2026 - val_mse: 5.2026\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2421 - mse: 5.2421 - val_loss: 5.5182 - val_mse: 5.5182\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2666 - mse: 5.2666 - val_loss: 5.9634 - val_mse: 5.9634\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3404 - mse: 5.3404 - val_loss: 5.9841 - val_mse: 5.9841\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2878 - mse: 5.2878 - val_loss: 5.5923 - val_mse: 5.5923\n",
      "wandb: Agent Finished Run: qe0bxqe9 \n",
      "\n",
      "wandb: Agent Starting Run: anc5h0om with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 16\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: anc5h0om\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/anc5h0om\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/anc5h0om</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 6s 94ms/step - loss: 19.0577 - mse: 19.0577 - val_loss: 12.8535 - val_mse: 12.8535\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 18.4476 - mse: 18.4476 - val_loss: 11.9912 - val_mse: 11.9912\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 16.9803 - mse: 16.9803 - val_loss: 9.9146 - val_mse: 9.9146\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 13.7060 - mse: 13.7060 - val_loss: 7.6394 - val_mse: 7.6394\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 10.8512 - mse: 10.8512 - val_loss: 6.1768 - val_mse: 6.1768\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 8.7643 - mse: 8.7643 - val_loss: 5.3151 - val_mse: 5.3151\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 7.3926 - mse: 7.3926 - val_loss: 4.8932 - val_mse: 4.8932\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 6.4322 - mse: 6.4322 - val_loss: 4.7614 - val_mse: 4.7614\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.9700 - mse: 5.9700 - val_loss: 4.7881 - val_mse: 4.7881\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.5489 - mse: 5.5489 - val_loss: 4.9043 - val_mse: 4.9043\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 3s 37ms/step - loss: 5.3996 - mse: 5.3996 - val_loss: 5.0685 - val_mse: 5.0685\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.3079 - mse: 5.3079 - val_loss: 5.1997 - val_mse: 5.1997\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2458 - mse: 5.2458 - val_loss: 5.2913 - val_mse: 5.2913\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2492 - mse: 5.2492 - val_loss: 5.3725 - val_mse: 5.3725\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 3s 37ms/step - loss: 5.2427 - mse: 5.2427 - val_loss: 5.3829 - val_mse: 5.3829\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2374 - mse: 5.2374 - val_loss: 5.4157 - val_mse: 5.4157\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2417 - mse: 5.2417 - val_loss: 5.4324 - val_mse: 5.4324\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2372 - mse: 5.2372 - val_loss: 5.4538 - val_mse: 5.4538\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 3s 37ms/step - loss: 5.2447 - mse: 5.2447 - val_loss: 5.5088 - val_mse: 5.5088\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2389 - mse: 5.2389 - val_loss: 5.5333 - val_mse: 5.5333\n",
      "wandb: Agent Finished Run: anc5h0om \n",
      "\n",
      "wandb: Agent Starting Run: johysudm with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 32\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: johysudm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/johysudm\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/johysudm</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 6s 88ms/step - loss: 18.6580 - mse: 18.6580 - val_loss: 11.3553 - val_mse: 11.3553\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 14.8089 - mse: 14.8089 - val_loss: 5.9711 - val_mse: 5.9711\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 7.5865 - mse: 7.5865 - val_loss: 4.7870 - val_mse: 4.7870\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.4343 - mse: 5.4344 - val_loss: 5.4570 - val_mse: 5.4570\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2103 - mse: 5.2103 - val_loss: 6.0879 - val_mse: 6.0879\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.3527 - mse: 5.3527 - val_loss: 6.1030 - val_mse: 6.1030\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.3662 - mse: 5.3662 - val_loss: 6.0069 - val_mse: 6.0069\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2375 - mse: 5.2375 - val_loss: 5.4711 - val_mse: 5.4711\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2311 - mse: 5.2311 - val_loss: 5.1494 - val_mse: 5.1494\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.3322 - mse: 5.3322 - val_loss: 5.0661 - val_mse: 5.0661\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.3076 - mse: 5.3076 - val_loss: 5.1266 - val_mse: 5.1266\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2765 - mse: 5.2765 - val_loss: 5.2150 - val_mse: 5.2150\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2607 - mse: 5.2607 - val_loss: 5.4501 - val_mse: 5.4501\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2693 - mse: 5.2693 - val_loss: 5.6039 - val_mse: 5.6039\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2500 - mse: 5.2500 - val_loss: 5.5839 - val_mse: 5.5839\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 3s 37ms/step - loss: 5.2352 - mse: 5.2352 - val_loss: 5.4563 - val_mse: 5.4563\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2562 - mse: 5.2562 - val_loss: 5.3557 - val_mse: 5.3557\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2413 - mse: 5.2413 - val_loss: 5.4153 - val_mse: 5.4153\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2720 - mse: 5.2720 - val_loss: 5.5786 - val_mse: 5.5786\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2423 - mse: 5.2423 - val_loss: 5.5125 - val_mse: 5.5125\n",
      "wandb: Agent Finished Run: johysudm \n",
      "\n",
      "wandb: Agent Starting Run: yi8rkna5 with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 64\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: yi8rkna5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/yi8rkna5\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/yi8rkna5</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 6s 94ms/step - loss: 18.3771 - mse: 18.3771 - val_loss: 8.8078 - val_mse: 8.8078\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 3s 46ms/step - loss: 8.3573 - mse: 8.3573 - val_loss: 5.7825 - val_mse: 5.7825\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 5.2941 - mse: 5.2941 - val_loss: 6.2284 - val_mse: 6.2284\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 5.4348 - mse: 5.4348 - val_loss: 5.9688 - val_mse: 5.9688\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2963 - mse: 5.2963 - val_loss: 5.0750 - val_mse: 5.0750\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3208 - mse: 5.3208 - val_loss: 5.1587 - val_mse: 5.1587\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2485 - mse: 5.2485 - val_loss: 5.4916 - val_mse: 5.4916\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2639 - mse: 5.2639 - val_loss: 5.9695 - val_mse: 5.9695\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3444 - mse: 5.3444 - val_loss: 6.0128 - val_mse: 6.0128\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2944 - mse: 5.2944 - val_loss: 5.6206 - val_mse: 5.6206\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2180 - mse: 5.2180 - val_loss: 5.2901 - val_mse: 5.2901\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2524 - mse: 5.2524 - val_loss: 5.2573 - val_mse: 5.2573\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2615 - mse: 5.2615 - val_loss: 5.2293 - val_mse: 5.2293\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2703 - mse: 5.2703 - val_loss: 5.2844 - val_mse: 5.2844\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2478 - mse: 5.2478 - val_loss: 5.1963 - val_mse: 5.1963\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3449 - mse: 5.3449 - val_loss: 5.4725 - val_mse: 5.4725\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2502 - mse: 5.2502 - val_loss: 5.6611 - val_mse: 5.6611\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2754 - mse: 5.2754 - val_loss: 5.9757 - val_mse: 5.9757\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3052 - mse: 5.3052 - val_loss: 5.7578 - val_mse: 5.7578\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2712 - mse: 5.2712 - val_loss: 5.6094 - val_mse: 5.6094\n",
      "wandb: Agent Finished Run: yi8rkna5 \n",
      "\n",
      "wandb: Agent Starting Run: m300c7cp with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 16\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: m300c7cp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/m300c7cp\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/m300c7cp</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 6s 88ms/step - loss: 19.0577 - mse: 19.0577 - val_loss: 12.8535 - val_mse: 12.8535\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 18.4476 - mse: 18.4476 - val_loss: 11.9912 - val_mse: 11.9912\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 16.9803 - mse: 16.9803 - val_loss: 9.9146 - val_mse: 9.9146\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 13.7060 - mse: 13.7060 - val_loss: 7.6394 - val_mse: 7.6394\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 10.8512 - mse: 10.8512 - val_loss: 6.1768 - val_mse: 6.1768\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 8.7643 - mse: 8.7643 - val_loss: 5.3151 - val_mse: 5.3151\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 7.3926 - mse: 7.3926 - val_loss: 4.8932 - val_mse: 4.8932\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 6.4322 - mse: 6.4322 - val_loss: 4.7614 - val_mse: 4.7614\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.9700 - mse: 5.9700 - val_loss: 4.7881 - val_mse: 4.7881\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.5489 - mse: 5.5489 - val_loss: 4.9043 - val_mse: 4.9043\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.3996 - mse: 5.3996 - val_loss: 5.0685 - val_mse: 5.0685\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.3079 - mse: 5.3079 - val_loss: 5.1997 - val_mse: 5.1997\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2458 - mse: 5.2458 - val_loss: 5.2913 - val_mse: 5.2913\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2492 - mse: 5.2492 - val_loss: 5.3724 - val_mse: 5.3724\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2428 - mse: 5.2428 - val_loss: 5.3828 - val_mse: 5.3828\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2374 - mse: 5.2374 - val_loss: 5.4156 - val_mse: 5.4156\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2417 - mse: 5.2417 - val_loss: 5.4323 - val_mse: 5.4323\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2372 - mse: 5.2372 - val_loss: 5.4537 - val_mse: 5.4537\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2447 - mse: 5.2447 - val_loss: 5.5087 - val_mse: 5.5087\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2389 - mse: 5.2389 - val_loss: 5.5333 - val_mse: 5.5333\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2404 - mse: 5.2404 - val_loss: 5.5130 - val_mse: 5.5130\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2440 - mse: 5.2440 - val_loss: 5.4825 - val_mse: 5.4825\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2433 - mse: 5.2433 - val_loss: 5.5065 - val_mse: 5.5065\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2393 - mse: 5.2393 - val_loss: 5.4783 - val_mse: 5.4783\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2428 - mse: 5.2428 - val_loss: 5.4125 - val_mse: 5.4125\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2395 - mse: 5.2395 - val_loss: 5.3814 - val_mse: 5.3814\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2409 - mse: 5.2409 - val_loss: 5.3674 - val_mse: 5.3674\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2420 - mse: 5.2420 - val_loss: 5.3712 - val_mse: 5.3712\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2572 - mse: 5.2572 - val_loss: 5.4166 - val_mse: 5.4166\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2389 - mse: 5.2389 - val_loss: 5.3505 - val_mse: 5.3505\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2493 - mse: 5.2493 - val_loss: 5.2763 - val_mse: 5.2763\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2531 - mse: 5.2531 - val_loss: 5.2341 - val_mse: 5.2341\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2616 - mse: 5.2616 - val_loss: 5.2091 - val_mse: 5.2091\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2641 - mse: 5.2641 - val_loss: 5.1908 - val_mse: 5.1908\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2690 - mse: 5.2690 - val_loss: 5.2058 - val_mse: 5.2058\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2632 - mse: 5.2632 - val_loss: 5.2098 - val_mse: 5.2098\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2606 - mse: 5.2606 - val_loss: 5.2303 - val_mse: 5.2303\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2580 - mse: 5.2580 - val_loss: 5.2439 - val_mse: 5.2439\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2573 - mse: 5.2573 - val_loss: 5.2599 - val_mse: 5.2599\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 5.2525 - mse: 5.2525 - val_loss: 5.2613 - val_mse: 5.2613\n",
      "wandb: Agent Finished Run: m300c7cp \n",
      "\n",
      "wandb: Agent Starting Run: h6tyf4ce with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 32\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: h6tyf4ce\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/h6tyf4ce\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/h6tyf4ce</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 6s 87ms/step - loss: 18.7657 - mse: 18.7657 - val_loss: 11.7468 - val_mse: 11.7468\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 15.6210 - mse: 15.6210 - val_loss: 6.5425 - val_mse: 6.5425\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 8.1909 - mse: 8.1909 - val_loss: 4.7619 - val_mse: 4.7619\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.5133 - mse: 5.5133 - val_loss: 5.4097 - val_mse: 5.4097\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2011 - mse: 5.2011 - val_loss: 6.1149 - val_mse: 6.1149\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.3618 - mse: 5.3618 - val_loss: 6.1402 - val_mse: 6.1402\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.3749 - mse: 5.3749 - val_loss: 6.0291 - val_mse: 6.0291\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2401 - mse: 5.2401 - val_loss: 5.4809 - val_mse: 5.4809\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2303 - mse: 5.2303 - val_loss: 5.1549 - val_mse: 5.1549\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.3309 - mse: 5.3309 - val_loss: 5.0674 - val_mse: 5.0674\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.3074 - mse: 5.3074 - val_loss: 5.1232 - val_mse: 5.1232\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2782 - mse: 5.2782 - val_loss: 5.2084 - val_mse: 5.2084\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2617 - mse: 5.2617 - val_loss: 5.4387 - val_mse: 5.4387\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2687 - mse: 5.2687 - val_loss: 5.5928 - val_mse: 5.5928\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2492 - mse: 5.2492 - val_loss: 5.5788 - val_mse: 5.5788\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2354 - mse: 5.2354 - val_loss: 5.4577 - val_mse: 5.4577\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2557 - mse: 5.2557 - val_loss: 5.3603 - val_mse: 5.3603\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2410 - mse: 5.2410 - val_loss: 5.4197 - val_mse: 5.4197\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2716 - mse: 5.2716 - val_loss: 5.5808 - val_mse: 5.5808\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2425 - mse: 5.2425 - val_loss: 5.5147 - val_mse: 5.5147\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2415 - mse: 5.2415 - val_loss: 5.4955 - val_mse: 5.4955\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2393 - mse: 5.2393 - val_loss: 5.5209 - val_mse: 5.5209\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2472 - mse: 5.2472 - val_loss: 5.5349 - val_mse: 5.5349\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2505 - mse: 5.2505 - val_loss: 5.3998 - val_mse: 5.3998\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2387 - mse: 5.2387 - val_loss: 5.3671 - val_mse: 5.3671\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2414 - mse: 5.2414 - val_loss: 5.4230 - val_mse: 5.4230\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2377 - mse: 5.2377 - val_loss: 5.4497 - val_mse: 5.4497\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2389 - mse: 5.2389 - val_loss: 5.5159 - val_mse: 5.5159\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2309 - mse: 5.2309 - val_loss: 5.6792 - val_mse: 5.6792\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2560 - mse: 5.2560 - val_loss: 5.6841 - val_mse: 5.6841\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2719 - mse: 5.2719 - val_loss: 5.7731 - val_mse: 5.7731\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2783 - mse: 5.2783 - val_loss: 5.8285 - val_mse: 5.8285\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2808 - mse: 5.2808 - val_loss: 5.7906 - val_mse: 5.7906\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2740 - mse: 5.2740 - val_loss: 5.8036 - val_mse: 5.8036\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2766 - mse: 5.2766 - val_loss: 5.7579 - val_mse: 5.7579\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2677 - mse: 5.2677 - val_loss: 5.5256 - val_mse: 5.5256\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2393 - mse: 5.2393 - val_loss: 5.4547 - val_mse: 5.4547\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2413 - mse: 5.2413 - val_loss: 5.4589 - val_mse: 5.4589\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2380 - mse: 5.2380 - val_loss: 5.4941 - val_mse: 5.4941\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2475 - mse: 5.2475 - val_loss: 5.6010 - val_mse: 5.6010\n",
      "wandb: Agent Finished Run: h6tyf4ce \n",
      "\n",
      "wandb: Agent Starting Run: 37oapf81 with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 64\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: 37oapf81\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/37oapf81\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/37oapf81</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 6s 92ms/step - loss: 18.0921 - mse: 18.0921 - val_loss: 7.4957 - val_mse: 7.4957\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 7.2836 - mse: 7.2836 - val_loss: 6.1509 - val_mse: 6.1509\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.4039 - mse: 5.4039 - val_loss: 6.0110 - val_mse: 6.0110\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.3514 - mse: 5.3514 - val_loss: 5.8196 - val_mse: 5.8196\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2845 - mse: 5.2845 - val_loss: 5.1016 - val_mse: 5.1016\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3095 - mse: 5.3095 - val_loss: 5.2026 - val_mse: 5.2026\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2421 - mse: 5.2421 - val_loss: 5.5182 - val_mse: 5.5182\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2666 - mse: 5.2666 - val_loss: 5.9634 - val_mse: 5.9634\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3404 - mse: 5.3404 - val_loss: 5.9841 - val_mse: 5.9841\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2878 - mse: 5.2878 - val_loss: 5.5923 - val_mse: 5.5923\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2181 - mse: 5.2181 - val_loss: 5.2751 - val_mse: 5.2751\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2567 - mse: 5.2567 - val_loss: 5.2605 - val_mse: 5.2605\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2615 - mse: 5.2615 - val_loss: 5.2402 - val_mse: 5.2402\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2690 - mse: 5.2690 - val_loss: 5.2954 - val_mse: 5.2954\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2458 - mse: 5.2458 - val_loss: 5.1947 - val_mse: 5.1947\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3478 - mse: 5.3478 - val_loss: 5.4760 - val_mse: 5.4760\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2515 - mse: 5.2515 - val_loss: 5.6666 - val_mse: 5.6666\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2774 - mse: 5.2774 - val_loss: 5.9877 - val_mse: 5.9877\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3069 - mse: 5.3069 - val_loss: 5.7464 - val_mse: 5.7464\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2695 - mse: 5.2695 - val_loss: 5.5926 - val_mse: 5.5926\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2828 - mse: 5.2828 - val_loss: 5.4772 - val_mse: 5.4772\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2451 - mse: 5.2451 - val_loss: 5.7479 - val_mse: 5.7479\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2911 - mse: 5.2911 - val_loss: 5.6856 - val_mse: 5.6856\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2469 - mse: 5.2469 - val_loss: 5.3473 - val_mse: 5.3473\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2401 - mse: 5.2401 - val_loss: 5.2813 - val_mse: 5.2813\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2680 - mse: 5.2680 - val_loss: 5.3031 - val_mse: 5.3031\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3499 - mse: 5.3499 - val_loss: 5.8499 - val_mse: 5.8499\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.3068 - mse: 5.3068 - val_loss: 5.7309 - val_mse: 5.7309\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2753 - mse: 5.2753 - val_loss: 5.8147 - val_mse: 5.8147\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2973 - mse: 5.2973 - val_loss: 5.5068 - val_mse: 5.5068\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2602 - mse: 5.2602 - val_loss: 5.5473 - val_mse: 5.5473\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2519 - mse: 5.2519 - val_loss: 5.4832 - val_mse: 5.4832\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2258 - mse: 5.2258 - val_loss: 5.2064 - val_mse: 5.2064\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2864 - mse: 5.2864 - val_loss: 5.1098 - val_mse: 5.1098\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2728 - mse: 5.2728 - val_loss: 5.3503 - val_mse: 5.3503\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2505 - mse: 5.2505 - val_loss: 5.5508 - val_mse: 5.5508\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2445 - mse: 5.2445 - val_loss: 5.5338 - val_mse: 5.5338\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2614 - mse: 5.2614 - val_loss: 5.4027 - val_mse: 5.4027\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2428 - mse: 5.2428 - val_loss: 5.3321 - val_mse: 5.3321\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2861 - mse: 5.2861 - val_loss: 5.2543 - val_mse: 5.2543\n",
      "wandb: Agent Finished Run: 37oapf81 \n",
      "\n",
      "wandb: Agent Starting Run: v8voimje with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 16\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: v8voimje\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/v8voimje\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/v8voimje</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 6s 86ms/step - loss: 18.9406 - mse: 18.9406 - val_loss: 12.6240 - val_mse: 12.6240\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 17.9982 - mse: 17.9982 - val_loss: 11.2898 - val_mse: 11.2898\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 15.8239 - mse: 15.8239 - val_loss: 8.7614 - val_mse: 8.7614\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 12.3427 - mse: 12.3427 - val_loss: 6.9987 - val_mse: 6.9987\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 10.0615 - mse: 10.0615 - val_loss: 5.8778 - val_mse: 5.8778\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 8.3262 - mse: 8.3262 - val_loss: 5.1618 - val_mse: 5.1618\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 7.0659 - mse: 7.0659 - val_loss: 4.8070 - val_mse: 4.8070\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 6.1051 - mse: 6.1051 - val_loss: 4.7694 - val_mse: 4.7694\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.6972 - mse: 5.6972 - val_loss: 4.8966 - val_mse: 4.8966\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.3594 - mse: 5.3594 - val_loss: 5.0844 - val_mse: 5.0844\n",
      "wandb: Agent Finished Run: v8voimje \n",
      "\n",
      "wandb: Agent Starting Run: 8m8ncs53 with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 32\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: 8m8ncs53\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/8m8ncs53\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/8m8ncs53</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 6s 88ms/step - loss: 18.6279 - mse: 18.6279 - val_loss: 11.2442 - val_mse: 11.2442\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 14.6932 - mse: 14.6932 - val_loss: 5.9015 - val_mse: 5.9015\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 7.5813 - mse: 7.5813 - val_loss: 4.7780 - val_mse: 4.7780\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.4502 - mse: 5.4502 - val_loss: 5.4915 - val_mse: 5.4915\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2163 - mse: 5.2163 - val_loss: 6.1735 - val_mse: 6.1735\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.3774 - mse: 5.3774 - val_loss: 6.1472 - val_mse: 6.1472\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.3700 - mse: 5.3700 - val_loss: 6.0017 - val_mse: 6.0017\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2298 - mse: 5.2298 - val_loss: 5.4158 - val_mse: 5.4158\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2382 - mse: 5.2382 - val_loss: 5.0890 - val_mse: 5.0890\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.3564 - mse: 5.3564 - val_loss: 5.0285 - val_mse: 5.0285\n",
      "wandb: Agent Finished Run: 8m8ncs53 \n",
      "\n",
      "wandb: Agent Starting Run: 3p25ry5b with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 64\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: 3p25ry5b\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/3p25ry5b\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/3p25ry5b</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 6s 95ms/step - loss: 18.0549 - mse: 18.0549 - val_loss: 7.3874 - val_mse: 7.3874\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 3s 47ms/step - loss: 7.1624 - mse: 7.1624 - val_loss: 6.2169 - val_mse: 6.2169\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 3s 47ms/step - loss: 5.4232 - mse: 5.4232 - val_loss: 5.9473 - val_mse: 5.9473\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 3s 47ms/step - loss: 5.3359 - mse: 5.3359 - val_loss: 5.7918 - val_mse: 5.7918\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 5.2833 - mse: 5.2833 - val_loss: 5.1099 - val_mse: 5.1099\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3076 - mse: 5.3076 - val_loss: 5.2301 - val_mse: 5.2301\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2385 - mse: 5.2385 - val_loss: 5.5449 - val_mse: 5.5449\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2713 - mse: 5.2713 - val_loss: 5.9694 - val_mse: 5.9694\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3387 - mse: 5.3387 - val_loss: 5.9634 - val_mse: 5.9634\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2830 - mse: 5.2830 - val_loss: 5.5714 - val_mse: 5.5714\n",
      "wandb: Agent Finished Run: 3p25ry5b \n",
      "\n",
      "wandb: Agent Starting Run: pv2k9tiy with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 16\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: pv2k9tiy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/pv2k9tiy\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/pv2k9tiy</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 6s 90ms/step - loss: 18.9406 - mse: 18.9406 - val_loss: 12.6240 - val_mse: 12.6240\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 17.9982 - mse: 17.9982 - val_loss: 11.2898 - val_mse: 11.2898\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 15.8239 - mse: 15.8239 - val_loss: 8.7614 - val_mse: 8.7614\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 12.3427 - mse: 12.3427 - val_loss: 6.9987 - val_mse: 6.9987\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 10.0615 - mse: 10.0615 - val_loss: 5.8778 - val_mse: 5.8778\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 8.3262 - mse: 8.3262 - val_loss: 5.1618 - val_mse: 5.1618\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 7.0659 - mse: 7.0659 - val_loss: 4.8070 - val_mse: 4.8070\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 6.1051 - mse: 6.1051 - val_loss: 4.7694 - val_mse: 4.7694\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.6972 - mse: 5.6972 - val_loss: 4.8966 - val_mse: 4.8966\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.3594 - mse: 5.3594 - val_loss: 5.0844 - val_mse: 5.0844\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2911 - mse: 5.2911 - val_loss: 5.2970 - val_mse: 5.2970\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2529 - mse: 5.2529 - val_loss: 5.4271 - val_mse: 5.4271\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2329 - mse: 5.2329 - val_loss: 5.4839 - val_mse: 5.4839\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2423 - mse: 5.2423 - val_loss: 5.5237 - val_mse: 5.5237\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2430 - mse: 5.2430 - val_loss: 5.4805 - val_mse: 5.4805\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2400 - mse: 5.2400 - val_loss: 5.4786 - val_mse: 5.4786\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.2443 - mse: 5.2443 - val_loss: 5.4672 - val_mse: 5.4672\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2409 - mse: 5.2409 - val_loss: 5.4706 - val_mse: 5.4706\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2456 - mse: 5.2456 - val_loss: 5.5213 - val_mse: 5.5213\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2400 - mse: 5.2400 - val_loss: 5.5397 - val_mse: 5.5397\n",
      "wandb: Agent Finished Run: pv2k9tiy \n",
      "\n",
      "wandb: Agent Starting Run: 3u337w4n with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 32\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: 3u337w4n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/3u337w4n\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/3u337w4n</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 6s 89ms/step - loss: 18.6698 - mse: 18.6698 - val_loss: 11.4668 - val_mse: 11.4668\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 15.1223 - mse: 15.1223 - val_loss: 6.1571 - val_mse: 6.1571\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 7.8619 - mse: 7.8619 - val_loss: 4.7651 - val_mse: 4.7652\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.4926 - mse: 5.4926 - val_loss: 5.4668 - val_mse: 5.4668\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2113 - mse: 5.2113 - val_loss: 6.1937 - val_mse: 6.1937\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 5.3843 - mse: 5.3843 - val_loss: 6.1700 - val_mse: 6.1700\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.3751 - mse: 5.3751 - val_loss: 6.0114 - val_mse: 6.0114\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2297 - mse: 5.2297 - val_loss: 5.4132 - val_mse: 5.4132\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2387 - mse: 5.2387 - val_loss: 5.0859 - val_mse: 5.0859\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.3580 - mse: 5.3580 - val_loss: 5.0254 - val_mse: 5.0254\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.3233 - mse: 5.3233 - val_loss: 5.1132 - val_mse: 5.1132\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2756 - mse: 5.2756 - val_loss: 5.2252 - val_mse: 5.2252\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2592 - mse: 5.2592 - val_loss: 5.4817 - val_mse: 5.4817\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2727 - mse: 5.2727 - val_loss: 5.6393 - val_mse: 5.6393\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2528 - mse: 5.2528 - val_loss: 5.6048 - val_mse: 5.6048\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2357 - mse: 5.2357 - val_loss: 5.4596 - val_mse: 5.4596\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.2576 - mse: 5.2576 - val_loss: 5.3487 - val_mse: 5.3487\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2423 - mse: 5.2423 - val_loss: 5.4054 - val_mse: 5.4054\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2723 - mse: 5.2723 - val_loss: 5.5704 - val_mse: 5.5704\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2420 - mse: 5.2420 - val_loss: 5.5037 - val_mse: 5.5037\n",
      "wandb: Agent Finished Run: 3u337w4n \n",
      "\n",
      "wandb: Agent Starting Run: abmvujqo with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 64\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: abmvujqo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/abmvujqo\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/abmvujqo</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 6s 93ms/step - loss: 18.3504 - mse: 18.3504 - val_loss: 8.7762 - val_mse: 8.7762\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 3s 46ms/step - loss: 8.2905 - mse: 8.2905 - val_loss: 5.8337 - val_mse: 5.8337\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 5.3084 - mse: 5.3084 - val_loss: 6.1820 - val_mse: 6.1820\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 5.4210 - mse: 5.4210 - val_loss: 5.9501 - val_mse: 5.9501\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 5.2952 - mse: 5.2952 - val_loss: 5.0827 - val_mse: 5.0827\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 5.3182 - mse: 5.3182 - val_loss: 5.1829 - val_mse: 5.1829\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 5.2437 - mse: 5.2437 - val_loss: 5.5254 - val_mse: 5.5254\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 3s 47ms/step - loss: 5.2706 - mse: 5.2706 - val_loss: 5.9915 - val_mse: 5.9915\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 3s 47ms/step - loss: 5.3462 - mse: 5.3462 - val_loss: 5.9875 - val_mse: 5.9875\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2866 - mse: 5.2866 - val_loss: 5.5753 - val_mse: 5.5753\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2176 - mse: 5.2176 - val_loss: 5.2640 - val_mse: 5.2640\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2592 - mse: 5.2592 - val_loss: 5.2593 - val_mse: 5.2593\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2616 - mse: 5.2616 - val_loss: 5.2459 - val_mse: 5.2459\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2679 - mse: 5.2679 - val_loss: 5.3014 - val_mse: 5.3014\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2445 - mse: 5.2445 - val_loss: 5.1987 - val_mse: 5.1987\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3449 - mse: 5.3449 - val_loss: 5.4718 - val_mse: 5.4718\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 5.2511 - mse: 5.2511 - val_loss: 5.6590 - val_mse: 5.6590\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 5.2749 - mse: 5.2749 - val_loss: 5.9774 - val_mse: 5.9774\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 5.3047 - mse: 5.3047 - val_loss: 5.7514 - val_mse: 5.7514\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2705 - mse: 5.2705 - val_loss: 5.6032 - val_mse: 5.6032\n",
      "wandb: Agent Finished Run: abmvujqo \n",
      "\n",
      "wandb: Agent Starting Run: eig7f6io with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 16\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: eig7f6io\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/eig7f6io\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/eig7f6io</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 6s 88ms/step - loss: 18.9406 - mse: 18.9406 - val_loss: 12.6240 - val_mse: 12.6240\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 17.9982 - mse: 17.9982 - val_loss: 11.2898 - val_mse: 11.2898\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 3s 39ms/step - loss: 15.8239 - mse: 15.8239 - val_loss: 8.7614 - val_mse: 8.7614\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 12.3427 - mse: 12.3427 - val_loss: 6.9987 - val_mse: 6.9987\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 10.0615 - mse: 10.0615 - val_loss: 5.8778 - val_mse: 5.8778\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 8.3262 - mse: 8.3262 - val_loss: 5.1618 - val_mse: 5.1618\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 7.0659 - mse: 7.0659 - val_loss: 4.8070 - val_mse: 4.8070\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 6.1051 - mse: 6.1051 - val_loss: 4.7694 - val_mse: 4.7694\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.6972 - mse: 5.6972 - val_loss: 4.8966 - val_mse: 4.8966\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.3594 - mse: 5.3594 - val_loss: 5.0844 - val_mse: 5.0844\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2911 - mse: 5.2911 - val_loss: 5.2970 - val_mse: 5.2970\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 3s 37ms/step - loss: 5.2529 - mse: 5.2529 - val_loss: 5.4271 - val_mse: 5.4271\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2329 - mse: 5.2329 - val_loss: 5.4839 - val_mse: 5.4839\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2423 - mse: 5.2423 - val_loss: 5.5237 - val_mse: 5.5237\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2430 - mse: 5.2430 - val_loss: 5.4805 - val_mse: 5.4805\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2400 - mse: 5.2400 - val_loss: 5.4786 - val_mse: 5.4786\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2443 - mse: 5.2443 - val_loss: 5.4672 - val_mse: 5.4672\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2409 - mse: 5.2409 - val_loss: 5.4706 - val_mse: 5.4706\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2456 - mse: 5.2456 - val_loss: 5.5213 - val_mse: 5.5213\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2400 - mse: 5.2400 - val_loss: 5.5397 - val_mse: 5.5397\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2405 - mse: 5.2405 - val_loss: 5.5086 - val_mse: 5.5086\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2453 - mse: 5.2453 - val_loss: 5.4690 - val_mse: 5.4690\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2430 - mse: 5.2430 - val_loss: 5.4970 - val_mse: 5.4970\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2393 - mse: 5.2393 - val_loss: 5.4645 - val_mse: 5.4645\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2441 - mse: 5.2441 - val_loss: 5.3886 - val_mse: 5.3886\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2411 - mse: 5.2411 - val_loss: 5.3559 - val_mse: 5.3559\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2421 - mse: 5.2421 - val_loss: 5.3447 - val_mse: 5.3447\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2436 - mse: 5.2436 - val_loss: 5.3552 - val_mse: 5.3552\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2636 - mse: 5.2636 - val_loss: 5.4152 - val_mse: 5.4152\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2387 - mse: 5.2387 - val_loss: 5.3387 - val_mse: 5.3387\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2514 - mse: 5.2514 - val_loss: 5.2531 - val_mse: 5.2531\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2586 - mse: 5.2586 - val_loss: 5.2073 - val_mse: 5.2073\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2686 - mse: 5.2686 - val_loss: 5.1829 - val_mse: 5.1829\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2703 - mse: 5.2703 - val_loss: 5.1671 - val_mse: 5.1671\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2771 - mse: 5.2771 - val_loss: 5.1907 - val_mse: 5.1907\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2660 - mse: 5.2660 - val_loss: 5.2006 - val_mse: 5.2006\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2618 - mse: 5.2618 - val_loss: 5.2294 - val_mse: 5.2294\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2586 - mse: 5.2586 - val_loss: 5.2484 - val_mse: 5.2484\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2564 - mse: 5.2564 - val_loss: 5.2689 - val_mse: 5.2689\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 5.2522 - mse: 5.2522 - val_loss: 5.2702 - val_mse: 5.2702\n",
      "wandb: Agent Finished Run: eig7f6io \n",
      "\n",
      "wandb: Agent Starting Run: ac8ulitd with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 32\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: ac8ulitd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/ac8ulitd\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/ac8ulitd</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 6s 88ms/step - loss: 18.6698 - mse: 18.6698 - val_loss: 11.4668 - val_mse: 11.4668\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 3s 40ms/step - loss: 15.1368 - mse: 15.1368 - val_loss: 6.1713 - val_mse: 6.1713\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 7.8776 - mse: 7.8776 - val_loss: 4.7644 - val_mse: 4.7644\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 3s 38ms/step - loss: 5.4959 - mse: 5.4959 - val_loss: 5.4642 - val_mse: 5.4642\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2107 - mse: 5.2107 - val_loss: 6.1943 - val_mse: 6.1943\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.3846 - mse: 5.3846 - val_loss: 6.1720 - val_mse: 6.1720\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.3756 - mse: 5.3756 - val_loss: 6.0130 - val_mse: 6.0130\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2299 - mse: 5.2299 - val_loss: 5.4142 - val_mse: 5.4142\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 2s 37ms/step - loss: 5.2385 - mse: 5.2385 - val_loss: 5.0861 - val_mse: 5.0862\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.3580 - mse: 5.3580 - val_loss: 5.0253 - val_mse: 5.0253\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.3234 - mse: 5.3234 - val_loss: 5.1129 - val_mse: 5.1129\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2758 - mse: 5.2758 - val_loss: 5.2246 - val_mse: 5.2246\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2593 - mse: 5.2593 - val_loss: 5.4814 - val_mse: 5.4814\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2727 - mse: 5.2727 - val_loss: 5.6393 - val_mse: 5.6393\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2528 - mse: 5.2528 - val_loss: 5.6050 - val_mse: 5.6050\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2357 - mse: 5.2357 - val_loss: 5.4598 - val_mse: 5.4598\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2576 - mse: 5.2576 - val_loss: 5.3490 - val_mse: 5.3490\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2423 - mse: 5.2423 - val_loss: 5.4054 - val_mse: 5.4054\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2723 - mse: 5.2723 - val_loss: 5.5703 - val_mse: 5.5703\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2420 - mse: 5.2420 - val_loss: 5.5036 - val_mse: 5.5036\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2411 - mse: 5.2411 - val_loss: 5.4871 - val_mse: 5.4871\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2395 - mse: 5.2395 - val_loss: 5.5172 - val_mse: 5.5172\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2486 - mse: 5.2486 - val_loss: 5.5347 - val_mse: 5.5347\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2509 - mse: 5.2509 - val_loss: 5.3962 - val_mse: 5.3962\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2391 - mse: 5.2391 - val_loss: 5.3638 - val_mse: 5.3638\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2419 - mse: 5.2419 - val_loss: 5.4225 - val_mse: 5.4225\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2377 - mse: 5.2377 - val_loss: 5.4509 - val_mse: 5.4509\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2387 - mse: 5.2387 - val_loss: 5.5197 - val_mse: 5.5197\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2310 - mse: 5.2310 - val_loss: 5.6891 - val_mse: 5.6891\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2577 - mse: 5.2577 - val_loss: 5.6899 - val_mse: 5.6899\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2732 - mse: 5.2732 - val_loss: 5.7796 - val_mse: 5.7796\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2796 - mse: 5.2796 - val_loss: 5.8336 - val_mse: 5.8336\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2822 - mse: 5.2822 - val_loss: 5.7891 - val_mse: 5.7891\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2740 - mse: 5.2740 - val_loss: 5.8006 - val_mse: 5.8006\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2761 - mse: 5.2761 - val_loss: 5.7516 - val_mse: 5.7516\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2672 - mse: 5.2672 - val_loss: 5.5097 - val_mse: 5.5097\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2386 - mse: 5.2386 - val_loss: 5.4400 - val_mse: 5.4400\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2412 - mse: 5.2412 - val_loss: 5.4492 - val_mse: 5.4492\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 2s 36ms/step - loss: 5.2369 - mse: 5.2369 - val_loss: 5.4905 - val_mse: 5.4905\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 2s 35ms/step - loss: 5.2477 - mse: 5.2477 - val_loss: 5.6053 - val_mse: 5.6053\n",
      "wandb: Agent Finished Run: ac8ulitd \n",
      "\n",
      "wandb: Agent Starting Run: mj0x8m25 with config:\n",
      "\tbatch_size: 16\n",
      "\tnn_units: 64\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: mj0x8m25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/mj0x8m25\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/mj0x8m25</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 6s 92ms/step - loss: 18.0549 - mse: 18.0549 - val_loss: 7.3874 - val_mse: 7.3874\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 7.1624 - mse: 7.1624 - val_loss: 6.2169 - val_mse: 6.2169\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.4232 - mse: 5.4232 - val_loss: 5.9473 - val_mse: 5.9473\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.3359 - mse: 5.3359 - val_loss: 5.7918 - val_mse: 5.7918\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 3s 47ms/step - loss: 5.2833 - mse: 5.2833 - val_loss: 5.1099 - val_mse: 5.1099\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.3076 - mse: 5.3076 - val_loss: 5.2301 - val_mse: 5.2301\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2385 - mse: 5.2385 - val_loss: 5.5449 - val_mse: 5.5449\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2713 - mse: 5.2713 - val_loss: 5.9694 - val_mse: 5.9694\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.3387 - mse: 5.3387 - val_loss: 5.9634 - val_mse: 5.9634\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2830 - mse: 5.2830 - val_loss: 5.5714 - val_mse: 5.5714\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2181 - mse: 5.2181 - val_loss: 5.2682 - val_mse: 5.2682\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2591 - mse: 5.2591 - val_loss: 5.2656 - val_mse: 5.2656\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2603 - mse: 5.2603 - val_loss: 5.2489 - val_mse: 5.2489\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2675 - mse: 5.2675 - val_loss: 5.3013 - val_mse: 5.3013\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 3s 45ms/step - loss: 5.2446 - mse: 5.2446 - val_loss: 5.1947 - val_mse: 5.1947\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 3s 46ms/step - loss: 5.3474 - mse: 5.3474 - val_loss: 5.4743 - val_mse: 5.4743\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 3s 46ms/step - loss: 5.2515 - mse: 5.2515 - val_loss: 5.6645 - val_mse: 5.6645\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 5.2764 - mse: 5.2764 - val_loss: 5.9850 - val_mse: 5.9850\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3061 - mse: 5.3061 - val_loss: 5.7506 - val_mse: 5.7506\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2703 - mse: 5.2703 - val_loss: 5.5984 - val_mse: 5.5984\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2833 - mse: 5.2833 - val_loss: 5.4817 - val_mse: 5.4817\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2455 - mse: 5.2455 - val_loss: 5.7463 - val_mse: 5.7463\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2904 - mse: 5.2904 - val_loss: 5.6847 - val_mse: 5.6847\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2469 - mse: 5.2469 - val_loss: 5.3530 - val_mse: 5.3530\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 5.2391 - mse: 5.2391 - val_loss: 5.2857 - val_mse: 5.2857\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2679 - mse: 5.2679 - val_loss: 5.3041 - val_mse: 5.3041\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3474 - mse: 5.3474 - val_loss: 5.8401 - val_mse: 5.8401\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.3037 - mse: 5.3037 - val_loss: 5.7320 - val_mse: 5.7320\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2756 - mse: 5.2756 - val_loss: 5.8177 - val_mse: 5.8177\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2965 - mse: 5.2965 - val_loss: 5.5188 - val_mse: 5.5188\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2599 - mse: 5.2599 - val_loss: 5.5540 - val_mse: 5.5540\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2511 - mse: 5.2511 - val_loss: 5.4862 - val_mse: 5.4862\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2256 - mse: 5.2256 - val_loss: 5.2117 - val_mse: 5.2117\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2850 - mse: 5.2850 - val_loss: 5.1130 - val_mse: 5.1130\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2723 - mse: 5.2723 - val_loss: 5.3458 - val_mse: 5.3458\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2502 - mse: 5.2502 - val_loss: 5.5410 - val_mse: 5.5410\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2437 - mse: 5.2437 - val_loss: 5.5305 - val_mse: 5.5305\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 3s 41ms/step - loss: 5.2602 - mse: 5.2602 - val_loss: 5.4081 - val_mse: 5.4081\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2424 - mse: 5.2424 - val_loss: 5.3395 - val_mse: 5.3395\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 3s 42ms/step - loss: 5.2845 - mse: 5.2845 - val_loss: 5.2605 - val_mse: 5.2605\n",
      "wandb: Agent Finished Run: mj0x8m25 \n",
      "\n",
      "wandb: Agent Starting Run: 3zr5fion with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 16\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: 3zr5fion\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/3zr5fion\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/3zr5fion</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 69ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: 3zr5fion \n",
      "\n",
      "wandb: Agent Starting Run: ely2brbw with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 32\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: ely2brbw\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/ely2brbw\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/ely2brbw</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 71ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: ely2brbw \n",
      "\n",
      "wandb: Agent Starting Run: 868a3rdn with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 64\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: 868a3rdn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/868a3rdn\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/868a3rdn</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 76ms/step - loss: 18.9166 - mse: 18.9166 - val_loss: 11.5102 - val_mse: 11.5102\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 15.5594 - mse: 15.5594 - val_loss: 5.6161 - val_mse: 5.6161\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 7.5322 - mse: 7.5322 - val_loss: 5.1218 - val_mse: 5.1218\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.2277 - mse: 5.2277 - val_loss: 6.9330 - val_mse: 6.9330\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.7056 - mse: 5.7056 - val_loss: 7.1325 - val_mse: 7.1325\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.6438 - mse: 5.6438 - val_loss: 6.0500 - val_mse: 6.0500\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.2734 - mse: 5.2734 - val_loss: 5.2428 - val_mse: 5.2428\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.3413 - mse: 5.3413 - val_loss: 4.9404 - val_mse: 4.9404\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.3965 - mse: 5.3965 - val_loss: 5.0256 - val_mse: 5.0256\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.3055 - mse: 5.3055 - val_loss: 5.2233 - val_mse: 5.2233\n",
      "wandb: Agent Finished Run: 868a3rdn \n",
      "\n",
      "wandb: Agent Starting Run: hhpwb797 with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 16\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: hhpwb797\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/hhpwb797\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/hhpwb797</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 71ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: hhpwb797 \n",
      "\n",
      "wandb: Agent Starting Run: 5twdoagc with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 32\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: 5twdoagc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/5twdoagc\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/5twdoagc</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 72ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: 5twdoagc \n",
      "\n",
      "wandb: Agent Starting Run: fcuko7x2 with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 64\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: fcuko7x2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/fcuko7x2\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/fcuko7x2</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 79ms/step - loss: 18.8417 - mse: 18.8417 - val_loss: 10.8224 - val_mse: 10.8224\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 14.2418 - mse: 14.2418 - val_loss: 5.0948 - val_mse: 5.0948\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 6.7119 - mse: 6.7119 - val_loss: 5.4092 - val_mse: 5.4092\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2394 - mse: 5.2394 - val_loss: 6.8075 - val_mse: 6.8075\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.6322 - mse: 5.6322 - val_loss: 6.8649 - val_mse: 6.8649\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.5388 - mse: 5.5388 - val_loss: 5.8975 - val_mse: 5.8975\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2502 - mse: 5.2502 - val_loss: 5.1989 - val_mse: 5.1989\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.3486 - mse: 5.3486 - val_loss: 4.9403 - val_mse: 4.9403\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.3959 - mse: 5.3959 - val_loss: 5.0530 - val_mse: 5.0530\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2866 - mse: 5.2866 - val_loss: 5.2798 - val_mse: 5.2798\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2359 - mse: 5.2359 - val_loss: 5.6645 - val_mse: 5.6645\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2654 - mse: 5.2654 - val_loss: 5.8221 - val_mse: 5.8221\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2684 - mse: 5.2684 - val_loss: 5.5295 - val_mse: 5.5295\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2339 - mse: 5.2339 - val_loss: 5.2570 - val_mse: 5.2570\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2620 - mse: 5.2620 - val_loss: 5.0756 - val_mse: 5.0756\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.3535 - mse: 5.3535 - val_loss: 4.9568 - val_mse: 4.9568\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.3953 - mse: 5.3953 - val_loss: 4.9479 - val_mse: 4.9479\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.4042 - mse: 5.4042 - val_loss: 4.8874 - val_mse: 4.8874\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.4585 - mse: 5.4585 - val_loss: 4.8892 - val_mse: 4.8892\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.4541 - mse: 5.4541 - val_loss: 4.8998 - val_mse: 4.8998\n",
      "wandb: Agent Finished Run: fcuko7x2 \n",
      "\n",
      "wandb: Agent Starting Run: ife8wg96 with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 16\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: ife8wg96\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/ife8wg96\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/ife8wg96</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 70ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 20ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 20ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: ife8wg96 \n",
      "\n",
      "wandb: Agent Starting Run: b4463b5h with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 32\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: b4463b5h\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/b4463b5h\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/b4463b5h</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 71ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: b4463b5h \n",
      "\n",
      "wandb: Agent Starting Run: k4hh6nfg with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 64\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: k4hh6nfg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/k4hh6nfg\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/k4hh6nfg</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 74ms/step - loss: 18.8471 - mse: 18.8471 - val_loss: 11.1923 - val_mse: 11.1923\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 14.9677 - mse: 14.9677 - val_loss: 5.3719 - val_mse: 5.3719\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 7.1660 - mse: 7.1660 - val_loss: 5.2411 - val_mse: 5.2411\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2279 - mse: 5.2279 - val_loss: 6.9156 - val_mse: 6.9156\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.6893 - mse: 5.6893 - val_loss: 7.0391 - val_mse: 7.0391\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.6052 - mse: 5.6052 - val_loss: 5.9882 - val_mse: 5.9882\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2623 - mse: 5.2623 - val_loss: 5.2116 - val_mse: 5.2116\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.3503 - mse: 5.3503 - val_loss: 4.9301 - val_mse: 4.9301\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.4056 - mse: 5.4056 - val_loss: 5.0301 - val_mse: 5.0301\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2992 - mse: 5.2992 - val_loss: 5.2470 - val_mse: 5.2470\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2392 - mse: 5.2392 - val_loss: 5.6268 - val_mse: 5.6268\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2601 - mse: 5.2601 - val_loss: 5.7985 - val_mse: 5.7985\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2652 - mse: 5.2652 - val_loss: 5.5293 - val_mse: 5.5293\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2341 - mse: 5.2341 - val_loss: 5.2685 - val_mse: 5.2685\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2593 - mse: 5.2593 - val_loss: 5.0893 - val_mse: 5.0893\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.3460 - mse: 5.3460 - val_loss: 4.9679 - val_mse: 4.9679\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.3859 - mse: 5.3859 - val_loss: 4.9566 - val_mse: 4.9566\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.3966 - mse: 5.3966 - val_loss: 4.8921 - val_mse: 4.8921\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.4529 - mse: 5.4529 - val_loss: 4.8913 - val_mse: 4.8913\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.4517 - mse: 5.4517 - val_loss: 4.8996 - val_mse: 4.8996\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.4417 - mse: 5.4417 - val_loss: 4.9173 - val_mse: 4.9173\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.4218 - mse: 5.4218 - val_loss: 4.9452 - val_mse: 4.9452\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.3958 - mse: 5.3958 - val_loss: 4.9323 - val_mse: 4.9323\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.4145 - mse: 5.4145 - val_loss: 4.9882 - val_mse: 4.9882\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.3515 - mse: 5.3515 - val_loss: 5.0527 - val_mse: 5.0527\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.3065 - mse: 5.3065 - val_loss: 5.1369 - val_mse: 5.1369\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2799 - mse: 5.2799 - val_loss: 5.1882 - val_mse: 5.1882\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2654 - mse: 5.2654 - val_loss: 5.2623 - val_mse: 5.2623\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2498 - mse: 5.2498 - val_loss: 5.2971 - val_mse: 5.2971\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2491 - mse: 5.2491 - val_loss: 5.2121 - val_mse: 5.2121\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.3126 - mse: 5.3126 - val_loss: 5.0807 - val_mse: 5.0807\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.3030 - mse: 5.3030 - val_loss: 5.1760 - val_mse: 5.1760\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2474 - mse: 5.2474 - val_loss: 5.3571 - val_mse: 5.3571\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2603 - mse: 5.2603 - val_loss: 5.6298 - val_mse: 5.6298\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2499 - mse: 5.2499 - val_loss: 5.7715 - val_mse: 5.7715\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2681 - mse: 5.2681 - val_loss: 5.7196 - val_mse: 5.7196\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2685 - mse: 5.2685 - val_loss: 5.6105 - val_mse: 5.6105\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2489 - mse: 5.2489 - val_loss: 5.5247 - val_mse: 5.5247\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2486 - mse: 5.2486 - val_loss: 5.5156 - val_mse: 5.5156\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2429 - mse: 5.2429 - val_loss: 5.6054 - val_mse: 5.6054\n",
      "wandb: Agent Finished Run: k4hh6nfg \n",
      "\n",
      "wandb: Agent Starting Run: fsaqmec7 with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 16\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: fsaqmec7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/fsaqmec7\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/fsaqmec7</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 71ms/step - loss: 19.1214 - mse: 19.1214 - val_loss: 13.0323 - val_mse: 13.0323\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 18.9206 - mse: 18.9206 - val_loss: 12.7544 - val_mse: 12.7544\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 18.5298 - mse: 18.5298 - val_loss: 12.2506 - val_mse: 12.2506\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 17.7416 - mse: 17.7416 - val_loss: 11.3584 - val_mse: 11.3584\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 16.4768 - mse: 16.4768 - val_loss: 10.0233 - val_mse: 10.0233\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 14.5683 - mse: 14.5683 - val_loss: 8.5872 - val_mse: 8.5872\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 12.6709 - mse: 12.6709 - val_loss: 7.3218 - val_mse: 7.3218\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 10.8182 - mse: 10.8182 - val_loss: 6.3850 - val_mse: 6.3850\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 9.4880 - mse: 9.4880 - val_loss: 5.7347 - val_mse: 5.7347\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 8.3420 - mse: 8.3420 - val_loss: 5.2907 - val_mse: 5.2907\n",
      "wandb: Agent Finished Run: fsaqmec7 \n",
      "\n",
      "wandb: Agent Starting Run: onxdiqv6 with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 32\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: onxdiqv6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/onxdiqv6\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/onxdiqv6</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 72ms/step - loss: 19.0148 - mse: 19.0148 - val_loss: 12.5563 - val_mse: 12.5563\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 18.0795 - mse: 18.0795 - val_loss: 11.0021 - val_mse: 11.0021\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 15.5607 - mse: 15.5607 - val_loss: 7.5386 - val_mse: 7.5386\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 10.3985 - mse: 10.3985 - val_loss: 5.0993 - val_mse: 5.0993\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 6.7067 - mse: 6.7067 - val_loss: 4.7860 - val_mse: 4.7860\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 5.4859 - mse: 5.4859 - val_loss: 5.2645 - val_mse: 5.2645\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 5.3510 - mse: 5.3510 - val_loss: 5.8050 - val_mse: 5.8050\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 5.2794 - mse: 5.2794 - val_loss: 5.7737 - val_mse: 5.7737\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 5.2665 - mse: 5.2665 - val_loss: 5.6272 - val_mse: 5.6272\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2565 - mse: 5.2565 - val_loss: 5.5269 - val_mse: 5.5269\n",
      "wandb: Agent Finished Run: onxdiqv6 \n",
      "\n",
      "wandb: Agent Starting Run: 1wbb3jqd with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 64\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: 1wbb3jqd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/1wbb3jqd\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/1wbb3jqd</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 78ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 19.1149 - mse: 19.1149 - val_loss: 12.7998 - val_mse: 12.7998\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 18.2884 - mse: 18.2884 - val_loss: 10.3125 - val_mse: 10.3125\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 13.3587 - mse: 13.3587 - val_loss: 4.9013 - val_mse: 4.9013\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 6.1343 - mse: 6.1343 - val_loss: 6.1451 - val_mse: 6.1451\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.7492 - mse: 5.7492 - val_loss: 7.4622 - val_mse: 7.4622\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.8306 - mse: 5.8306 - val_loss: 6.3979 - val_mse: 6.3979\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.4031 - mse: 5.4031 - val_loss: 5.8051 - val_mse: 5.8051\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2601 - mse: 5.2601 - val_loss: 5.4502 - val_mse: 5.4502\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2347 - mse: 5.2347 - val_loss: 5.2195 - val_mse: 5.2195\n",
      "wandb: Agent Finished Run: 1wbb3jqd \n",
      "\n",
      "wandb: Agent Starting Run: cr66jtvw with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 16\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: cr66jtvw\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/cr66jtvw\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/cr66jtvw</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 72ms/step - loss: 19.1218 - mse: 19.1218 - val_loss: 13.0472 - val_mse: 13.0472\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 18.9453 - mse: 18.9453 - val_loss: 12.7912 - val_mse: 12.7912\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 18.5828 - mse: 18.5828 - val_loss: 12.3221 - val_mse: 12.3221\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 17.8507 - mse: 17.8507 - val_loss: 11.4838 - val_mse: 11.4838\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 16.6532 - mse: 16.6532 - val_loss: 10.1757 - val_mse: 10.1757\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 14.7735 - mse: 14.7735 - val_loss: 8.7279 - val_mse: 8.7279\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 12.8658 - mse: 12.8658 - val_loss: 7.4321 - val_mse: 7.4321\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 10.9718 - mse: 10.9718 - val_loss: 6.4488 - val_mse: 6.4488\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 9.5836 - mse: 9.5836 - val_loss: 5.7656 - val_mse: 5.7656\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 8.3926 - mse: 8.3926 - val_loss: 5.3064 - val_mse: 5.3064\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 7.5699 - mse: 7.5699 - val_loss: 5.0249 - val_mse: 5.0249\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 6.9814 - mse: 6.9814 - val_loss: 4.8664 - val_mse: 4.8664\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 6.4849 - mse: 6.4849 - val_loss: 4.7849 - val_mse: 4.7849\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 6.2027 - mse: 6.2027 - val_loss: 4.7575 - val_mse: 4.7575\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.9370 - mse: 5.9370 - val_loss: 4.7620 - val_mse: 4.7620\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 5.7803 - mse: 5.7803 - val_loss: 4.7887 - val_mse: 4.7887\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6497 - mse: 5.6497 - val_loss: 4.8327 - val_mse: 4.8327\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.5110 - mse: 5.5110 - val_loss: 4.8925 - val_mse: 4.8925\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4440 - mse: 5.4440 - val_loss: 4.9720 - val_mse: 4.9720\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3606 - mse: 5.3606 - val_loss: 5.0555 - val_mse: 5.0555\n",
      "wandb: Agent Finished Run: cr66jtvw \n",
      "\n",
      "wandb: Agent Starting Run: 5dvwkygz with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 32\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: 5dvwkygz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/5dvwkygz\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/5dvwkygz</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 73ms/step - loss: 18.9784 - mse: 18.9784 - val_loss: 12.3931 - val_mse: 12.3931\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 17.7927 - mse: 17.7927 - val_loss: 10.4567 - val_mse: 10.4567\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 14.6991 - mse: 14.6991 - val_loss: 6.8239 - val_mse: 6.8239\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 9.4366 - mse: 9.4366 - val_loss: 4.9241 - val_mse: 4.9241\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 6.3320 - mse: 6.3320 - val_loss: 4.8407 - val_mse: 4.8407\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.3777 - mse: 5.3777 - val_loss: 5.3892 - val_mse: 5.3892\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.3448 - mse: 5.3448 - val_loss: 5.9245 - val_mse: 5.9245\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.3015 - mse: 5.3015 - val_loss: 5.8585 - val_mse: 5.8585\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2811 - mse: 5.2811 - val_loss: 5.6807 - val_mse: 5.6807\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 5.2637 - mse: 5.2637 - val_loss: 5.5547 - val_mse: 5.5547\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2386 - mse: 5.2386 - val_loss: 5.4565 - val_mse: 5.4565\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2465 - mse: 5.2465 - val_loss: 5.3957 - val_mse: 5.3957\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2413 - mse: 5.2413 - val_loss: 5.4292 - val_mse: 5.4292\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2420 - mse: 5.2420 - val_loss: 5.4391 - val_mse: 5.4391\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2390 - mse: 5.2390 - val_loss: 5.3970 - val_mse: 5.3970\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2376 - mse: 5.2376 - val_loss: 5.3195 - val_mse: 5.3195\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2534 - mse: 5.2534 - val_loss: 5.2900 - val_mse: 5.2900\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2439 - mse: 5.2439 - val_loss: 5.3567 - val_mse: 5.3567\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2525 - mse: 5.2525 - val_loss: 5.4502 - val_mse: 5.4502\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2383 - mse: 5.2383 - val_loss: 5.4580 - val_mse: 5.4580\n",
      "wandb: Agent Finished Run: 5dvwkygz \n",
      "\n",
      "wandb: Agent Starting Run: 7cazy6k4 with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 64\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: 7cazy6k4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/7cazy6k4\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/7cazy6k4</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 76ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 19.1149 - mse: 19.1149 - val_loss: 12.7926 - val_mse: 12.7926\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 18.2640 - mse: 18.2640 - val_loss: 10.2120 - val_mse: 10.2120\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 13.1809 - mse: 13.1809 - val_loss: 4.8807 - val_mse: 4.8807\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 6.0954 - mse: 6.0954 - val_loss: 6.1277 - val_mse: 6.1277\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.7354 - mse: 5.7354 - val_loss: 7.4424 - val_mse: 7.4424\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.8234 - mse: 5.8234 - val_loss: 6.4103 - val_mse: 6.4103\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.4074 - mse: 5.4074 - val_loss: 5.8194 - val_mse: 5.8194\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2622 - mse: 5.2622 - val_loss: 5.4550 - val_mse: 5.4550\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2345 - mse: 5.2345 - val_loss: 5.2175 - val_mse: 5.2175\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2580 - mse: 5.2580 - val_loss: 5.1534 - val_mse: 5.1534\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2809 - mse: 5.2809 - val_loss: 5.2634 - val_mse: 5.2634\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2559 - mse: 5.2559 - val_loss: 5.3270 - val_mse: 5.3270\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2535 - mse: 5.2535 - val_loss: 5.2947 - val_mse: 5.2947\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2503 - mse: 5.2503 - val_loss: 5.1210 - val_mse: 5.1210\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.3148 - mse: 5.3148 - val_loss: 5.2668 - val_mse: 5.2668\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2250 - mse: 5.2250 - val_loss: 5.6036 - val_mse: 5.6036\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2550 - mse: 5.2550 - val_loss: 6.1372 - val_mse: 6.1372\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.3708 - mse: 5.3708 - val_loss: 6.2983 - val_mse: 6.2983\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.4059 - mse: 5.4059 - val_loss: 6.2336 - val_mse: 6.2336\n",
      "wandb: Agent Finished Run: 7cazy6k4 \n",
      "\n",
      "wandb: Agent Starting Run: slio42ay with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 16\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: slio42ay\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/slio42ay\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/slio42ay</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 72ms/step - loss: 19.1214 - mse: 19.1214 - val_loss: 13.0323 - val_mse: 13.0323\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 18.9206 - mse: 18.9206 - val_loss: 12.7544 - val_mse: 12.7544\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 18.5298 - mse: 18.5298 - val_loss: 12.2506 - val_mse: 12.2506\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 17.7416 - mse: 17.7416 - val_loss: 11.3584 - val_mse: 11.3584\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 16.4768 - mse: 16.4768 - val_loss: 10.0233 - val_mse: 10.0233\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 14.5683 - mse: 14.5683 - val_loss: 8.5872 - val_mse: 8.5872\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 12.6709 - mse: 12.6709 - val_loss: 7.3218 - val_mse: 7.3218\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 10.8182 - mse: 10.8182 - val_loss: 6.3850 - val_mse: 6.3850\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 9.4880 - mse: 9.4880 - val_loss: 5.7347 - val_mse: 5.7347\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 8.3420 - mse: 8.3420 - val_loss: 5.2907 - val_mse: 5.2907\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 7.5372 - mse: 7.5372 - val_loss: 5.0157 - val_mse: 5.0157\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 6.9588 - mse: 6.9588 - val_loss: 4.8622 - val_mse: 4.8622\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 6.4721 - mse: 6.4721 - val_loss: 4.7837 - val_mse: 4.7837\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 6.1960 - mse: 6.1960 - val_loss: 4.7574 - val_mse: 4.7574\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.9347 - mse: 5.9347 - val_loss: 4.7620 - val_mse: 4.7620\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.7804 - mse: 5.7804 - val_loss: 4.7882 - val_mse: 4.7882\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6513 - mse: 5.6513 - val_loss: 4.8314 - val_mse: 4.8314\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 5.5138 - mse: 5.5138 - val_loss: 4.8902 - val_mse: 4.8902\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4469 - mse: 5.4469 - val_loss: 4.9681 - val_mse: 4.9681\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3635 - mse: 5.3635 - val_loss: 5.0502 - val_mse: 5.0502\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3115 - mse: 5.3115 - val_loss: 5.1200 - val_mse: 5.1200\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2812 - mse: 5.2812 - val_loss: 5.1879 - val_mse: 5.1879\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2697 - mse: 5.2697 - val_loss: 5.2619 - val_mse: 5.2619\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2556 - mse: 5.2556 - val_loss: 5.3040 - val_mse: 5.3040\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2456 - mse: 5.2456 - val_loss: 5.3188 - val_mse: 5.3188\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2442 - mse: 5.2442 - val_loss: 5.3330 - val_mse: 5.3330\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2432 - mse: 5.2432 - val_loss: 5.3464 - val_mse: 5.3464\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2400 - mse: 5.2400 - val_loss: 5.3730 - val_mse: 5.3730\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2441 - mse: 5.2441 - val_loss: 5.3935 - val_mse: 5.3935\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2394 - mse: 5.2394 - val_loss: 5.3538 - val_mse: 5.3538\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2384 - mse: 5.2384 - val_loss: 5.3021 - val_mse: 5.3021\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2460 - mse: 5.2460 - val_loss: 5.2568 - val_mse: 5.2568\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2519 - mse: 5.2519 - val_loss: 5.2181 - val_mse: 5.2181\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2625 - mse: 5.2625 - val_loss: 5.1839 - val_mse: 5.1839\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2679 - mse: 5.2679 - val_loss: 5.1650 - val_mse: 5.1650\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2761 - mse: 5.2761 - val_loss: 5.1502 - val_mse: 5.1502\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 20ms/step - loss: 5.2792 - mse: 5.2792 - val_loss: 5.1474 - val_mse: 5.1474\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 20ms/step - loss: 5.2799 - mse: 5.2799 - val_loss: 5.1406 - val_mse: 5.1406\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2823 - mse: 5.2823 - val_loss: 5.1370 - val_mse: 5.1370\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 20ms/step - loss: 5.2836 - mse: 5.2836 - val_loss: 5.1334 - val_mse: 5.1334\n",
      "wandb: Agent Finished Run: slio42ay \n",
      "\n",
      "wandb: Agent Starting Run: k8hk8tud with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 32\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: k8hk8tud\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/k8hk8tud\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/k8hk8tud</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 76ms/step - loss: 18.9784 - mse: 18.9784 - val_loss: 12.3931 - val_mse: 12.3931\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 17.7927 - mse: 17.7927 - val_loss: 10.4567 - val_mse: 10.4567\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 14.6991 - mse: 14.6991 - val_loss: 6.8239 - val_mse: 6.8239\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 9.4366 - mse: 9.4366 - val_loss: 4.9241 - val_mse: 4.9241\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 6.3320 - mse: 6.3320 - val_loss: 4.8407 - val_mse: 4.8407\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.3777 - mse: 5.3777 - val_loss: 5.3892 - val_mse: 5.3892\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.3448 - mse: 5.3448 - val_loss: 5.9245 - val_mse: 5.9245\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 5.3015 - mse: 5.3015 - val_loss: 5.8585 - val_mse: 5.8585\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2811 - mse: 5.2811 - val_loss: 5.6807 - val_mse: 5.6807\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2637 - mse: 5.2637 - val_loss: 5.5547 - val_mse: 5.5547\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2386 - mse: 5.2386 - val_loss: 5.4565 - val_mse: 5.4565\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2465 - mse: 5.2465 - val_loss: 5.3957 - val_mse: 5.3957\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 5.2413 - mse: 5.2413 - val_loss: 5.4292 - val_mse: 5.4292\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2420 - mse: 5.2420 - val_loss: 5.4391 - val_mse: 5.4391\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2390 - mse: 5.2390 - val_loss: 5.3970 - val_mse: 5.3970\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2376 - mse: 5.2376 - val_loss: 5.3195 - val_mse: 5.3195\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2534 - mse: 5.2534 - val_loss: 5.2900 - val_mse: 5.2900\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2439 - mse: 5.2439 - val_loss: 5.3567 - val_mse: 5.3567\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2525 - mse: 5.2525 - val_loss: 5.4502 - val_mse: 5.4502\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 5.2383 - mse: 5.2383 - val_loss: 5.4580 - val_mse: 5.4580\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2367 - mse: 5.2367 - val_loss: 5.4958 - val_mse: 5.4958\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2372 - mse: 5.2372 - val_loss: 5.5568 - val_mse: 5.5568\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2461 - mse: 5.2461 - val_loss: 5.5728 - val_mse: 5.5728\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2448 - mse: 5.2448 - val_loss: 5.5049 - val_mse: 5.5049\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2398 - mse: 5.2398 - val_loss: 5.4710 - val_mse: 5.4710\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2379 - mse: 5.2379 - val_loss: 5.4980 - val_mse: 5.4980\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2384 - mse: 5.2384 - val_loss: 5.5077 - val_mse: 5.5077\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2403 - mse: 5.2403 - val_loss: 5.5557 - val_mse: 5.5557\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2397 - mse: 5.2397 - val_loss: 5.6683 - val_mse: 5.6683\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2535 - mse: 5.2535 - val_loss: 5.7134 - val_mse: 5.7134\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2706 - mse: 5.2706 - val_loss: 5.8242 - val_mse: 5.8242\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2870 - mse: 5.2870 - val_loss: 5.9323 - val_mse: 5.9323\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3016 - mse: 5.3016 - val_loss: 6.0137 - val_mse: 6.0137\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3232 - mse: 5.3232 - val_loss: 6.1053 - val_mse: 6.1053\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3551 - mse: 5.3551 - val_loss: 6.0910 - val_mse: 6.0910\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3398 - mse: 5.3398 - val_loss: 5.9241 - val_mse: 5.9241\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2963 - mse: 5.2963 - val_loss: 5.8180 - val_mse: 5.8180\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2771 - mse: 5.2771 - val_loss: 5.7567 - val_mse: 5.7567\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2684 - mse: 5.2684 - val_loss: 5.7236 - val_mse: 5.7236\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2613 - mse: 5.2613 - val_loss: 5.7233 - val_mse: 5.7233\n",
      "wandb: Agent Finished Run: k8hk8tud \n",
      "\n",
      "wandb: Agent Starting Run: 3ruowq6i with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 64\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: 3ruowq6i\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/3ruowq6i\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/3ruowq6i</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 76ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 19.1153 - mse: 19.1153 - val_loss: 12.8085 - val_mse: 12.8085\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 18.3076 - mse: 18.3076 - val_loss: 10.3696 - val_mse: 10.3696\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 13.4591 - mse: 13.4591 - val_loss: 4.9149 - val_mse: 4.9149\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 6.1595 - mse: 6.1595 - val_loss: 6.1553 - val_mse: 6.1553\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.7547 - mse: 5.7547 - val_loss: 7.4683 - val_mse: 7.4683\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.8318 - mse: 5.8318 - val_loss: 6.3921 - val_mse: 6.3921\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.4010 - mse: 5.4010 - val_loss: 5.8018 - val_mse: 5.8018\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2598 - mse: 5.2598 - val_loss: 5.4497 - val_mse: 5.4497\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2347 - mse: 5.2347 - val_loss: 5.2212 - val_mse: 5.2212\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2573 - mse: 5.2573 - val_loss: 5.1593 - val_mse: 5.1593\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2793 - mse: 5.2793 - val_loss: 5.2683 - val_mse: 5.2683\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2550 - mse: 5.2550 - val_loss: 5.3281 - val_mse: 5.3281\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2532 - mse: 5.2532 - val_loss: 5.2923 - val_mse: 5.2923\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2507 - mse: 5.2507 - val_loss: 5.1175 - val_mse: 5.1175\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.3161 - mse: 5.3161 - val_loss: 5.2640 - val_mse: 5.2640\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2251 - mse: 5.2251 - val_loss: 5.6030 - val_mse: 5.6030\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2550 - mse: 5.2550 - val_loss: 6.1395 - val_mse: 6.1395\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.3716 - mse: 5.3716 - val_loss: 6.3015 - val_mse: 6.3015\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.4069 - mse: 5.4069 - val_loss: 6.2360 - val_mse: 6.2360\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.3949 - mse: 5.3949 - val_loss: 6.1067 - val_mse: 6.1067\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.3527 - mse: 5.3527 - val_loss: 6.0696 - val_mse: 6.0696\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.3130 - mse: 5.3130 - val_loss: 5.7501 - val_mse: 5.7501\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2468 - mse: 5.2468 - val_loss: 5.4119 - val_mse: 5.4119\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2297 - mse: 5.2297 - val_loss: 5.2233 - val_mse: 5.2233\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2768 - mse: 5.2768 - val_loss: 5.2137 - val_mse: 5.2137\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2789 - mse: 5.2789 - val_loss: 5.4821 - val_mse: 5.4821\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2290 - mse: 5.2290 - val_loss: 5.6884 - val_mse: 5.6884\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2692 - mse: 5.2692 - val_loss: 5.9285 - val_mse: 5.9285\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.3024 - mse: 5.3024 - val_loss: 5.9252 - val_mse: 5.9252\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.3238 - mse: 5.3238 - val_loss: 5.9708 - val_mse: 5.9708\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.3065 - mse: 5.3065 - val_loss: 5.7769 - val_mse: 5.7769\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2372 - mse: 5.2372 - val_loss: 5.3975 - val_mse: 5.3975\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2594 - mse: 5.2594 - val_loss: 5.1811 - val_mse: 5.1811\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2710 - mse: 5.2710 - val_loss: 5.1854 - val_mse: 5.1854\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2684 - mse: 5.2684 - val_loss: 5.2177 - val_mse: 5.2177\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2611 - mse: 5.2611 - val_loss: 5.2336 - val_mse: 5.2336\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2601 - mse: 5.2601 - val_loss: 5.2495 - val_mse: 5.2495\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2545 - mse: 5.2545 - val_loss: 5.2484 - val_mse: 5.2484\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2597 - mse: 5.2597 - val_loss: 5.2997 - val_mse: 5.2997\n",
      "wandb: Agent Finished Run: 3ruowq6i \n",
      "\n",
      "wandb: Agent Starting Run: hbh8fsnd with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 16\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: hbh8fsnd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/hbh8fsnd\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/hbh8fsnd</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 72ms/step - loss: 19.1213 - mse: 19.1213 - val_loss: 13.0288 - val_mse: 13.0288\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 18.9125 - mse: 18.9125 - val_loss: 12.7387 - val_mse: 12.7387\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 18.5059 - mse: 18.5059 - val_loss: 12.1972 - val_mse: 12.1972\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 17.6588 - mse: 17.6588 - val_loss: 11.2485 - val_mse: 11.2485\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 16.3420 - mse: 16.3420 - val_loss: 9.9407 - val_mse: 9.9407\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 14.5050 - mse: 14.5050 - val_loss: 8.5957 - val_mse: 8.5957\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 12.7140 - mse: 12.7140 - val_loss: 7.3776 - val_mse: 7.3776\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 10.9022 - mse: 10.9022 - val_loss: 6.4151 - val_mse: 6.4151\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 9.5346 - mse: 9.5346 - val_loss: 5.7537 - val_mse: 5.7537\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 8.3881 - mse: 8.3881 - val_loss: 5.3252 - val_mse: 5.3252\n",
      "wandb: Agent Finished Run: hbh8fsnd \n",
      "\n",
      "wandb: Agent Starting Run: l5x5hwch with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 32\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: l5x5hwch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/l5x5hwch\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/l5x5hwch</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 72ms/step - loss: 18.9785 - mse: 18.9785 - val_loss: 12.4075 - val_mse: 12.4075\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 17.8082 - mse: 17.8082 - val_loss: 10.4782 - val_mse: 10.4782\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 14.7207 - mse: 14.7207 - val_loss: 6.8221 - val_mse: 6.8221\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 9.4478 - mse: 9.4478 - val_loss: 4.9295 - val_mse: 4.9295\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 6.3657 - mse: 6.3657 - val_loss: 4.8350 - val_mse: 4.8350\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.3879 - mse: 5.3879 - val_loss: 5.3797 - val_mse: 5.3797\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.3454 - mse: 5.3454 - val_loss: 5.9197 - val_mse: 5.9197\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.3001 - mse: 5.3001 - val_loss: 5.8583 - val_mse: 5.8583\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2808 - mse: 5.2808 - val_loss: 5.6816 - val_mse: 5.6816\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2638 - mse: 5.2638 - val_loss: 5.5555 - val_mse: 5.5555\n",
      "wandb: Agent Finished Run: l5x5hwch \n",
      "\n",
      "wandb: Agent Starting Run: yhmp3sar with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 64\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: yhmp3sar\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/yhmp3sar\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/yhmp3sar</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 78ms/step - loss: 18.9579 - mse: 18.9579 - val_loss: 12.0123 - val_mse: 12.0123\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 16.4760 - mse: 16.4760 - val_loss: 6.1101 - val_mse: 6.1101\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 7.2231 - mse: 7.2231 - val_loss: 5.7463 - val_mse: 5.7463\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.3817 - mse: 5.3817 - val_loss: 7.2072 - val_mse: 7.2072\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.6922 - mse: 5.6922 - val_loss: 6.1818 - val_mse: 6.1818\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.3017 - mse: 5.3017 - val_loss: 5.5034 - val_mse: 5.5034\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.2624 - mse: 5.2624 - val_loss: 5.2718 - val_mse: 5.2718\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.2489 - mse: 5.2489 - val_loss: 5.4622 - val_mse: 5.4622\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2435 - mse: 5.2435 - val_loss: 5.7035 - val_mse: 5.7035\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.2663 - mse: 5.2663 - val_loss: 5.7631 - val_mse: 5.7631\n",
      "wandb: Agent Finished Run: yhmp3sar \n",
      "\n",
      "wandb: Agent Starting Run: wfaklhfx with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 16\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: wfaklhfx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/wfaklhfx\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/wfaklhfx</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 73ms/step - loss: 19.1213 - mse: 19.1213 - val_loss: 13.0288 - val_mse: 13.0288\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 18.9125 - mse: 18.9125 - val_loss: 12.7387 - val_mse: 12.7387\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 18.5059 - mse: 18.5059 - val_loss: 12.1972 - val_mse: 12.1972\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 17.6588 - mse: 17.6588 - val_loss: 11.2485 - val_mse: 11.2485\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 16.3420 - mse: 16.3420 - val_loss: 9.9407 - val_mse: 9.9407\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 14.5050 - mse: 14.5050 - val_loss: 8.5957 - val_mse: 8.5957\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 12.7140 - mse: 12.7140 - val_loss: 7.3776 - val_mse: 7.3776\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 10.9022 - mse: 10.9022 - val_loss: 6.4151 - val_mse: 6.4151\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 9.5346 - mse: 9.5346 - val_loss: 5.7537 - val_mse: 5.7537\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 8.3881 - mse: 8.3881 - val_loss: 5.3252 - val_mse: 5.3252\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 7.6224 - mse: 7.6224 - val_loss: 5.0550 - val_mse: 5.0550\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 7.0655 - mse: 7.0655 - val_loss: 4.8932 - val_mse: 4.8932\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 6.5783 - mse: 6.5783 - val_loss: 4.8010 - val_mse: 4.8010\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 6.2881 - mse: 6.2881 - val_loss: 4.7621 - val_mse: 4.7621\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 6.0118 - mse: 6.0118 - val_loss: 4.7576 - val_mse: 4.7576\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.8440 - mse: 5.8440 - val_loss: 4.7757 - val_mse: 4.7757\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.7041 - mse: 5.7041 - val_loss: 4.8121 - val_mse: 4.8121\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.5568 - mse: 5.5568 - val_loss: 4.8650 - val_mse: 4.8650\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.4815 - mse: 5.4815 - val_loss: 4.9377 - val_mse: 4.9377\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3895 - mse: 5.3895 - val_loss: 5.0168 - val_mse: 5.0168\n",
      "wandb: Agent Finished Run: wfaklhfx \n",
      "\n",
      "wandb: Agent Starting Run: cequimfr with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 32\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: cequimfr\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/cequimfr\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/cequimfr</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 74ms/step - loss: 18.9785 - mse: 18.9785 - val_loss: 12.4075 - val_mse: 12.4075\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 17.8082 - mse: 17.8082 - val_loss: 10.4782 - val_mse: 10.4782\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 14.7207 - mse: 14.7207 - val_loss: 6.8221 - val_mse: 6.8221\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 9.4478 - mse: 9.4478 - val_loss: 4.9295 - val_mse: 4.9295\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 6.3657 - mse: 6.3657 - val_loss: 4.8350 - val_mse: 4.8350\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.3879 - mse: 5.3879 - val_loss: 5.3797 - val_mse: 5.3797\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.3454 - mse: 5.3454 - val_loss: 5.9197 - val_mse: 5.9197\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.3001 - mse: 5.3001 - val_loss: 5.8583 - val_mse: 5.8583\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2808 - mse: 5.2808 - val_loss: 5.6816 - val_mse: 5.6816\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2638 - mse: 5.2638 - val_loss: 5.5555 - val_mse: 5.5555\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2384 - mse: 5.2384 - val_loss: 5.4567 - val_mse: 5.4567\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2464 - mse: 5.2464 - val_loss: 5.3955 - val_mse: 5.3955\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2412 - mse: 5.2412 - val_loss: 5.4303 - val_mse: 5.4303\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2420 - mse: 5.2420 - val_loss: 5.4407 - val_mse: 5.4407\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2391 - mse: 5.2391 - val_loss: 5.3979 - val_mse: 5.3979\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2374 - mse: 5.2374 - val_loss: 5.3190 - val_mse: 5.3190\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2534 - mse: 5.2534 - val_loss: 5.2891 - val_mse: 5.2891\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2442 - mse: 5.2442 - val_loss: 5.3566 - val_mse: 5.3566\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2528 - mse: 5.2528 - val_loss: 5.4515 - val_mse: 5.4515\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2383 - mse: 5.2383 - val_loss: 5.4592 - val_mse: 5.4592\n",
      "wandb: Agent Finished Run: cequimfr \n",
      "\n",
      "wandb: Agent Starting Run: 5wkjvkum with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 64\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: 5wkjvkum\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/5wkjvkum\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/5wkjvkum</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 75ms/step - loss: 18.9579 - mse: 18.9579 - val_loss: 12.0123 - val_mse: 12.0123\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 16.4760 - mse: 16.4760 - val_loss: 6.1101 - val_mse: 6.1101\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 7.2477 - mse: 7.2477 - val_loss: 5.7262 - val_mse: 5.7262\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.3761 - mse: 5.3761 - val_loss: 7.1983 - val_mse: 7.1983\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.6893 - mse: 5.6893 - val_loss: 6.1827 - val_mse: 6.1827\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.3024 - mse: 5.3024 - val_loss: 5.5068 - val_mse: 5.5068\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2623 - mse: 5.2623 - val_loss: 5.2747 - val_mse: 5.2747\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2485 - mse: 5.2485 - val_loss: 5.4652 - val_mse: 5.4652\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2436 - mse: 5.2436 - val_loss: 5.7067 - val_mse: 5.7067\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2667 - mse: 5.2667 - val_loss: 5.7637 - val_mse: 5.7637\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2642 - mse: 5.2642 - val_loss: 5.6494 - val_mse: 5.6494\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2467 - mse: 5.2467 - val_loss: 5.5068 - val_mse: 5.5068\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2283 - mse: 5.2283 - val_loss: 5.3231 - val_mse: 5.3231\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2395 - mse: 5.2395 - val_loss: 5.1607 - val_mse: 5.1607\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2850 - mse: 5.2850 - val_loss: 5.0137 - val_mse: 5.0137\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.3549 - mse: 5.3549 - val_loss: 5.1034 - val_mse: 5.1034\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2645 - mse: 5.2645 - val_loss: 5.3621 - val_mse: 5.3621\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2331 - mse: 5.2331 - val_loss: 5.8313 - val_mse: 5.8313\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2948 - mse: 5.2948 - val_loss: 6.1323 - val_mse: 6.1323\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.3571 - mse: 5.3571 - val_loss: 6.2703 - val_mse: 6.2703\n",
      "wandb: Agent Finished Run: 5wkjvkum \n",
      "\n",
      "wandb: Agent Starting Run: nt8qni2q with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 16\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: nt8qni2q\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/nt8qni2q\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/nt8qni2q</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 71ms/step - loss: 19.1217 - mse: 19.1217 - val_loss: 13.0440 - val_mse: 13.0440\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 18.9380 - mse: 18.9380 - val_loss: 12.7778 - val_mse: 12.7778\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 18.5643 - mse: 18.5643 - val_loss: 12.2802 - val_mse: 12.2802\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 17.7848 - mse: 17.7848 - val_loss: 11.3881 - val_mse: 11.3881\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 16.5345 - mse: 16.5345 - val_loss: 10.0917 - val_mse: 10.0917\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 14.7057 - mse: 14.7057 - val_loss: 8.7211 - val_mse: 8.7211\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 12.8855 - mse: 12.8855 - val_loss: 7.4702 - val_mse: 7.4702\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 11.0323 - mse: 11.0323 - val_loss: 6.4712 - val_mse: 6.4712\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 9.6191 - mse: 9.6191 - val_loss: 5.7816 - val_mse: 5.7816\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 8.4327 - mse: 8.4327 - val_loss: 5.3376 - val_mse: 5.3376\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 7.6450 - mse: 7.6450 - val_loss: 5.0599 - val_mse: 5.0599\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 7.0764 - mse: 7.0764 - val_loss: 4.8944 - val_mse: 4.8944\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 6.5809 - mse: 6.5809 - val_loss: 4.8007 - val_mse: 4.8007\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 6.2863 - mse: 6.2863 - val_loss: 4.7617 - val_mse: 4.7617\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 6.0068 - mse: 6.0068 - val_loss: 4.7578 - val_mse: 4.7578\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.8377 - mse: 5.8377 - val_loss: 4.7771 - val_mse: 4.7771\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6973 - mse: 5.6973 - val_loss: 4.8149 - val_mse: 4.8149\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.5497 - mse: 5.5497 - val_loss: 4.8695 - val_mse: 4.8695\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4750 - mse: 5.4750 - val_loss: 4.9442 - val_mse: 4.9442\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3836 - mse: 5.3836 - val_loss: 5.0252 - val_mse: 5.0252\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3252 - mse: 5.3252 - val_loss: 5.0961 - val_mse: 5.0961\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2894 - mse: 5.2894 - val_loss: 5.1664 - val_mse: 5.1664\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2755 - mse: 5.2755 - val_loss: 5.2437 - val_mse: 5.2437\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2589 - mse: 5.2589 - val_loss: 5.2891 - val_mse: 5.2891\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2478 - mse: 5.2478 - val_loss: 5.3067 - val_mse: 5.3067\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2455 - mse: 5.2455 - val_loss: 5.3236 - val_mse: 5.3236\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2446 - mse: 5.2446 - val_loss: 5.3396 - val_mse: 5.3396\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2403 - mse: 5.2403 - val_loss: 5.3689 - val_mse: 5.3689\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2447 - mse: 5.2447 - val_loss: 5.3918 - val_mse: 5.3918\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2394 - mse: 5.2394 - val_loss: 5.3523 - val_mse: 5.3523\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2377 - mse: 5.2377 - val_loss: 5.3004 - val_mse: 5.3004\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2461 - mse: 5.2461 - val_loss: 5.2550 - val_mse: 5.2550\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2526 - mse: 5.2526 - val_loss: 5.2163 - val_mse: 5.2163\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2630 - mse: 5.2630 - val_loss: 5.1822 - val_mse: 5.1822\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2689 - mse: 5.2689 - val_loss: 5.1635 - val_mse: 5.1635\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2765 - mse: 5.2765 - val_loss: 5.1490 - val_mse: 5.1490\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2793 - mse: 5.2793 - val_loss: 5.1464 - val_mse: 5.1464\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2807 - mse: 5.2807 - val_loss: 5.1398 - val_mse: 5.1398\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2828 - mse: 5.2828 - val_loss: 5.1364 - val_mse: 5.1364\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2839 - mse: 5.2839 - val_loss: 5.1330 - val_mse: 5.1330\n",
      "wandb: Agent Finished Run: nt8qni2q \n",
      "\n",
      "wandb: Agent Starting Run: 6xyer5rc with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 32\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: 6xyer5rc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/6xyer5rc\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/6xyer5rc</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 73ms/step - loss: 18.9785 - mse: 18.9785 - val_loss: 12.4075 - val_mse: 12.4075\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 17.8082 - mse: 17.8082 - val_loss: 10.4782 - val_mse: 10.4782\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 14.7207 - mse: 14.7207 - val_loss: 6.8221 - val_mse: 6.8221\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 9.4478 - mse: 9.4478 - val_loss: 4.9295 - val_mse: 4.9295\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 6.3657 - mse: 6.3657 - val_loss: 4.8350 - val_mse: 4.8350\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 5.3879 - mse: 5.3879 - val_loss: 5.3797 - val_mse: 5.3797\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 5.3454 - mse: 5.3454 - val_loss: 5.9197 - val_mse: 5.9197\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 5.3001 - mse: 5.3001 - val_loss: 5.8583 - val_mse: 5.8583\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2808 - mse: 5.2808 - val_loss: 5.6816 - val_mse: 5.6816\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2638 - mse: 5.2638 - val_loss: 5.5555 - val_mse: 5.5555\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 5.2384 - mse: 5.2384 - val_loss: 5.4567 - val_mse: 5.4567\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 5.2464 - mse: 5.2464 - val_loss: 5.3955 - val_mse: 5.3955\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2412 - mse: 5.2412 - val_loss: 5.4303 - val_mse: 5.4303\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2420 - mse: 5.2420 - val_loss: 5.4406 - val_mse: 5.4406\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2391 - mse: 5.2391 - val_loss: 5.3978 - val_mse: 5.3978\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2374 - mse: 5.2374 - val_loss: 5.3190 - val_mse: 5.3190\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2534 - mse: 5.2534 - val_loss: 5.2891 - val_mse: 5.2891\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2442 - mse: 5.2442 - val_loss: 5.3566 - val_mse: 5.3566\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2528 - mse: 5.2528 - val_loss: 5.4515 - val_mse: 5.4515\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2383 - mse: 5.2383 - val_loss: 5.4592 - val_mse: 5.4592\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2368 - mse: 5.2368 - val_loss: 5.4974 - val_mse: 5.4974\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2371 - mse: 5.2371 - val_loss: 5.5591 - val_mse: 5.5591\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2462 - mse: 5.2462 - val_loss: 5.5749 - val_mse: 5.5749\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2448 - mse: 5.2448 - val_loss: 5.5054 - val_mse: 5.5054\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2400 - mse: 5.2400 - val_loss: 5.4706 - val_mse: 5.4706\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 5.2378 - mse: 5.2378 - val_loss: 5.4978 - val_mse: 5.4978\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2383 - mse: 5.2383 - val_loss: 5.5076 - val_mse: 5.5076\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 5.2408 - mse: 5.2408 - val_loss: 5.5563 - val_mse: 5.5563\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2401 - mse: 5.2401 - val_loss: 5.6706 - val_mse: 5.6706\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2537 - mse: 5.2537 - val_loss: 5.7162 - val_mse: 5.7162\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2713 - mse: 5.2713 - val_loss: 5.8286 - val_mse: 5.8286\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2882 - mse: 5.2882 - val_loss: 5.9381 - val_mse: 5.9381\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3028 - mse: 5.3028 - val_loss: 6.0200 - val_mse: 6.0200\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3246 - mse: 5.3246 - val_loss: 6.1121 - val_mse: 6.1121\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3567 - mse: 5.3567 - val_loss: 6.0956 - val_mse: 6.0956\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3409 - mse: 5.3409 - val_loss: 5.9235 - val_mse: 5.9235\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2961 - mse: 5.2961 - val_loss: 5.8140 - val_mse: 5.8140\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2763 - mse: 5.2763 - val_loss: 5.7512 - val_mse: 5.7512\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2671 - mse: 5.2671 - val_loss: 5.7179 - val_mse: 5.7179\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2603 - mse: 5.2603 - val_loss: 5.7185 - val_mse: 5.7185\n",
      "wandb: Agent Finished Run: 6xyer5rc \n",
      "\n",
      "wandb: Agent Starting Run: sro7kbxj with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 64\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: sro7kbxj\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/sro7kbxj\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/sro7kbxj</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 77ms/step - loss: 18.9539 - mse: 18.9539 - val_loss: 11.8986 - val_mse: 11.8986\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 2s 32ms/step - loss: 16.2351 - mse: 16.2351 - val_loss: 5.8691 - val_mse: 5.8691\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 6.9618 - mse: 6.9618 - val_loss: 5.8815 - val_mse: 5.8815\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 5.4229 - mse: 5.4229 - val_loss: 7.2839 - val_mse: 7.2839\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 5.7210 - mse: 5.7210 - val_loss: 6.2001 - val_mse: 6.2001\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.3034 - mse: 5.3034 - val_loss: 5.4953 - val_mse: 5.4953\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2636 - mse: 5.2636 - val_loss: 5.2547 - val_mse: 5.2547\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2512 - mse: 5.2512 - val_loss: 5.4383 - val_mse: 5.4383\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2425 - mse: 5.2425 - val_loss: 5.6810 - val_mse: 5.6810\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2632 - mse: 5.2632 - val_loss: 5.7517 - val_mse: 5.7517\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2637 - mse: 5.2637 - val_loss: 5.6546 - val_mse: 5.6546\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2481 - mse: 5.2481 - val_loss: 5.5226 - val_mse: 5.5226\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2291 - mse: 5.2291 - val_loss: 5.3411 - val_mse: 5.3411\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2372 - mse: 5.2372 - val_loss: 5.1738 - val_mse: 5.1738\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2805 - mse: 5.2805 - val_loss: 5.0209 - val_mse: 5.0209\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.3495 - mse: 5.3495 - val_loss: 5.1025 - val_mse: 5.1025\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2671 - mse: 5.2671 - val_loss: 5.3499 - val_mse: 5.3499\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2331 - mse: 5.2331 - val_loss: 5.8028 - val_mse: 5.8028\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2884 - mse: 5.2884 - val_loss: 6.1030 - val_mse: 6.1030\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.3485 - mse: 5.3485 - val_loss: 6.2523 - val_mse: 6.2523\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.3943 - mse: 5.3943 - val_loss: 6.2871 - val_mse: 6.2871\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.4140 - mse: 5.4140 - val_loss: 6.3043 - val_mse: 6.3043\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.3840 - mse: 5.3840 - val_loss: 5.9733 - val_mse: 5.9733\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2852 - mse: 5.2852 - val_loss: 5.5770 - val_mse: 5.5770\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2280 - mse: 5.2280 - val_loss: 5.3184 - val_mse: 5.3184\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2631 - mse: 5.2631 - val_loss: 5.2403 - val_mse: 5.2403\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2680 - mse: 5.2680 - val_loss: 5.4103 - val_mse: 5.4103\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2331 - mse: 5.2331 - val_loss: 5.5463 - val_mse: 5.5463\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2480 - mse: 5.2480 - val_loss: 5.7455 - val_mse: 5.7455\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2653 - mse: 5.2653 - val_loss: 5.7849 - val_mse: 5.7849\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2980 - mse: 5.2980 - val_loss: 5.8867 - val_mse: 5.8867\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2948 - mse: 5.2948 - val_loss: 5.7715 - val_mse: 5.7715\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2424 - mse: 5.2424 - val_loss: 5.4489 - val_mse: 5.4489\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2534 - mse: 5.2534 - val_loss: 5.2466 - val_mse: 5.2466\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2558 - mse: 5.2558 - val_loss: 5.2460 - val_mse: 5.2460\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2552 - mse: 5.2552 - val_loss: 5.2628 - val_mse: 5.2628\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2537 - mse: 5.2537 - val_loss: 5.2580 - val_mse: 5.2580\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2582 - mse: 5.2582 - val_loss: 5.2537 - val_mse: 5.2537\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2536 - mse: 5.2536 - val_loss: 5.2376 - val_mse: 5.2376\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2630 - mse: 5.2630 - val_loss: 5.2775 - val_mse: 5.2775\n",
      "wandb: Agent Finished Run: sro7kbxj \n",
      "\n",
      "wandb: Agent Starting Run: hibq8lzj with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 16\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: hibq8lzj\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/hibq8lzj\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/hibq8lzj</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 71ms/step - loss: 19.0677 - mse: 19.0677 - val_loss: 12.8937 - val_mse: 12.8937\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 18.7077 - mse: 18.7077 - val_loss: 12.4915 - val_mse: 12.4915\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 18.1399 - mse: 18.1399 - val_loss: 11.7375 - val_mse: 11.7375\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 16.9843 - mse: 16.9843 - val_loss: 10.4134 - val_mse: 10.4134\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 15.2201 - mse: 15.2201 - val_loss: 8.8077 - val_mse: 8.8077\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 13.0166 - mse: 13.0166 - val_loss: 7.5747 - val_mse: 7.5747\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 11.3463 - mse: 11.3463 - val_loss: 6.6836 - val_mse: 6.6836\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 9.9294 - mse: 9.9294 - val_loss: 6.0098 - val_mse: 6.0098\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 8.9070 - mse: 8.9070 - val_loss: 5.5051 - val_mse: 5.5051\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 7.9374 - mse: 7.9374 - val_loss: 5.1340 - val_mse: 5.1340\n",
      "wandb: Agent Finished Run: hibq8lzj \n",
      "\n",
      "wandb: Agent Starting Run: xvgtxezm with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 32\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: xvgtxezm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/xvgtxezm\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/xvgtxezm</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 73ms/step - loss: 18.9710 - mse: 18.9710 - val_loss: 12.3697 - val_mse: 12.3697\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 17.7635 - mse: 17.7635 - val_loss: 10.3491 - val_mse: 10.3491\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 14.5194 - mse: 14.5194 - val_loss: 6.6716 - val_mse: 6.6716\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 9.3114 - mse: 9.3114 - val_loss: 4.9049 - val_mse: 4.9049\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 6.3296 - mse: 6.3296 - val_loss: 4.8427 - val_mse: 4.8427\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.3793 - mse: 5.3793 - val_loss: 5.4213 - val_mse: 5.4213\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.3551 - mse: 5.3551 - val_loss: 5.9961 - val_mse: 5.9961\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.3145 - mse: 5.3145 - val_loss: 5.9024 - val_mse: 5.9024\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2884 - mse: 5.2884 - val_loss: 5.6897 - val_mse: 5.6897\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2663 - mse: 5.2663 - val_loss: 5.5416 - val_mse: 5.5416\n",
      "wandb: Agent Finished Run: xvgtxezm \n",
      "\n",
      "wandb: Agent Starting Run: x5reile1 with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 64\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: x5reile1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/x5reile1\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/x5reile1</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 78ms/step - loss: 18.8884 - mse: 18.8884 - val_loss: 11.5311 - val_mse: 11.5311\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 15.3882 - mse: 15.3882 - val_loss: 5.1990 - val_mse: 5.1990\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 6.1552 - mse: 6.1552 - val_loss: 6.4074 - val_mse: 6.4074\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.6041 - mse: 5.6041 - val_loss: 7.4618 - val_mse: 7.4618\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.7769 - mse: 5.7769 - val_loss: 6.0868 - val_mse: 6.0868\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 30ms/step - loss: 5.2672 - mse: 5.2672 - val_loss: 5.3692 - val_mse: 5.3692\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2699 - mse: 5.2699 - val_loss: 5.1909 - val_mse: 5.1909\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2615 - mse: 5.2615 - val_loss: 5.4210 - val_mse: 5.4210\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2437 - mse: 5.2437 - val_loss: 5.7073 - val_mse: 5.7073\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2688 - mse: 5.2688 - val_loss: 5.8046 - val_mse: 5.8046\n",
      "wandb: Agent Finished Run: x5reile1 \n",
      "\n",
      "wandb: Agent Starting Run: youpagu9 with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 16\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: youpagu9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/youpagu9\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/youpagu9</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 73ms/step - loss: 19.0844 - mse: 19.0844 - val_loss: 12.9433 - val_mse: 12.9433\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 18.7860 - mse: 18.7860 - val_loss: 12.5925 - val_mse: 12.5925\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 18.2907 - mse: 18.2907 - val_loss: 11.9474 - val_mse: 11.9474\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 17.3000 - mse: 17.3000 - val_loss: 10.7820 - val_mse: 10.7820\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 15.7149 - mse: 15.7149 - val_loss: 9.1675 - val_mse: 9.1675\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 13.4883 - mse: 13.4883 - val_loss: 7.8057 - val_mse: 7.8057\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 11.6841 - mse: 11.6841 - val_loss: 6.8421 - val_mse: 6.8421\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 10.1716 - mse: 10.1716 - val_loss: 6.1219 - val_mse: 6.1219\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 9.0916 - mse: 9.0916 - val_loss: 5.5780 - val_mse: 5.5780\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 8.0696 - mse: 8.0696 - val_loss: 5.1762 - val_mse: 5.1762\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 7.2866 - mse: 7.2866 - val_loss: 4.9181 - val_mse: 4.9181\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 6.6789 - mse: 6.6789 - val_loss: 4.7896 - val_mse: 4.7896\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 6.1498 - mse: 6.1498 - val_loss: 4.7568 - val_mse: 4.7568\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.8713 - mse: 5.8713 - val_loss: 4.7903 - val_mse: 4.7903\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 5.6136 - mse: 5.6136 - val_loss: 4.8505 - val_mse: 4.8505\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.4888 - mse: 5.4888 - val_loss: 4.9402 - val_mse: 4.9402\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.3965 - mse: 5.3965 - val_loss: 5.0429 - val_mse: 5.0429\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2997 - mse: 5.2997 - val_loss: 5.1570 - val_mse: 5.1570\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2807 - mse: 5.2807 - val_loss: 5.2925 - val_mse: 5.2925\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2451 - mse: 5.2451 - val_loss: 5.4107 - val_mse: 5.4107\n",
      "wandb: Agent Finished Run: youpagu9 \n",
      "\n",
      "wandb: Agent Starting Run: 79tq3eg2 with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 32\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: 79tq3eg2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/79tq3eg2\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/79tq3eg2</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 74ms/step - loss: 18.9710 - mse: 18.9710 - val_loss: 12.3697 - val_mse: 12.3697\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 17.7635 - mse: 17.7635 - val_loss: 10.3491 - val_mse: 10.3491\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 14.5194 - mse: 14.5194 - val_loss: 6.6716 - val_mse: 6.6716\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 9.3114 - mse: 9.3114 - val_loss: 4.9049 - val_mse: 4.9049\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 6.3296 - mse: 6.3296 - val_loss: 4.8427 - val_mse: 4.8427\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.3793 - mse: 5.3793 - val_loss: 5.4213 - val_mse: 5.4213\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.3551 - mse: 5.3551 - val_loss: 5.9961 - val_mse: 5.9961\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.3145 - mse: 5.3145 - val_loss: 5.9024 - val_mse: 5.9024\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2884 - mse: 5.2884 - val_loss: 5.6897 - val_mse: 5.6897\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2663 - mse: 5.2663 - val_loss: 5.5416 - val_mse: 5.5416\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2375 - mse: 5.2375 - val_loss: 5.4312 - val_mse: 5.4312\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2468 - mse: 5.2468 - val_loss: 5.3665 - val_mse: 5.3665\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2431 - mse: 5.2431 - val_loss: 5.4060 - val_mse: 5.4060\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2432 - mse: 5.2432 - val_loss: 5.4218 - val_mse: 5.4218\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2395 - mse: 5.2395 - val_loss: 5.3834 - val_mse: 5.3834\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2389 - mse: 5.2389 - val_loss: 5.3076 - val_mse: 5.3076\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2540 - mse: 5.2540 - val_loss: 5.2818 - val_mse: 5.2818\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2447 - mse: 5.2447 - val_loss: 5.3560 - val_mse: 5.3560\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 5.2537 - mse: 5.2537 - val_loss: 5.4576 - val_mse: 5.4576\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2388 - mse: 5.2388 - val_loss: 5.4675 - val_mse: 5.4675\n",
      "wandb: Agent Finished Run: 79tq3eg2 \n",
      "\n",
      "wandb: Agent Starting Run: x4voq8bo with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 64\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: x4voq8bo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/x4voq8bo\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/x4voq8bo</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 80ms/step - loss: 18.8884 - mse: 18.8884 - val_loss: 11.6378 - val_mse: 11.6378\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 15.7074 - mse: 15.7074 - val_loss: 5.4465 - val_mse: 5.4465\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 6.5013 - mse: 6.5013 - val_loss: 6.1343 - val_mse: 6.1343\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.5026 - mse: 5.5026 - val_loss: 7.3585 - val_mse: 7.3585\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.7415 - mse: 5.7415 - val_loss: 6.0904 - val_mse: 6.0904\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2727 - mse: 5.2727 - val_loss: 5.3974 - val_mse: 5.3974\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2666 - mse: 5.2666 - val_loss: 5.2187 - val_mse: 5.2187\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2560 - mse: 5.2560 - val_loss: 5.4565 - val_mse: 5.4565\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2450 - mse: 5.2450 - val_loss: 5.7418 - val_mse: 5.7418\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2739 - mse: 5.2739 - val_loss: 5.8180 - val_mse: 5.8180\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2737 - mse: 5.2737 - val_loss: 5.6962 - val_mse: 5.6962\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2513 - mse: 5.2513 - val_loss: 5.5342 - val_mse: 5.5342\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2279 - mse: 5.2279 - val_loss: 5.3320 - val_mse: 5.3320\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2376 - mse: 5.2376 - val_loss: 5.1587 - val_mse: 5.1587\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2860 - mse: 5.2860 - val_loss: 5.0086 - val_mse: 5.0086\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.3569 - mse: 5.3569 - val_loss: 5.0882 - val_mse: 5.0882\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2724 - mse: 5.2724 - val_loss: 5.3329 - val_mse: 5.3329\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2333 - mse: 5.2333 - val_loss: 5.7844 - val_mse: 5.7844\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2848 - mse: 5.2848 - val_loss: 6.0923 - val_mse: 6.0923\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.3453 - mse: 5.3453 - val_loss: 6.2531 - val_mse: 6.2531\n",
      "wandb: Agent Finished Run: x4voq8bo \n",
      "\n",
      "wandb: Agent Starting Run: 4ae70h3z with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 16\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: 4ae70h3z\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/4ae70h3z\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/4ae70h3z</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 71ms/step - loss: 19.0844 - mse: 19.0844 - val_loss: 12.9433 - val_mse: 12.9433\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 18.7860 - mse: 18.7860 - val_loss: 12.5925 - val_mse: 12.5925\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 18.2907 - mse: 18.2907 - val_loss: 11.9474 - val_mse: 11.9474\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 17.3000 - mse: 17.3000 - val_loss: 10.7825 - val_mse: 10.7825\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 15.7156 - mse: 15.7156 - val_loss: 9.1680 - val_mse: 9.1680\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 13.4890 - mse: 13.4890 - val_loss: 7.8060 - val_mse: 7.8060\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 11.6846 - mse: 11.6846 - val_loss: 6.8423 - val_mse: 6.8423\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 10.1719 - mse: 10.1719 - val_loss: 6.1221 - val_mse: 6.1221\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 9.0919 - mse: 9.0919 - val_loss: 5.5781 - val_mse: 5.5781\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 8.0698 - mse: 8.0698 - val_loss: 5.1763 - val_mse: 5.1763\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 7.2868 - mse: 7.2868 - val_loss: 4.9181 - val_mse: 4.9181\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 6.6790 - mse: 6.6790 - val_loss: 4.7896 - val_mse: 4.7896\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 6.1499 - mse: 6.1499 - val_loss: 4.7568 - val_mse: 4.7568\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.8713 - mse: 5.8713 - val_loss: 4.7902 - val_mse: 4.7902\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6136 - mse: 5.6136 - val_loss: 4.8504 - val_mse: 4.8504\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.4888 - mse: 5.4888 - val_loss: 4.9402 - val_mse: 4.9402\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3966 - mse: 5.3966 - val_loss: 5.0429 - val_mse: 5.0429\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2995 - mse: 5.2995 - val_loss: 5.1573 - val_mse: 5.1573\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2806 - mse: 5.2806 - val_loss: 5.2929 - val_mse: 5.2929\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2451 - mse: 5.2451 - val_loss: 5.4114 - val_mse: 5.4114\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2363 - mse: 5.2363 - val_loss: 5.4822 - val_mse: 5.4822\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2355 - mse: 5.2355 - val_loss: 5.5356 - val_mse: 5.5356\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2441 - mse: 5.2441 - val_loss: 5.5928 - val_mse: 5.5928\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2473 - mse: 5.2473 - val_loss: 5.5899 - val_mse: 5.5899\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2434 - mse: 5.2434 - val_loss: 5.5461 - val_mse: 5.5461\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2402 - mse: 5.2402 - val_loss: 5.5071 - val_mse: 5.5071\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2381 - mse: 5.2381 - val_loss: 5.4739 - val_mse: 5.4739\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2407 - mse: 5.2407 - val_loss: 5.4663 - val_mse: 5.4663\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2403 - mse: 5.2403 - val_loss: 5.4583 - val_mse: 5.4583\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2375 - mse: 5.2375 - val_loss: 5.3801 - val_mse: 5.3801\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2340 - mse: 5.2340 - val_loss: 5.2949 - val_mse: 5.2949\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2470 - mse: 5.2470 - val_loss: 5.2254 - val_mse: 5.2254\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2584 - mse: 5.2584 - val_loss: 5.1703 - val_mse: 5.1703\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2760 - mse: 5.2760 - val_loss: 5.1258 - val_mse: 5.1258\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2868 - mse: 5.2868 - val_loss: 5.1035 - val_mse: 5.1035\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2984 - mse: 5.2984 - val_loss: 5.0887 - val_mse: 5.0887\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.3024 - mse: 5.3024 - val_loss: 5.0900 - val_mse: 5.0900\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.3026 - mse: 5.3026 - val_loss: 5.0875 - val_mse: 5.0875\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.3034 - mse: 5.3034 - val_loss: 5.0895 - val_mse: 5.0895\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.3020 - mse: 5.3020 - val_loss: 5.0919 - val_mse: 5.0919\n",
      "wandb: Agent Finished Run: 4ae70h3z \n",
      "\n",
      "wandb: Agent Starting Run: 7o052134 with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 32\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: 7o052134\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/7o052134\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/7o052134</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 73ms/step - loss: 19.0093 - mse: 19.0093 - val_loss: 12.5117 - val_mse: 12.5117\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 18.0166 - mse: 18.0166 - val_loss: 10.8654 - val_mse: 10.8654\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 2s 25ms/step - loss: 15.3265 - mse: 15.3265 - val_loss: 7.3114 - val_mse: 7.3114\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 10.1751 - mse: 10.1751 - val_loss: 5.0493 - val_mse: 5.0493\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 6.6667 - mse: 6.6667 - val_loss: 4.7906 - val_mse: 4.7906\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.4788 - mse: 5.4788 - val_loss: 5.3020 - val_mse: 5.3020\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.3569 - mse: 5.3569 - val_loss: 5.8807 - val_mse: 5.8807\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2910 - mse: 5.2910 - val_loss: 5.8234 - val_mse: 5.8234\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2740 - mse: 5.2740 - val_loss: 5.6420 - val_mse: 5.6420\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2598 - mse: 5.2598 - val_loss: 5.5179 - val_mse: 5.5179\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2371 - mse: 5.2371 - val_loss: 5.4255 - val_mse: 5.4255\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2460 - mse: 5.2460 - val_loss: 5.3728 - val_mse: 5.3728\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2435 - mse: 5.2435 - val_loss: 5.4237 - val_mse: 5.4237\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2437 - mse: 5.2437 - val_loss: 5.4455 - val_mse: 5.4455\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2392 - mse: 5.2392 - val_loss: 5.4060 - val_mse: 5.4060\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2378 - mse: 5.2378 - val_loss: 5.3245 - val_mse: 5.3245\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2526 - mse: 5.2526 - val_loss: 5.2942 - val_mse: 5.2942\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2434 - mse: 5.2434 - val_loss: 5.3682 - val_mse: 5.3682\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2534 - mse: 5.2534 - val_loss: 5.4700 - val_mse: 5.4700\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2388 - mse: 5.2388 - val_loss: 5.4760 - val_mse: 5.4760\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2369 - mse: 5.2369 - val_loss: 5.5130 - val_mse: 5.5130\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2384 - mse: 5.2384 - val_loss: 5.5738 - val_mse: 5.5738\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2474 - mse: 5.2474 - val_loss: 5.5855 - val_mse: 5.5855\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2463 - mse: 5.2463 - val_loss: 5.5082 - val_mse: 5.5082\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.2401 - mse: 5.2401 - val_loss: 5.4683 - val_mse: 5.4683\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2378 - mse: 5.2378 - val_loss: 5.4942 - val_mse: 5.4942\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 5.2378 - mse: 5.2378 - val_loss: 5.5029 - val_mse: 5.5029\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2407 - mse: 5.2407 - val_loss: 5.5528 - val_mse: 5.5528\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2392 - mse: 5.2392 - val_loss: 5.6710 - val_mse: 5.6710\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2530 - mse: 5.2530 - val_loss: 5.7181 - val_mse: 5.7181\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2717 - mse: 5.2717 - val_loss: 5.8340 - val_mse: 5.8340\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2897 - mse: 5.2897 - val_loss: 5.9463 - val_mse: 5.9463\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3046 - mse: 5.3046 - val_loss: 6.0295 - val_mse: 6.0295\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3262 - mse: 5.3262 - val_loss: 6.1225 - val_mse: 6.1225\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3592 - mse: 5.3592 - val_loss: 6.1029 - val_mse: 6.1029\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3430 - mse: 5.3430 - val_loss: 5.9238 - val_mse: 5.9238\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2959 - mse: 5.2959 - val_loss: 5.8107 - val_mse: 5.8107\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2754 - mse: 5.2754 - val_loss: 5.7461 - val_mse: 5.7461\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2664 - mse: 5.2664 - val_loss: 5.7122 - val_mse: 5.7122\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2587 - mse: 5.2587 - val_loss: 5.7133 - val_mse: 5.7133\n",
      "wandb: Agent Finished Run: 7o052134 \n",
      "\n",
      "wandb: Agent Starting Run: 4rl2b5s4 with config:\n",
      "\tbatch_size: 32\n",
      "\tnn_units: 64\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: 4rl2b5s4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/4rl2b5s4\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/4rl2b5s4</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 77ms/step - loss: 18.8884 - mse: 18.8884 - val_loss: 11.5311 - val_mse: 11.5311\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 15.3882 - mse: 15.3882 - val_loss: 5.1990 - val_mse: 5.1990\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 6.1552 - mse: 6.1552 - val_loss: 6.4074 - val_mse: 6.4074\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.6041 - mse: 5.6041 - val_loss: 7.4618 - val_mse: 7.4618\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.7769 - mse: 5.7769 - val_loss: 6.0868 - val_mse: 6.0868\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2672 - mse: 5.2672 - val_loss: 5.3692 - val_mse: 5.3692\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2699 - mse: 5.2699 - val_loss: 5.1909 - val_mse: 5.1909\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2615 - mse: 5.2615 - val_loss: 5.4210 - val_mse: 5.4210\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2437 - mse: 5.2437 - val_loss: 5.7073 - val_mse: 5.7073\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2688 - mse: 5.2688 - val_loss: 5.8046 - val_mse: 5.8046\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2736 - mse: 5.2736 - val_loss: 5.7099 - val_mse: 5.7099\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2545 - mse: 5.2545 - val_loss: 5.5633 - val_mse: 5.5633\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2298 - mse: 5.2298 - val_loss: 5.3632 - val_mse: 5.3632\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2342 - mse: 5.2342 - val_loss: 5.1808 - val_mse: 5.1808\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2785 - mse: 5.2785 - val_loss: 5.0205 - val_mse: 5.0205\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.3481 - mse: 5.3481 - val_loss: 5.0887 - val_mse: 5.0887\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2754 - mse: 5.2754 - val_loss: 5.3169 - val_mse: 5.3169\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2339 - mse: 5.2339 - val_loss: 5.7475 - val_mse: 5.7475\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2770 - mse: 5.2770 - val_loss: 6.0496 - val_mse: 6.0496\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.3331 - mse: 5.3331 - val_loss: 6.2222 - val_mse: 6.2222\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.3836 - mse: 5.3836 - val_loss: 6.2880 - val_mse: 6.2880\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.4156 - mse: 5.4156 - val_loss: 6.3285 - val_mse: 6.3285\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.3949 - mse: 5.3949 - val_loss: 6.0187 - val_mse: 6.0187\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2963 - mse: 5.2963 - val_loss: 5.6267 - val_mse: 5.6267\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2313 - mse: 5.2313 - val_loss: 5.3576 - val_mse: 5.3576\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2591 - mse: 5.2591 - val_loss: 5.2652 - val_mse: 5.2652\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2632 - mse: 5.2632 - val_loss: 5.4183 - val_mse: 5.4183\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2344 - mse: 5.2344 - val_loss: 5.5364 - val_mse: 5.5364\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2464 - mse: 5.2464 - val_loss: 5.7186 - val_mse: 5.7186\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2611 - mse: 5.2611 - val_loss: 5.7559 - val_mse: 5.7559\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2922 - mse: 5.2922 - val_loss: 5.8583 - val_mse: 5.8583\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2897 - mse: 5.2897 - val_loss: 5.7602 - val_mse: 5.7602\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2426 - mse: 5.2426 - val_loss: 5.4587 - val_mse: 5.4587\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2524 - mse: 5.2524 - val_loss: 5.2650 - val_mse: 5.2650\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2523 - mse: 5.2523 - val_loss: 5.2649 - val_mse: 5.2649\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 2s 29ms/step - loss: 5.2521 - mse: 5.2521 - val_loss: 5.2784 - val_mse: 5.2784\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 2s 28ms/step - loss: 5.2516 - mse: 5.2516 - val_loss: 5.2684 - val_mse: 5.2684\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2572 - mse: 5.2572 - val_loss: 5.2584 - val_mse: 5.2584\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 2s 27ms/step - loss: 5.2524 - mse: 5.2524 - val_loss: 5.2375 - val_mse: 5.2375\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 2s 26ms/step - loss: 5.2636 - mse: 5.2636 - val_loss: 5.2730 - val_mse: 5.2730\n",
      "wandb: Agent Finished Run: 4rl2b5s4 \n",
      "\n",
      "wandb: Agent Starting Run: qqf67au9 with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 16\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: qqf67au9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/qqf67au9\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/qqf67au9</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 4s 64ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: qqf67au9 \n",
      "\n",
      "wandb: Agent Starting Run: ivlcr9x3 with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 32\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: ivlcr9x3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/ivlcr9x3\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/ivlcr9x3</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 4s 63ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: ivlcr9x3 \n",
      "\n",
      "wandb: Agent Starting Run: 6x3k0uxp with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 64\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: 6x3k0uxp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/6x3k0uxp\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/6x3k0uxp</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 70ms/step - loss: 19.0882 - mse: 19.0882 - val_loss: 12.1742 - val_mse: 12.1742\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 17.7321 - mse: 17.7321 - val_loss: 8.7105 - val_mse: 8.7105\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 12.8948 - mse: 12.8948 - val_loss: 5.1147 - val_mse: 5.1147\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 7.1215 - mse: 7.1215 - val_loss: 4.9816 - val_mse: 4.9816\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.3909 - mse: 5.3909 - val_loss: 5.9062 - val_mse: 5.9062\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 5.2967 - mse: 5.2967 - val_loss: 6.3854 - val_mse: 6.3854\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.4402 - mse: 5.4402 - val_loss: 6.2280 - val_mse: 6.2280\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.3937 - mse: 5.3937 - val_loss: 5.9369 - val_mse: 5.9369\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 5.3033 - mse: 5.3033 - val_loss: 5.8149 - val_mse: 5.8149\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 5.2792 - mse: 5.2792 - val_loss: 5.7581 - val_mse: 5.7581\n",
      "wandb: Agent Finished Run: 6x3k0uxp \n",
      "\n",
      "wandb: Agent Starting Run: 5a3mqdlv with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 16\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: 5a3mqdlv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/5a3mqdlv\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/5a3mqdlv</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 73ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: 5a3mqdlv \n",
      "\n",
      "wandb: Agent Starting Run: a5logbwn with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 32\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: a5logbwn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/a5logbwn\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/a5logbwn</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 4s 64ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: a5logbwn \n",
      "\n",
      "wandb: Agent Starting Run: lj9mxvp5 with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 64\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: lj9mxvp5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/lj9mxvp5\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/lj9mxvp5</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 70ms/step - loss: 19.0882 - mse: 19.0882 - val_loss: 12.1742 - val_mse: 12.1742\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 17.7321 - mse: 17.7321 - val_loss: 8.7105 - val_mse: 8.7105\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 12.8948 - mse: 12.8948 - val_loss: 5.1147 - val_mse: 5.1147\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 7.1215 - mse: 7.1215 - val_loss: 4.9816 - val_mse: 4.9816\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.3909 - mse: 5.3909 - val_loss: 5.9062 - val_mse: 5.9062\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2967 - mse: 5.2967 - val_loss: 6.3854 - val_mse: 6.3854\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.4402 - mse: 5.4402 - val_loss: 6.2280 - val_mse: 6.2280\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.3937 - mse: 5.3937 - val_loss: 5.9369 - val_mse: 5.9369\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.3033 - mse: 5.3033 - val_loss: 5.8149 - val_mse: 5.8149\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2792 - mse: 5.2792 - val_loss: 5.7581 - val_mse: 5.7581\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2667 - mse: 5.2667 - val_loss: 5.7658 - val_mse: 5.7658\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2688 - mse: 5.2688 - val_loss: 5.5816 - val_mse: 5.5816\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2331 - mse: 5.2331 - val_loss: 5.2403 - val_mse: 5.2403\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2588 - mse: 5.2588 - val_loss: 5.0175 - val_mse: 5.0175\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.3301 - mse: 5.3301 - val_loss: 4.8782 - val_mse: 4.8782\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4818 - mse: 5.4818 - val_loss: 4.8197 - val_mse: 4.8197\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.5747 - mse: 5.5747 - val_loss: 4.7928 - val_mse: 4.7928\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6493 - mse: 5.6493 - val_loss: 4.7708 - val_mse: 4.7708\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.7425 - mse: 5.7425 - val_loss: 4.7676 - val_mse: 4.7676\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.7644 - mse: 5.7644 - val_loss: 4.7657 - val_mse: 4.7657\n",
      "wandb: Agent Finished Run: lj9mxvp5 \n",
      "\n",
      "wandb: Agent Starting Run: 6nwpybl6 with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 16\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: 6nwpybl6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/6nwpybl6\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/6nwpybl6</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 4s 62ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: 6nwpybl6 \n",
      "\n",
      "wandb: Agent Starting Run: 2prnpno4 with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 32\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: 2prnpno4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/2prnpno4\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/2prnpno4</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 4s 63ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "wandb: Agent Finished Run: 2prnpno4 \n",
      "\n",
      "wandb: Agent Starting Run: htlfsc3p with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 64\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0\n",
      "wandb: Agent Started Run: htlfsc3p\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/htlfsc3p\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/htlfsc3p</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 68ms/step - loss: 19.0882 - mse: 19.0882 - val_loss: 12.1742 - val_mse: 12.1742\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 17.7321 - mse: 17.7321 - val_loss: 8.7105 - val_mse: 8.7105\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 12.8948 - mse: 12.8948 - val_loss: 5.1147 - val_mse: 5.1147\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 7.1215 - mse: 7.1215 - val_loss: 4.9816 - val_mse: 4.9816\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3909 - mse: 5.3909 - val_loss: 5.9062 - val_mse: 5.9062\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2967 - mse: 5.2967 - val_loss: 6.3854 - val_mse: 6.3854\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4402 - mse: 5.4402 - val_loss: 6.2280 - val_mse: 6.2280\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3937 - mse: 5.3937 - val_loss: 5.9369 - val_mse: 5.9369\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.3033 - mse: 5.3033 - val_loss: 5.8149 - val_mse: 5.8149\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2792 - mse: 5.2792 - val_loss: 5.7581 - val_mse: 5.7581\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2667 - mse: 5.2667 - val_loss: 5.7658 - val_mse: 5.7658\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2688 - mse: 5.2688 - val_loss: 5.5816 - val_mse: 5.5816\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2331 - mse: 5.2331 - val_loss: 5.2403 - val_mse: 5.2403\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2588 - mse: 5.2588 - val_loss: 5.0175 - val_mse: 5.0175\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3301 - mse: 5.3301 - val_loss: 4.8782 - val_mse: 4.8782\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4818 - mse: 5.4818 - val_loss: 4.8197 - val_mse: 4.8197\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.5747 - mse: 5.5747 - val_loss: 4.7928 - val_mse: 4.7928\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6493 - mse: 5.6493 - val_loss: 4.7708 - val_mse: 4.7708\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.7425 - mse: 5.7425 - val_loss: 4.7676 - val_mse: 4.7676\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.7644 - mse: 5.7644 - val_loss: 4.7657 - val_mse: 4.7657\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.7769 - mse: 5.7769 - val_loss: 4.7700 - val_mse: 4.7700\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.7576 - mse: 5.7576 - val_loss: 4.7729 - val_mse: 4.7729\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.7322 - mse: 5.7322 - val_loss: 4.7743 - val_mse: 4.7743\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.7238 - mse: 5.7238 - val_loss: 4.7919 - val_mse: 4.7919\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6480 - mse: 5.6480 - val_loss: 4.8300 - val_mse: 4.8300\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.5543 - mse: 5.5543 - val_loss: 4.8855 - val_mse: 4.8855\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4594 - mse: 5.4594 - val_loss: 4.9475 - val_mse: 4.9475\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3915 - mse: 5.3915 - val_loss: 5.0306 - val_mse: 5.0306\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3363 - mse: 5.3363 - val_loss: 5.1049 - val_mse: 5.1049\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3029 - mse: 5.3029 - val_loss: 5.0816 - val_mse: 5.0816\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3154 - mse: 5.3154 - val_loss: 5.0654 - val_mse: 5.0654\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3102 - mse: 5.3102 - val_loss: 5.1537 - val_mse: 5.1537\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2723 - mse: 5.2723 - val_loss: 5.3111 - val_mse: 5.3111\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2436 - mse: 5.2436 - val_loss: 5.5141 - val_mse: 5.5141\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2461 - mse: 5.2461 - val_loss: 5.6565 - val_mse: 5.6565\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2523 - mse: 5.2523 - val_loss: 5.7000 - val_mse: 5.7000\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 20ms/step - loss: 5.2575 - mse: 5.2575 - val_loss: 5.7146 - val_mse: 5.7146\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 20ms/step - loss: 5.2597 - mse: 5.2597 - val_loss: 5.7039 - val_mse: 5.7039\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2610 - mse: 5.2610 - val_loss: 5.7650 - val_mse: 5.7650\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2749 - mse: 5.2749 - val_loss: 5.8182 - val_mse: 5.8182\n",
      "wandb: Agent Finished Run: htlfsc3p \n",
      "\n",
      "wandb: Agent Starting Run: 147amzxx with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 16\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: 147amzxx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/147amzxx\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/147amzxx</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 4s 64ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 19.1223 - mse: 19.1223 - val_loss: 13.0701 - val_mse: 13.0701\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 19.0276 - mse: 19.0276 - val_loss: 12.9379 - val_mse: 12.9379\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 18.8544 - mse: 18.8544 - val_loss: 12.7363 - val_mse: 12.7363\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 18.5867 - mse: 18.5867 - val_loss: 12.4283 - val_mse: 12.4283\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 18.1747 - mse: 18.1747 - val_loss: 11.9552 - val_mse: 11.9552\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 17.5358 - mse: 17.5358 - val_loss: 11.2110 - val_mse: 11.2110\n",
      "wandb: Agent Finished Run: 147amzxx \n",
      "\n",
      "wandb: Agent Starting Run: 0o1w0yzs with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 32\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: 0o1w0yzs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/0o1w0yzs\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/0o1w0yzs</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 4s 65ms/step - loss: 19.1106 - mse: 19.1106 - val_loss: 12.7126 - val_mse: 12.7126\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 18.5281 - mse: 18.5281 - val_loss: 11.9610 - val_mse: 11.9610\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 17.5324 - mse: 17.5324 - val_loss: 10.4198 - val_mse: 10.4198\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 15.3568 - mse: 15.3568 - val_loss: 8.0499 - val_mse: 8.0499\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 12.0464 - mse: 12.0464 - val_loss: 5.8976 - val_mse: 5.8976\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 8.6947 - mse: 8.6947 - val_loss: 4.9277 - val_mse: 4.9277\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 6.8229 - mse: 6.8229 - val_loss: 4.7583 - val_mse: 4.7583\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.9795 - mse: 5.9795 - val_loss: 4.7786 - val_mse: 4.7786\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.6980 - mse: 5.6980 - val_loss: 4.8421 - val_mse: 4.8421\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.5240 - mse: 5.5240 - val_loss: 4.9679 - val_mse: 4.9679\n",
      "wandb: Agent Finished Run: 0o1w0yzs \n",
      "\n",
      "wandb: Agent Starting Run: 51mx768c with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 64\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: 51mx768c\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/51mx768c\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/51mx768c</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 71ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1198 - mse: 19.1198 - val_loss: 12.8103 - val_mse: 12.8103\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 18.6537 - mse: 18.6537 - val_loss: 11.8164 - val_mse: 11.8164\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 17.2420 - mse: 17.2420 - val_loss: 8.3226 - val_mse: 8.3226\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 12.1944 - mse: 12.1944 - val_loss: 4.8200 - val_mse: 4.8200\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 6.2521 - mse: 6.2521 - val_loss: 5.8818 - val_mse: 5.8818\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2971 - mse: 5.2971 - val_loss: 7.6447 - val_mse: 7.6447\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.9981 - mse: 5.9981 - val_loss: 7.3877 - val_mse: 7.3877\n",
      "wandb: Agent Finished Run: 51mx768c \n",
      "\n",
      "wandb: Agent Starting Run: vb2nzzj0 with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 16\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: vb2nzzj0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/vb2nzzj0\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/vb2nzzj0</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 4s 65ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1223 - mse: 19.1223 - val_loss: 13.0699 - val_mse: 13.0699\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.0273 - mse: 19.0273 - val_loss: 12.9377 - val_mse: 12.9377\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.8541 - mse: 18.8541 - val_loss: 12.7369 - val_mse: 12.7369\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.5876 - mse: 18.5876 - val_loss: 12.4316 - val_mse: 12.4316\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.1793 - mse: 18.1793 - val_loss: 11.9621 - val_mse: 11.9621\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 17.5454 - mse: 17.5454 - val_loss: 11.2293 - val_mse: 11.2293\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 16.5726 - mse: 16.5726 - val_loss: 10.1655 - val_mse: 10.1655\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 15.1387 - mse: 15.1387 - val_loss: 8.9710 - val_mse: 8.9710\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 13.4831 - mse: 13.4831 - val_loss: 7.9635 - val_mse: 7.9635\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 12.1159 - mse: 12.1159 - val_loss: 7.2472 - val_mse: 7.2472\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 11.0093 - mse: 11.0093 - val_loss: 6.6755 - val_mse: 6.6755\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 10.1369 - mse: 10.1369 - val_loss: 6.1713 - val_mse: 6.1713\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 9.2995 - mse: 9.2995 - val_loss: 5.7329 - val_mse: 5.7329\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 8.5206 - mse: 8.5206 - val_loss: 5.3687 - val_mse: 5.3687\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 7.8210 - mse: 7.8210 - val_loss: 5.0936 - val_mse: 5.0936\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 7.2257 - mse: 7.2257 - val_loss: 4.9126 - val_mse: 4.9126\n",
      "wandb: Agent Finished Run: vb2nzzj0 \n",
      "\n",
      "wandb: Agent Starting Run: 6m1xx75o with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 32\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: 6m1xx75o\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/6m1xx75o\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/6m1xx75o</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 67ms/step - loss: 19.1106 - mse: 19.1106 - val_loss: 12.7126 - val_mse: 12.7126\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 1s 20ms/step - loss: 18.5281 - mse: 18.5281 - val_loss: 11.9610 - val_mse: 11.9610\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 1s 20ms/step - loss: 17.5324 - mse: 17.5324 - val_loss: 10.4510 - val_mse: 10.4510\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 15.4021 - mse: 15.4021 - val_loss: 8.1097 - val_mse: 8.1097\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 12.1321 - mse: 12.1321 - val_loss: 5.9301 - val_mse: 5.9301\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 8.7500 - mse: 8.7500 - val_loss: 4.9370 - val_mse: 4.9370\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 6.8502 - mse: 6.8502 - val_loss: 4.7588 - val_mse: 4.7588\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.9941 - mse: 5.9941 - val_loss: 4.7767 - val_mse: 4.7767\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.7069 - mse: 5.7069 - val_loss: 4.8394 - val_mse: 4.8394\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.5290 - mse: 5.5290 - val_loss: 4.9653 - val_mse: 4.9653\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 5.3802 - mse: 5.3802 - val_loss: 5.1285 - val_mse: 5.1285\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.2736 - mse: 5.2736 - val_loss: 5.3419 - val_mse: 5.3419\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.2459 - mse: 5.2459 - val_loss: 5.5928 - val_mse: 5.5928\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.2459 - mse: 5.2459 - val_loss: 5.7807 - val_mse: 5.7807\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2767 - mse: 5.2767 - val_loss: 5.8329 - val_mse: 5.8329\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2802 - mse: 5.2802 - val_loss: 5.7721 - val_mse: 5.7721\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2713 - mse: 5.2713 - val_loss: 5.7439 - val_mse: 5.7439\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.2640 - mse: 5.2640 - val_loss: 5.8027 - val_mse: 5.8027\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2803 - mse: 5.2803 - val_loss: 5.8166 - val_mse: 5.8166\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2781 - mse: 5.2781 - val_loss: 5.7608 - val_mse: 5.7608\n",
      "wandb: Agent Finished Run: 6m1xx75o \n",
      "\n",
      "wandb: Agent Starting Run: aln4gv6w with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 64\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: aln4gv6w\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/aln4gv6w\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/aln4gv6w</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 73ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 19.1199 - mse: 19.1199 - val_loss: 12.8141 - val_mse: 12.8141\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 18.6591 - mse: 18.6591 - val_loss: 11.8276 - val_mse: 11.8276\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 17.2579 - mse: 17.2579 - val_loss: 8.3527 - val_mse: 8.3527\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 12.2381 - mse: 12.2381 - val_loss: 4.8245 - val_mse: 4.8245\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 6.2708 - mse: 6.2708 - val_loss: 5.8690 - val_mse: 5.8690\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2942 - mse: 5.2942 - val_loss: 7.6383 - val_mse: 7.6383\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.9950 - mse: 5.9950 - val_loss: 7.3913 - val_mse: 7.3913\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.8630 - mse: 5.8630 - val_loss: 6.3919 - val_mse: 6.3919\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4202 - mse: 5.4202 - val_loss: 5.5236 - val_mse: 5.5236\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2425 - mse: 5.2425 - val_loss: 5.0422 - val_mse: 5.0422\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.3119 - mse: 5.3119 - val_loss: 4.8354 - val_mse: 4.8354\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.5518 - mse: 5.5518 - val_loss: 4.7802 - val_mse: 4.7802\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6926 - mse: 5.6926 - val_loss: 4.8196 - val_mse: 4.8196\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.5523 - mse: 5.5523 - val_loss: 5.0033 - val_mse: 5.0033\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3521 - mse: 5.3521 - val_loss: 5.3373 - val_mse: 5.3373\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2334 - mse: 5.2334 - val_loss: 5.7246 - val_mse: 5.7246\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2597 - mse: 5.2597 - val_loss: 6.1405 - val_mse: 6.1405\n",
      "wandb: Agent Finished Run: aln4gv6w \n",
      "\n",
      "wandb: Agent Starting Run: o962fs86 with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 16\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: o962fs86\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/o962fs86\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/o962fs86</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 4s 65ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.1223 - mse: 19.1223 - val_loss: 13.0699 - val_mse: 13.0699\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 19.0273 - mse: 19.0273 - val_loss: 12.9377 - val_mse: 12.9377\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.8541 - mse: 18.8541 - val_loss: 12.7369 - val_mse: 12.7369\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.5876 - mse: 18.5876 - val_loss: 12.4305 - val_mse: 12.4305\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 18.1778 - mse: 18.1778 - val_loss: 11.9606 - val_mse: 11.9606\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 17.5433 - mse: 17.5433 - val_loss: 11.2270 - val_mse: 11.2270\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 16.5696 - mse: 16.5696 - val_loss: 10.1627 - val_mse: 10.1627\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 15.1350 - mse: 15.1350 - val_loss: 8.9684 - val_mse: 8.9684\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 13.4796 - mse: 13.4796 - val_loss: 7.9616 - val_mse: 7.9616\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 12.1131 - mse: 12.1131 - val_loss: 7.2458 - val_mse: 7.2458\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 11.0071 - mse: 11.0071 - val_loss: 6.6743 - val_mse: 6.6743\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 10.1350 - mse: 10.1350 - val_loss: 6.1703 - val_mse: 6.1703\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 9.2978 - mse: 9.2978 - val_loss: 5.7321 - val_mse: 5.7321\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 8.5192 - mse: 8.5192 - val_loss: 5.3681 - val_mse: 5.3681\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 7.8198 - mse: 7.8198 - val_loss: 5.0932 - val_mse: 5.0932\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 7.2248 - mse: 7.2248 - val_loss: 4.9116 - val_mse: 4.9116\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 6.7336 - mse: 6.7336 - val_loss: 4.8091 - val_mse: 4.8091\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 6.3434 - mse: 6.3434 - val_loss: 4.7622 - val_mse: 4.7622\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 6.0600 - mse: 6.0600 - val_loss: 4.7603 - val_mse: 4.7603\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.8348 - mse: 5.8348 - val_loss: 4.7852 - val_mse: 4.7852\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.6716 - mse: 5.6716 - val_loss: 4.8238 - val_mse: 4.8238\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.5638 - mse: 5.5638 - val_loss: 4.8740 - val_mse: 4.8740\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.4722 - mse: 5.4722 - val_loss: 4.9350 - val_mse: 4.9350\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3985 - mse: 5.3985 - val_loss: 5.0133 - val_mse: 5.0133\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3551 - mse: 5.3551 - val_loss: 5.0760 - val_mse: 5.0760\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3113 - mse: 5.3113 - val_loss: 5.0956 - val_mse: 5.0956\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.2998 - mse: 5.2998 - val_loss: 5.0975 - val_mse: 5.0975\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.2986 - mse: 5.2986 - val_loss: 5.0957 - val_mse: 5.0957\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.2992 - mse: 5.2992 - val_loss: 5.0875 - val_mse: 5.0875\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3032 - mse: 5.3032 - val_loss: 5.0794 - val_mse: 5.0794\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3064 - mse: 5.3064 - val_loss: 5.0732 - val_mse: 5.0732\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3096 - mse: 5.3096 - val_loss: 5.0712 - val_mse: 5.0712\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3107 - mse: 5.3107 - val_loss: 5.0718 - val_mse: 5.0718\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3100 - mse: 5.3100 - val_loss: 5.0675 - val_mse: 5.0675\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3125 - mse: 5.3125 - val_loss: 5.0622 - val_mse: 5.0622\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3149 - mse: 5.3149 - val_loss: 5.0582 - val_mse: 5.0582\n",
      "wandb: Agent Finished Run: o962fs86 \n",
      "\n",
      "wandb: Agent Starting Run: wh0w8d3a with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 32\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: wh0w8d3a\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/wh0w8d3a\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/wh0w8d3a</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 4s 66ms/step - loss: 19.1106 - mse: 19.1106 - val_loss: 12.7126 - val_mse: 12.7126\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 18.5281 - mse: 18.5281 - val_loss: 11.9917 - val_mse: 11.9917\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 17.5739 - mse: 17.5739 - val_loss: 10.4735 - val_mse: 10.4735\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 15.4314 - mse: 15.4314 - val_loss: 8.1155 - val_mse: 8.1155\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 12.1406 - mse: 12.1406 - val_loss: 5.9358 - val_mse: 5.9358\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 8.7597 - mse: 8.7597 - val_loss: 4.9389 - val_mse: 4.9389\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 6.8559 - mse: 6.8559 - val_loss: 4.7589 - val_mse: 4.7589\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.9972 - mse: 5.9972 - val_loss: 4.7763 - val_mse: 4.7763\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.7090 - mse: 5.7090 - val_loss: 4.8387 - val_mse: 4.8387\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.5302 - mse: 5.5302 - val_loss: 4.9647 - val_mse: 4.9647\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.3807 - mse: 5.3807 - val_loss: 5.1282 - val_mse: 5.1282\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2737 - mse: 5.2737 - val_loss: 5.3421 - val_mse: 5.3421\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2459 - mse: 5.2459 - val_loss: 5.5935 - val_mse: 5.5935\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2459 - mse: 5.2459 - val_loss: 5.7818 - val_mse: 5.7818\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2769 - mse: 5.2769 - val_loss: 5.8340 - val_mse: 5.8340\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2804 - mse: 5.2804 - val_loss: 5.7730 - val_mse: 5.7730\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2714 - mse: 5.2714 - val_loss: 5.7447 - val_mse: 5.7447\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2642 - mse: 5.2642 - val_loss: 5.8034 - val_mse: 5.8034\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2804 - mse: 5.2804 - val_loss: 5.8171 - val_mse: 5.8171\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2783 - mse: 5.2783 - val_loss: 5.7612 - val_mse: 5.7612\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2672 - mse: 5.2672 - val_loss: 5.7440 - val_mse: 5.7440\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2641 - mse: 5.2641 - val_loss: 5.7396 - val_mse: 5.7396\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2628 - mse: 5.2628 - val_loss: 5.6829 - val_mse: 5.6829\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2545 - mse: 5.2545 - val_loss: 5.5828 - val_mse: 5.5828\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2475 - mse: 5.2475 - val_loss: 5.5257 - val_mse: 5.5257\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2398 - mse: 5.2398 - val_loss: 5.5123 - val_mse: 5.5123\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2393 - mse: 5.2393 - val_loss: 5.4944 - val_mse: 5.4944\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2411 - mse: 5.2411 - val_loss: 5.5357 - val_mse: 5.5357\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2445 - mse: 5.2445 - val_loss: 5.6031 - val_mse: 5.6031\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2453 - mse: 5.2453 - val_loss: 5.6574 - val_mse: 5.6574\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2496 - mse: 5.2496 - val_loss: 5.7614 - val_mse: 5.7614\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2663 - mse: 5.2663 - val_loss: 5.8816 - val_mse: 5.8816\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2878 - mse: 5.2878 - val_loss: 6.0200 - val_mse: 6.0200\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.3228 - mse: 5.3228 - val_loss: 6.1746 - val_mse: 6.1746\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.3734 - mse: 5.3734 - val_loss: 6.2214 - val_mse: 6.2214\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.3826 - mse: 5.3826 - val_loss: 6.1766 - val_mse: 6.1766\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.3683 - mse: 5.3683 - val_loss: 6.1486 - val_mse: 6.1486\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.3601 - mse: 5.3601 - val_loss: 6.1364 - val_mse: 6.1364\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.3559 - mse: 5.3559 - val_loss: 6.1297 - val_mse: 6.1297\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.3541 - mse: 5.3541 - val_loss: 6.1290 - val_mse: 6.1290\n",
      "wandb: Agent Finished Run: wh0w8d3a \n",
      "\n",
      "wandb: Agent Starting Run: xroklajq with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 64\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.2\n",
      "wandb: Agent Started Run: xroklajq\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/xroklajq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/xroklajq</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 70ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1236 - mse: 19.1236 - val_loss: 13.1431 - val_mse: 13.1431\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 19.1199 - mse: 19.1199 - val_loss: 12.8141 - val_mse: 12.8141\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 18.6591 - mse: 18.6591 - val_loss: 11.8276 - val_mse: 11.8276\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 17.2579 - mse: 17.2579 - val_loss: 8.3527 - val_mse: 8.3527\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 12.2381 - mse: 12.2381 - val_loss: 4.8245 - val_mse: 4.8245\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 6.2708 - mse: 6.2708 - val_loss: 5.8690 - val_mse: 5.8690\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2942 - mse: 5.2942 - val_loss: 7.6383 - val_mse: 7.6383\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.9950 - mse: 5.9950 - val_loss: 7.3873 - val_mse: 7.3873\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.8611 - mse: 5.8611 - val_loss: 6.3895 - val_mse: 6.3895\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4192 - mse: 5.4192 - val_loss: 5.5198 - val_mse: 5.5198\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2423 - mse: 5.2423 - val_loss: 5.0412 - val_mse: 5.0412\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 5.3124 - mse: 5.3124 - val_loss: 4.8350 - val_mse: 4.8350\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.5526 - mse: 5.5526 - val_loss: 4.7803 - val_mse: 4.7803\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6923 - mse: 5.6923 - val_loss: 4.8200 - val_mse: 4.8200\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.5514 - mse: 5.5514 - val_loss: 5.0041 - val_mse: 5.0041\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 5.3515 - mse: 5.3515 - val_loss: 5.3384 - val_mse: 5.3384\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2332 - mse: 5.2332 - val_loss: 5.7281 - val_mse: 5.7281\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2603 - mse: 5.2603 - val_loss: 6.1439 - val_mse: 6.1439\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.3476 - mse: 5.3476 - val_loss: 6.5984 - val_mse: 6.5984\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.5251 - mse: 5.5251 - val_loss: 6.9183 - val_mse: 6.9183\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.6474 - mse: 5.6474 - val_loss: 6.8332 - val_mse: 6.8332\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6092 - mse: 5.6092 - val_loss: 6.5525 - val_mse: 6.5525\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.4920 - mse: 5.4920 - val_loss: 6.1945 - val_mse: 6.1945\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3943 - mse: 5.3943 - val_loss: 5.9971 - val_mse: 5.9971\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3174 - mse: 5.3174 - val_loss: 5.9163 - val_mse: 5.9163\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.3033 - mse: 5.3033 - val_loss: 5.8531 - val_mse: 5.8531\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2846 - mse: 5.2846 - val_loss: 5.7991 - val_mse: 5.7991\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2767 - mse: 5.2767 - val_loss: 5.7322 - val_mse: 5.7322\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2625 - mse: 5.2625 - val_loss: 5.7078 - val_mse: 5.7078\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2576 - mse: 5.2576 - val_loss: 5.6045 - val_mse: 5.6045\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2422 - mse: 5.2422 - val_loss: 5.4246 - val_mse: 5.4246\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2467 - mse: 5.2467 - val_loss: 5.3233 - val_mse: 5.3233\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2437 - mse: 5.2437 - val_loss: 5.3032 - val_mse: 5.3032\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2458 - mse: 5.2458 - val_loss: 5.2758 - val_mse: 5.2758\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2498 - mse: 5.2498 - val_loss: 5.2459 - val_mse: 5.2459\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2546 - mse: 5.2546 - val_loss: 5.2144 - val_mse: 5.2144\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2606 - mse: 5.2606 - val_loss: 5.1676 - val_mse: 5.1676\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2778 - mse: 5.2778 - val_loss: 5.1857 - val_mse: 5.1857\n",
      "wandb: Agent Finished Run: xroklajq \n",
      "\n",
      "wandb: Agent Starting Run: ztaifwhh with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 16\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: ztaifwhh\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/ztaifwhh\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/ztaifwhh</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 4s 64ms/step - loss: 19.1202 - mse: 19.1202 - val_loss: 12.9965 - val_mse: 12.9965\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.9314 - mse: 18.9314 - val_loss: 12.8009 - val_mse: 12.8009\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.6785 - mse: 18.6785 - val_loss: 12.5161 - val_mse: 12.5161\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 18.2896 - mse: 18.2896 - val_loss: 12.1018 - val_mse: 12.1018\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 17.7516 - mse: 17.7516 - val_loss: 11.4787 - val_mse: 11.4787\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 16.9280 - mse: 16.9280 - val_loss: 10.6529 - val_mse: 10.6529\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 15.8297 - mse: 15.8298 - val_loss: 9.7335 - val_mse: 9.7335\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 14.5931 - mse: 14.5931 - val_loss: 8.8302 - val_mse: 8.8302\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 13.3222 - mse: 13.3222 - val_loss: 7.9854 - val_mse: 7.9854\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 12.1181 - mse: 12.1181 - val_loss: 7.2182 - val_mse: 7.2182\n",
      "wandb: Agent Finished Run: ztaifwhh \n",
      "\n",
      "wandb: Agent Starting Run: jahqikxy with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 32\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: jahqikxy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/jahqikxy\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/jahqikxy</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 4s 65ms/step - loss: 19.1110 - mse: 19.1110 - val_loss: 12.8119 - val_mse: 12.8119\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 18.6668 - mse: 18.6668 - val_loss: 12.2112 - val_mse: 12.2112\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 17.8536 - mse: 17.8536 - val_loss: 10.9587 - val_mse: 10.9587\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 16.0930 - mse: 16.0930 - val_loss: 8.8241 - val_mse: 8.8241\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 13.1736 - mse: 13.1736 - val_loss: 6.4132 - val_mse: 6.4132\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 9.6023 - mse: 9.6023 - val_loss: 5.1043 - val_mse: 5.1043\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 7.2952 - mse: 7.2952 - val_loss: 4.7864 - val_mse: 4.7864\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 6.2556 - mse: 6.2556 - val_loss: 4.7581 - val_mse: 4.7581\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 5.8697 - mse: 5.8697 - val_loss: 4.8023 - val_mse: 4.8023\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 5.6163 - mse: 5.6163 - val_loss: 4.9324 - val_mse: 4.9324\n",
      "wandb: Agent Finished Run: jahqikxy \n",
      "\n",
      "wandb: Agent Starting Run: qmhsol5j with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 64\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: qmhsol5j\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/qmhsol5j\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/qmhsol5j</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 72ms/step - loss: 19.1107 - mse: 19.1107 - val_loss: 12.5794 - val_mse: 12.5794\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 18.3256 - mse: 18.3256 - val_loss: 10.5972 - val_mse: 10.5972\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 15.4720 - mse: 15.4720 - val_loss: 5.9366 - val_mse: 5.9366\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 8.8515 - mse: 8.8515 - val_loss: 4.7706 - val_mse: 4.7706\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.7578 - mse: 5.7578 - val_loss: 5.4435 - val_mse: 5.4435\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2339 - mse: 5.2339 - val_loss: 6.8285 - val_mse: 6.8285\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.5977 - mse: 5.5977 - val_loss: 7.7080 - val_mse: 7.7080\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 6.0212 - mse: 6.0212 - val_loss: 7.7312 - val_mse: 7.7312\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 6.0307 - mse: 6.0307 - val_loss: 6.9401 - val_mse: 6.9401\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.6353 - mse: 5.6353 - val_loss: 5.8906 - val_mse: 5.8906\n",
      "wandb: Agent Finished Run: qmhsol5j \n",
      "\n",
      "wandb: Agent Starting Run: l4p0ts96 with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 16\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: l4p0ts96\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/l4p0ts96\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/l4p0ts96</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 4s 65ms/step - loss: 19.1202 - mse: 19.1202 - val_loss: 12.9965 - val_mse: 12.9965\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 18.9314 - mse: 18.9314 - val_loss: 12.8009 - val_mse: 12.8009\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 18.6785 - mse: 18.6785 - val_loss: 12.5161 - val_mse: 12.5161\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.2896 - mse: 18.2896 - val_loss: 12.1018 - val_mse: 12.1018\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 17.7516 - mse: 17.7516 - val_loss: 11.4787 - val_mse: 11.4787\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 16.9280 - mse: 16.9280 - val_loss: 10.6529 - val_mse: 10.6529\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 15.8297 - mse: 15.8298 - val_loss: 9.7335 - val_mse: 9.7335\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 14.5931 - mse: 14.5931 - val_loss: 8.8302 - val_mse: 8.8302\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 13.3222 - mse: 13.3222 - val_loss: 7.9854 - val_mse: 7.9854\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 12.1181 - mse: 12.1181 - val_loss: 7.2182 - val_mse: 7.2182\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 10.9853 - mse: 10.9853 - val_loss: 6.5901 - val_mse: 6.5901\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 9.9983 - mse: 9.9983 - val_loss: 6.0943 - val_mse: 6.0943\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 9.1742 - mse: 9.1742 - val_loss: 5.7045 - val_mse: 5.7045\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 8.5029 - mse: 8.5029 - val_loss: 5.4282 - val_mse: 5.4282\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 7.9464 - mse: 7.9464 - val_loss: 5.2247 - val_mse: 5.2247\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 7.5319 - mse: 7.5319 - val_loss: 5.0654 - val_mse: 5.0654\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 7.1531 - mse: 7.1531 - val_loss: 4.9459 - val_mse: 4.9459\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 6.8303 - mse: 6.8303 - val_loss: 4.8581 - val_mse: 4.8581\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 6.5532 - mse: 6.5532 - val_loss: 4.7997 - val_mse: 4.7997\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 6.3192 - mse: 6.3192 - val_loss: 4.7683 - val_mse: 4.7683\n",
      "wandb: Agent Finished Run: l4p0ts96 \n",
      "\n",
      "wandb: Agent Starting Run: nl38ctsv with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 32\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: nl38ctsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/nl38ctsv\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/nl38ctsv</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 4s 67ms/step - loss: 19.1082 - mse: 19.1082 - val_loss: 12.7149 - val_mse: 12.7149\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 18.5327 - mse: 18.5327 - val_loss: 11.9587 - val_mse: 11.9587\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 17.5094 - mse: 17.5094 - val_loss: 10.4000 - val_mse: 10.4000\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 15.3164 - mse: 15.3164 - val_loss: 8.0103 - val_mse: 8.0103\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 12.0258 - mse: 12.0258 - val_loss: 5.8715 - val_mse: 5.8715\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 8.7074 - mse: 8.7074 - val_loss: 4.9271 - val_mse: 4.9271\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 6.8288 - mse: 6.8288 - val_loss: 4.7585 - val_mse: 4.7585\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.9985 - mse: 5.9985 - val_loss: 4.7761 - val_mse: 4.7761\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.7135 - mse: 5.7135 - val_loss: 4.8386 - val_mse: 4.8386\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.5321 - mse: 5.5321 - val_loss: 4.9666 - val_mse: 4.9666\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.3803 - mse: 5.3803 - val_loss: 5.1339 - val_mse: 5.1339\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2719 - mse: 5.2719 - val_loss: 5.3530 - val_mse: 5.3530\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2451 - mse: 5.2451 - val_loss: 5.6094 - val_mse: 5.6094\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2472 - mse: 5.2472 - val_loss: 5.7994 - val_mse: 5.7994\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2801 - mse: 5.2801 - val_loss: 5.8501 - val_mse: 5.8501\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2829 - mse: 5.2829 - val_loss: 5.7858 - val_mse: 5.7858\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2738 - mse: 5.2738 - val_loss: 5.7544 - val_mse: 5.7544\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2658 - mse: 5.2658 - val_loss: 5.8110 - val_mse: 5.8110\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2821 - mse: 5.2821 - val_loss: 5.8226 - val_mse: 5.8226\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2792 - mse: 5.2792 - val_loss: 5.7643 - val_mse: 5.7643\n",
      "wandb: Agent Finished Run: nl38ctsv \n",
      "\n",
      "wandb: Agent Starting Run: bqvq2bfz with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 64\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: bqvq2bfz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/bqvq2bfz\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/bqvq2bfz</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 71ms/step - loss: 19.1066 - mse: 19.1066 - val_loss: 12.4618 - val_mse: 12.4618\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 18.1542 - mse: 18.1542 - val_loss: 9.8894 - val_mse: 9.8894\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 14.4700 - mse: 14.4700 - val_loss: 5.3440 - val_mse: 5.3440\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 7.7807 - mse: 7.7807 - val_loss: 4.8388 - val_mse: 4.8388\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.5249 - mse: 5.5249 - val_loss: 5.4971 - val_mse: 5.4971\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2358 - mse: 5.2358 - val_loss: 6.6066 - val_mse: 6.6066\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.5079 - mse: 5.5079 - val_loss: 7.4269 - val_mse: 7.4269\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.8799 - mse: 5.8799 - val_loss: 7.6064 - val_mse: 7.6064\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.9678 - mse: 5.9678 - val_loss: 6.9814 - val_mse: 6.9814\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6552 - mse: 5.6552 - val_loss: 5.9918 - val_mse: 5.9918\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3227 - mse: 5.3227 - val_loss: 5.3227 - val_mse: 5.3227\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2364 - mse: 5.2364 - val_loss: 5.0099 - val_mse: 5.0099\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3490 - mse: 5.3490 - val_loss: 4.8886 - val_mse: 4.8886\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4552 - mse: 5.4552 - val_loss: 4.8315 - val_mse: 4.8315\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.5528 - mse: 5.5528 - val_loss: 4.8167 - val_mse: 4.8167\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.5748 - mse: 5.5748 - val_loss: 4.8966 - val_mse: 4.8966\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4255 - mse: 5.4255 - val_loss: 5.1449 - val_mse: 5.1449\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2857 - mse: 5.2857 - val_loss: 5.5391 - val_mse: 5.5391\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2329 - mse: 5.2329 - val_loss: 5.9613 - val_mse: 5.9613\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3084 - mse: 5.3084 - val_loss: 6.3755 - val_mse: 6.3755\n",
      "wandb: Agent Finished Run: bqvq2bfz \n",
      "\n",
      "wandb: Agent Starting Run: dgf7uktc with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 16\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: dgf7uktc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/dgf7uktc\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/dgf7uktc</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 4s 64ms/step - loss: 19.1202 - mse: 19.1202 - val_loss: 12.9965 - val_mse: 12.9965\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.9314 - mse: 18.9314 - val_loss: 12.8009 - val_mse: 12.8009\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.6785 - mse: 18.6785 - val_loss: 12.5161 - val_mse: 12.5161\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 18.2896 - mse: 18.2896 - val_loss: 12.1018 - val_mse: 12.1018\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 17.7516 - mse: 17.7516 - val_loss: 11.4787 - val_mse: 11.4787\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 16.9280 - mse: 16.9280 - val_loss: 10.6529 - val_mse: 10.6529\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 15.8297 - mse: 15.8298 - val_loss: 9.7335 - val_mse: 9.7335\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 14.5931 - mse: 14.5931 - val_loss: 8.8302 - val_mse: 8.8302\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 13.3222 - mse: 13.3222 - val_loss: 7.9854 - val_mse: 7.9854\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 12.1181 - mse: 12.1181 - val_loss: 7.2182 - val_mse: 7.2182\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 10.9853 - mse: 10.9853 - val_loss: 6.5901 - val_mse: 6.5901\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 9.9983 - mse: 9.9983 - val_loss: 6.0943 - val_mse: 6.0943\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 9.1742 - mse: 9.1742 - val_loss: 5.7045 - val_mse: 5.7045\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 8.5029 - mse: 8.5029 - val_loss: 5.4282 - val_mse: 5.4282\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 7.9464 - mse: 7.9464 - val_loss: 5.2247 - val_mse: 5.2247\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 7.5319 - mse: 7.5319 - val_loss: 5.0654 - val_mse: 5.0654\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 7.1531 - mse: 7.1531 - val_loss: 4.9459 - val_mse: 4.9459\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 6.8303 - mse: 6.8303 - val_loss: 4.8581 - val_mse: 4.8581\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 6.5532 - mse: 6.5532 - val_loss: 4.7997 - val_mse: 4.7997\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 6.3192 - mse: 6.3192 - val_loss: 4.7683 - val_mse: 4.7683\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 6.1208 - mse: 6.1208 - val_loss: 4.7570 - val_mse: 4.7570\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.9580 - mse: 5.9580 - val_loss: 4.7600 - val_mse: 4.7600\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.8351 - mse: 5.8351 - val_loss: 4.7748 - val_mse: 4.7748\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.7252 - mse: 5.7252 - val_loss: 4.7951 - val_mse: 4.7951\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.6385 - mse: 5.6385 - val_loss: 4.8185 - val_mse: 4.8185\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.5767 - mse: 5.5767 - val_loss: 4.8459 - val_mse: 4.8459\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.5191 - mse: 5.5191 - val_loss: 4.8780 - val_mse: 4.8780\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.4668 - mse: 5.4668 - val_loss: 4.9190 - val_mse: 4.9190\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.4292 - mse: 5.4292 - val_loss: 4.9531 - val_mse: 4.9531\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.3902 - mse: 5.3902 - val_loss: 4.9670 - val_mse: 4.9670\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3757 - mse: 5.3757 - val_loss: 4.9726 - val_mse: 4.9726\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3720 - mse: 5.3720 - val_loss: 4.9770 - val_mse: 4.9770\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3682 - mse: 5.3682 - val_loss: 4.9783 - val_mse: 4.9783\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3675 - mse: 5.3675 - val_loss: 4.9798 - val_mse: 4.9798\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3671 - mse: 5.3671 - val_loss: 4.9825 - val_mse: 4.9825\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3636 - mse: 5.3636 - val_loss: 4.9877 - val_mse: 4.9877\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3605 - mse: 5.3605 - val_loss: 4.9945 - val_mse: 4.9945\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3556 - mse: 5.3556 - val_loss: 4.9981 - val_mse: 4.9981\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3532 - mse: 5.3532 - val_loss: 5.0009 - val_mse: 5.0009\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3513 - mse: 5.3513 - val_loss: 5.0042 - val_mse: 5.0042\n",
      "wandb: Agent Finished Run: dgf7uktc \n",
      "\n",
      "wandb: Agent Starting Run: qnlw1aor with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 32\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: qnlw1aor\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/qnlw1aor\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/qnlw1aor</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 69ms/step - loss: 19.1110 - mse: 19.1110 - val_loss: 12.8119 - val_mse: 12.8119\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 18.6668 - mse: 18.6668 - val_loss: 12.2112 - val_mse: 12.2112\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 17.8536 - mse: 17.8536 - val_loss: 10.9587 - val_mse: 10.9587\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 16.0930 - mse: 16.0930 - val_loss: 8.8241 - val_mse: 8.8241\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 13.1736 - mse: 13.1736 - val_loss: 6.4132 - val_mse: 6.4132\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 9.6023 - mse: 9.6023 - val_loss: 5.1043 - val_mse: 5.1043\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 7.2952 - mse: 7.2952 - val_loss: 4.7864 - val_mse: 4.7864\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 6.2556 - mse: 6.2556 - val_loss: 4.7581 - val_mse: 4.7581\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.8697 - mse: 5.8697 - val_loss: 4.8023 - val_mse: 4.8023\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.6163 - mse: 5.6163 - val_loss: 4.9324 - val_mse: 4.9324\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.4116 - mse: 5.4116 - val_loss: 5.1219 - val_mse: 5.1219\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2738 - mse: 5.2738 - val_loss: 5.3747 - val_mse: 5.3747\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2442 - mse: 5.2442 - val_loss: 5.6631 - val_mse: 5.6631\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2536 - mse: 5.2536 - val_loss: 5.8727 - val_mse: 5.8727\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2950 - mse: 5.2950 - val_loss: 5.9245 - val_mse: 5.9245\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2991 - mse: 5.2991 - val_loss: 5.8483 - val_mse: 5.8483\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2861 - mse: 5.2861 - val_loss: 5.8050 - val_mse: 5.8050\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.2753 - mse: 5.2753 - val_loss: 5.8522 - val_mse: 5.8522\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2897 - mse: 5.2897 - val_loss: 5.8533 - val_mse: 5.8533\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2855 - mse: 5.2855 - val_loss: 5.7840 - val_mse: 5.7840\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2711 - mse: 5.2711 - val_loss: 5.7557 - val_mse: 5.7557\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2658 - mse: 5.2658 - val_loss: 5.7419 - val_mse: 5.7419\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2628 - mse: 5.2628 - val_loss: 5.6772 - val_mse: 5.6772\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2537 - mse: 5.2537 - val_loss: 5.5710 - val_mse: 5.5710\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2467 - mse: 5.2467 - val_loss: 5.5102 - val_mse: 5.5102\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2385 - mse: 5.2385 - val_loss: 5.4953 - val_mse: 5.4953\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2385 - mse: 5.2385 - val_loss: 5.4769 - val_mse: 5.4769\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2409 - mse: 5.2409 - val_loss: 5.5189 - val_mse: 5.5189\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2438 - mse: 5.2438 - val_loss: 5.5878 - val_mse: 5.5878\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2435 - mse: 5.2435 - val_loss: 5.6441 - val_mse: 5.6441\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2481 - mse: 5.2481 - val_loss: 5.7505 - val_mse: 5.7505\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2644 - mse: 5.2644 - val_loss: 5.8730 - val_mse: 5.8730\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2857 - mse: 5.2857 - val_loss: 6.0131 - val_mse: 6.0131\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.3204 - mse: 5.3204 - val_loss: 6.1685 - val_mse: 6.1685\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.3712 - mse: 5.3712 - val_loss: 6.2182 - val_mse: 6.2182\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.3813 - mse: 5.3813 - val_loss: 6.1774 - val_mse: 6.1774\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.3684 - mse: 5.3684 - val_loss: 6.1524 - val_mse: 6.1524\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.3607 - mse: 5.3607 - val_loss: 6.1424 - val_mse: 6.1424\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.3573 - mse: 5.3573 - val_loss: 6.1371 - val_mse: 6.1371\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.3558 - mse: 5.3558 - val_loss: 6.1373 - val_mse: 6.1373\n",
      "wandb: Agent Finished Run: qnlw1aor \n",
      "\n",
      "wandb: Agent Starting Run: v8ir2re0 with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 64\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.4\n",
      "wandb: Agent Started Run: v8ir2re0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/v8ir2re0\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/v8ir2re0</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 70ms/step - loss: 19.1066 - mse: 19.1066 - val_loss: 12.4618 - val_mse: 12.4618\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 18.1542 - mse: 18.1542 - val_loss: 9.8894 - val_mse: 9.8894\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 14.4700 - mse: 14.4700 - val_loss: 5.3440 - val_mse: 5.3440\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 7.7807 - mse: 7.7807 - val_loss: 4.8388 - val_mse: 4.8388\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.5249 - mse: 5.5249 - val_loss: 5.4971 - val_mse: 5.4971\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2358 - mse: 5.2358 - val_loss: 6.6066 - val_mse: 6.6066\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 5.5079 - mse: 5.5079 - val_loss: 7.4269 - val_mse: 7.4269\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.8799 - mse: 5.8799 - val_loss: 7.6064 - val_mse: 7.6064\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.9678 - mse: 5.9678 - val_loss: 6.9814 - val_mse: 6.9814\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6552 - mse: 5.6552 - val_loss: 5.9918 - val_mse: 5.9918\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3227 - mse: 5.3227 - val_loss: 5.3227 - val_mse: 5.3227\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2364 - mse: 5.2364 - val_loss: 5.0099 - val_mse: 5.0099\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.3490 - mse: 5.3490 - val_loss: 4.8886 - val_mse: 4.8886\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4552 - mse: 5.4552 - val_loss: 4.8315 - val_mse: 4.8315\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.5528 - mse: 5.5528 - val_loss: 4.8167 - val_mse: 4.8167\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.5748 - mse: 5.5748 - val_loss: 4.8966 - val_mse: 4.8966\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4255 - mse: 5.4255 - val_loss: 5.1449 - val_mse: 5.1449\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2857 - mse: 5.2857 - val_loss: 5.5391 - val_mse: 5.5391\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2329 - mse: 5.2329 - val_loss: 5.9613 - val_mse: 5.9613\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3084 - mse: 5.3084 - val_loss: 6.3755 - val_mse: 6.3755\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4247 - mse: 5.4247 - val_loss: 6.8001 - val_mse: 6.8001\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6057 - mse: 5.6057 - val_loss: 7.0497 - val_mse: 7.0497\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.7036 - mse: 5.7036 - val_loss: 6.8583 - val_mse: 6.8583\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6192 - mse: 5.6192 - val_loss: 6.4890 - val_mse: 6.4890\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4671 - mse: 5.4671 - val_loss: 6.0804 - val_mse: 6.0804\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3635 - mse: 5.3635 - val_loss: 5.8654 - val_mse: 5.8654\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2867 - mse: 5.2867 - val_loss: 5.7853 - val_mse: 5.7853\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2764 - mse: 5.2764 - val_loss: 5.7339 - val_mse: 5.7339\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2631 - mse: 5.2631 - val_loss: 5.6997 - val_mse: 5.6997\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2599 - mse: 5.2599 - val_loss: 5.6567 - val_mse: 5.6567\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2518 - mse: 5.2518 - val_loss: 5.6578 - val_mse: 5.6578\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 5.2518 - mse: 5.2518 - val_loss: 5.5758 - val_mse: 5.5758\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 5.2395 - mse: 5.2395 - val_loss: 5.4086 - val_mse: 5.4086\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2464 - mse: 5.2464 - val_loss: 5.3184 - val_mse: 5.3184\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 20ms/step - loss: 5.2440 - mse: 5.2440 - val_loss: 5.3105 - val_mse: 5.3105\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2451 - mse: 5.2451 - val_loss: 5.2913 - val_mse: 5.2913\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2476 - mse: 5.2476 - val_loss: 5.2657 - val_mse: 5.2657\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2512 - mse: 5.2512 - val_loss: 5.2351 - val_mse: 5.2351\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 20ms/step - loss: 5.2561 - mse: 5.2561 - val_loss: 5.1850 - val_mse: 5.1850\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2734 - mse: 5.2734 - val_loss: 5.2048 - val_mse: 5.2048\n",
      "wandb: Agent Finished Run: v8ir2re0 \n",
      "\n",
      "wandb: Agent Starting Run: qiex6jm0 with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 16\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: qiex6jm0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/qiex6jm0\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/qiex6jm0</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 4s 65ms/step - loss: 19.1184 - mse: 19.1184 - val_loss: 12.9867 - val_mse: 12.9867\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 18.9168 - mse: 18.9168 - val_loss: 12.7832 - val_mse: 12.7832\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.6556 - mse: 18.6556 - val_loss: 12.4964 - val_mse: 12.4964\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.2760 - mse: 18.2760 - val_loss: 12.0861 - val_mse: 12.0861\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 17.7405 - mse: 17.7405 - val_loss: 11.4802 - val_mse: 11.4802\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 16.9608 - mse: 16.9608 - val_loss: 10.6627 - val_mse: 10.6627\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 15.8787 - mse: 15.8787 - val_loss: 9.7287 - val_mse: 9.7287\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 14.6168 - mse: 14.6168 - val_loss: 8.8089 - val_mse: 8.8089\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 13.3583 - mse: 13.3583 - val_loss: 7.9703 - val_mse: 7.9703\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 12.1524 - mse: 12.1524 - val_loss: 7.2409 - val_mse: 7.2409\n",
      "wandb: Agent Finished Run: qiex6jm0 \n",
      "\n",
      "wandb: Agent Starting Run: l8p3bss9 with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 32\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: l8p3bss9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/l8p3bss9\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/l8p3bss9</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 4s 65ms/step - loss: 19.1023 - mse: 19.1023 - val_loss: 12.7055 - val_mse: 12.7055\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 18.5155 - mse: 18.5155 - val_loss: 11.9376 - val_mse: 11.9376\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 17.4801 - mse: 17.4801 - val_loss: 10.3538 - val_mse: 10.3538\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 15.3108 - mse: 15.3108 - val_loss: 7.9183 - val_mse: 7.9183\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 11.9894 - mse: 11.9894 - val_loss: 5.7963 - val_mse: 5.7963\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 8.6625 - mse: 8.6625 - val_loss: 4.9082 - val_mse: 4.9082\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 6.7971 - mse: 6.7971 - val_loss: 4.7578 - val_mse: 4.7578\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.9844 - mse: 5.9844 - val_loss: 4.7776 - val_mse: 4.7776\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.7099 - mse: 5.7099 - val_loss: 4.8425 - val_mse: 4.8425\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.5276 - mse: 5.5276 - val_loss: 4.9775 - val_mse: 4.9775\n",
      "wandb: Agent Finished Run: l8p3bss9 \n",
      "\n",
      "wandb: Agent Starting Run: 4bwq7wjy with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 64\n",
      "\tepochs: 10\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: 4bwq7wjy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/4bwq7wjy\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/4bwq7wjy</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 72ms/step - loss: 19.1032 - mse: 19.1032 - val_loss: 12.4402 - val_mse: 12.4402\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 18.1296 - mse: 18.1296 - val_loss: 9.8002 - val_mse: 9.8002\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 14.3979 - mse: 14.3979 - val_loss: 5.3136 - val_mse: 5.3136\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 7.7610 - mse: 7.7610 - val_loss: 4.8558 - val_mse: 4.8558\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.5070 - mse: 5.5070 - val_loss: 5.5080 - val_mse: 5.5080\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2351 - mse: 5.2351 - val_loss: 6.5630 - val_mse: 6.5630\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.4928 - mse: 5.4928 - val_loss: 7.3661 - val_mse: 7.3661\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.8488 - mse: 5.8488 - val_loss: 7.5609 - val_mse: 7.5609\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 5.9521 - mse: 5.9521 - val_loss: 6.9477 - val_mse: 6.9477\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.6448 - mse: 5.6448 - val_loss: 5.9835 - val_mse: 5.9835\n",
      "wandb: Agent Finished Run: 4bwq7wjy \n",
      "\n",
      "wandb: Agent Starting Run: ezrqbsyq with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 16\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: ezrqbsyq\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/ezrqbsyq\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/ezrqbsyq</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 4s 64ms/step - loss: 19.1184 - mse: 19.1184 - val_loss: 12.9867 - val_mse: 12.9867\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 18.9168 - mse: 18.9168 - val_loss: 12.7832 - val_mse: 12.7832\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.6556 - mse: 18.6556 - val_loss: 12.4964 - val_mse: 12.4964\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.2760 - mse: 18.2760 - val_loss: 12.0861 - val_mse: 12.0861\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 17.7405 - mse: 17.7405 - val_loss: 11.4802 - val_mse: 11.4802\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 16.9608 - mse: 16.9608 - val_loss: 10.6627 - val_mse: 10.6627\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 15.8787 - mse: 15.8787 - val_loss: 9.7287 - val_mse: 9.7287\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 14.6168 - mse: 14.6168 - val_loss: 8.8089 - val_mse: 8.8089\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 13.3583 - mse: 13.3583 - val_loss: 7.9703 - val_mse: 7.9703\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 12.1524 - mse: 12.1524 - val_loss: 7.2409 - val_mse: 7.2409\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 11.0629 - mse: 11.0629 - val_loss: 6.6479 - val_mse: 6.6479\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 10.1155 - mse: 10.1155 - val_loss: 6.1660 - val_mse: 6.1660\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 9.3094 - mse: 9.3094 - val_loss: 5.7638 - val_mse: 5.7638\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 8.6196 - mse: 8.6196 - val_loss: 5.4603 - val_mse: 5.4603\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 8.0097 - mse: 8.0097 - val_loss: 5.2312 - val_mse: 5.2312\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 7.5498 - mse: 7.5498 - val_loss: 5.0549 - val_mse: 5.0549\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 7.1289 - mse: 7.1289 - val_loss: 4.9269 - val_mse: 4.9269\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 6.7746 - mse: 6.7746 - val_loss: 4.8372 - val_mse: 4.8372\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 6.4784 - mse: 6.4784 - val_loss: 4.7822 - val_mse: 4.7822\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 6.2288 - mse: 6.2288 - val_loss: 4.7588 - val_mse: 4.7588\n",
      "wandb: Agent Finished Run: ezrqbsyq \n",
      "\n",
      "wandb: Agent Starting Run: mj8jz3gg with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 32\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: mj8jz3gg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/mj8jz3gg\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/mj8jz3gg</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 4s 66ms/step - loss: 19.1023 - mse: 19.1023 - val_loss: 12.7055 - val_mse: 12.7055\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 18.5155 - mse: 18.5155 - val_loss: 11.9376 - val_mse: 11.9376\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 17.4801 - mse: 17.4801 - val_loss: 10.3859 - val_mse: 10.3859\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 15.3545 - mse: 15.3545 - val_loss: 7.9537 - val_mse: 7.9537\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 12.0398 - mse: 12.0398 - val_loss: 5.8147 - val_mse: 5.8147\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 8.6955 - mse: 8.6955 - val_loss: 4.9131 - val_mse: 4.9131\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 6.8124 - mse: 6.8124 - val_loss: 4.7580 - val_mse: 4.7580\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.9931 - mse: 5.9931 - val_loss: 4.7764 - val_mse: 4.7764\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.7155 - mse: 5.7155 - val_loss: 4.8407 - val_mse: 4.8407\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.5308 - mse: 5.5308 - val_loss: 4.9758 - val_mse: 4.9758\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.3739 - mse: 5.3739 - val_loss: 5.1530 - val_mse: 5.1530\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.2640 - mse: 5.2640 - val_loss: 5.3837 - val_mse: 5.3837\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 5.2441 - mse: 5.2441 - val_loss: 5.6524 - val_mse: 5.6524\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 1s 20ms/step - loss: 5.2523 - mse: 5.2523 - val_loss: 5.8433 - val_mse: 5.8433\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 5.2886 - mse: 5.2886 - val_loss: 5.8867 - val_mse: 5.8867\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 5.2894 - mse: 5.2894 - val_loss: 5.8125 - val_mse: 5.8125\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 5.2784 - mse: 5.2784 - val_loss: 5.7724 - val_mse: 5.7724\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.2689 - mse: 5.2689 - val_loss: 5.8222 - val_mse: 5.8222\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.2842 - mse: 5.2842 - val_loss: 5.8278 - val_mse: 5.8278\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 5.2798 - mse: 5.2798 - val_loss: 5.7645 - val_mse: 5.7645\n",
      "wandb: Agent Finished Run: mj8jz3gg \n",
      "\n",
      "wandb: Agent Starting Run: eofddjtl with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 64\n",
      "\tepochs: 20\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: eofddjtl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/eofddjtl\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/eofddjtl</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 5s 72ms/step - loss: 19.1078 - mse: 19.1078 - val_loss: 12.5685 - val_mse: 12.5685\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 18.3156 - mse: 18.3156 - val_loss: 10.5694 - val_mse: 10.5694\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 15.4829 - mse: 15.4829 - val_loss: 5.9839 - val_mse: 5.9839\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 8.9746 - mse: 8.9746 - val_loss: 4.7695 - val_mse: 4.7695\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.7880 - mse: 5.7880 - val_loss: 5.4247 - val_mse: 5.4247\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2333 - mse: 5.2333 - val_loss: 6.7816 - val_mse: 6.7816\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.5776 - mse: 5.5776 - val_loss: 7.6995 - val_mse: 7.6995\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 2s 22ms/step - loss: 6.0167 - mse: 6.0167 - val_loss: 7.7659 - val_mse: 7.7659\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 6.0536 - mse: 6.0536 - val_loss: 6.9835 - val_mse: 6.9835\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6573 - mse: 5.6573 - val_loss: 5.9222 - val_mse: 5.9222\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.3088 - mse: 5.3088 - val_loss: 5.2599 - val_mse: 5.2599\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.2459 - mse: 5.2459 - val_loss: 4.9716 - val_mse: 4.9716\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.3763 - mse: 5.3763 - val_loss: 4.8703 - val_mse: 4.8703\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.4812 - mse: 5.4812 - val_loss: 4.8254 - val_mse: 4.8254\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.5641 - mse: 5.5641 - val_loss: 4.8170 - val_mse: 4.8170\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.5735 - mse: 5.5735 - val_loss: 4.9042 - val_mse: 4.9042\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 2s 24ms/step - loss: 5.4152 - mse: 5.4152 - val_loss: 5.1644 - val_mse: 5.1644\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2800 - mse: 5.2800 - val_loss: 5.5698 - val_mse: 5.5698\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2352 - mse: 5.2352 - val_loss: 5.9968 - val_mse: 5.9968\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.3170 - mse: 5.3170 - val_loss: 6.4080 - val_mse: 6.4080\n",
      "wandb: Agent Finished Run: eofddjtl \n",
      "\n",
      "wandb: Agent Starting Run: 6y5vj22v with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 16\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: 6y5vj22v\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/6y5vj22v\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/6y5vj22v</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 4s 64ms/step - loss: 19.1194 - mse: 19.1194 - val_loss: 13.0221 - val_mse: 13.0221\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.9635 - mse: 18.9635 - val_loss: 12.8463 - val_mse: 12.8463\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 18.7381 - mse: 18.7381 - val_loss: 12.5985 - val_mse: 12.5985\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 18.4092 - mse: 18.4092 - val_loss: 12.2398 - val_mse: 12.2398\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 17.9414 - mse: 17.9414 - val_loss: 11.7041 - val_mse: 11.7041\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 17.2518 - mse: 17.2518 - val_loss: 10.9467 - val_mse: 10.9467\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 16.2516 - mse: 16.2516 - val_loss: 10.0258 - val_mse: 10.0258\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 15.0184 - mse: 15.0184 - val_loss: 9.0777 - val_mse: 9.0777\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 13.7327 - mse: 13.7327 - val_loss: 8.1925 - val_mse: 8.1925\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 12.4736 - mse: 12.4736 - val_loss: 7.4115 - val_mse: 7.4115\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 11.3229 - mse: 11.3229 - val_loss: 6.7724 - val_mse: 6.7724\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 10.3163 - mse: 10.3163 - val_loss: 6.2555 - val_mse: 6.2555\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 9.4627 - mse: 9.4627 - val_loss: 5.8299 - val_mse: 5.8299\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 8.7405 - mse: 8.7405 - val_loss: 5.5078 - val_mse: 5.5078\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 8.1036 - mse: 8.1036 - val_loss: 5.2649 - val_mse: 5.2649\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 7.6234 - mse: 7.6234 - val_loss: 5.0776 - val_mse: 5.0776\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 7.1854 - mse: 7.1854 - val_loss: 4.9413 - val_mse: 4.9413\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 6.8173 - mse: 6.8173 - val_loss: 4.8458 - val_mse: 4.8458\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 6.5115 - mse: 6.5115 - val_loss: 4.7862 - val_mse: 4.7862\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 6.2531 - mse: 6.2531 - val_loss: 4.7598 - val_mse: 4.7598\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 6.0315 - mse: 6.0315 - val_loss: 4.7584 - val_mse: 4.7584\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.8520 - mse: 5.8520 - val_loss: 4.7766 - val_mse: 4.7766\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.7188 - mse: 5.7188 - val_loss: 4.8109 - val_mse: 4.8109\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.6040 - mse: 5.6040 - val_loss: 4.8488 - val_mse: 4.8488\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.5157 - mse: 5.5157 - val_loss: 4.8876 - val_mse: 4.8876\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.4608 - mse: 5.4608 - val_loss: 4.9296 - val_mse: 4.9296\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.4086 - mse: 5.4086 - val_loss: 4.9757 - val_mse: 4.9757\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3675 - mse: 5.3675 - val_loss: 5.0322 - val_mse: 5.0322\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.3406 - mse: 5.3406 - val_loss: 5.0751 - val_mse: 5.0751\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.3117 - mse: 5.3117 - val_loss: 5.0861 - val_mse: 5.0861\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 5.3052 - mse: 5.3052 - val_loss: 5.0846 - val_mse: 5.0846\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3042 - mse: 5.3042 - val_loss: 5.0811 - val_mse: 5.0811\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3057 - mse: 5.3057 - val_loss: 5.0735 - val_mse: 5.0735\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3104 - mse: 5.3104 - val_loss: 5.0664 - val_mse: 5.0664\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3141 - mse: 5.3141 - val_loss: 5.0612 - val_mse: 5.0612\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3162 - mse: 5.3162 - val_loss: 5.0595 - val_mse: 5.0595\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3162 - mse: 5.3162 - val_loss: 5.0601 - val_mse: 5.0601\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3161 - mse: 5.3161 - val_loss: 5.0571 - val_mse: 5.0571\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3176 - mse: 5.3176 - val_loss: 5.0535 - val_mse: 5.0535\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 5.3205 - mse: 5.3205 - val_loss: 5.0509 - val_mse: 5.0509\n",
      "wandb: Agent Finished Run: 6y5vj22v \n",
      "\n",
      "wandb: Agent Starting Run: j85mfrf0 with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 32\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: j85mfrf0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/j85mfrf0\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/j85mfrf0</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 4s 66ms/step - loss: 19.1050 - mse: 19.1050 - val_loss: 12.8041 - val_mse: 12.8041\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 18.6522 - mse: 18.6522 - val_loss: 12.1920 - val_mse: 12.1920\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 17.8260 - mse: 17.8260 - val_loss: 10.9195 - val_mse: 10.9195\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 16.0822 - mse: 16.0822 - val_loss: 8.7354 - val_mse: 8.7354\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 13.1342 - mse: 13.1342 - val_loss: 6.3208 - val_mse: 6.3208\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 9.5562 - mse: 9.5562 - val_loss: 5.0663 - val_mse: 5.0663\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 7.2372 - mse: 7.2372 - val_loss: 4.7797 - val_mse: 4.7797\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 6.2211 - mse: 6.2211 - val_loss: 4.7590 - val_mse: 4.7590\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.8570 - mse: 5.8570 - val_loss: 4.8058 - val_mse: 4.8058\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.6108 - mse: 5.6108 - val_loss: 4.9406 - val_mse: 4.9406\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.4049 - mse: 5.4049 - val_loss: 5.1376 - val_mse: 5.1376\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2670 - mse: 5.2670 - val_loss: 5.3980 - val_mse: 5.3980\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2437 - mse: 5.2437 - val_loss: 5.6951 - val_mse: 5.6951\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2581 - mse: 5.2581 - val_loss: 5.9050 - val_mse: 5.9050\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.3021 - mse: 5.3021 - val_loss: 5.9512 - val_mse: 5.9512\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.3038 - mse: 5.3038 - val_loss: 5.8684 - val_mse: 5.8684\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2896 - mse: 5.2896 - val_loss: 5.8185 - val_mse: 5.8185\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2777 - mse: 5.2777 - val_loss: 5.8608 - val_mse: 5.8608\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2915 - mse: 5.2915 - val_loss: 5.8575 - val_mse: 5.8575\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2861 - mse: 5.2861 - val_loss: 5.7847 - val_mse: 5.7847\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2706 - mse: 5.2706 - val_loss: 5.7539 - val_mse: 5.7539\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2661 - mse: 5.2661 - val_loss: 5.7383 - val_mse: 5.7383\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2621 - mse: 5.2621 - val_loss: 5.6725 - val_mse: 5.6725\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2528 - mse: 5.2528 - val_loss: 5.5657 - val_mse: 5.5657\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2464 - mse: 5.2464 - val_loss: 5.5053 - val_mse: 5.5053\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2381 - mse: 5.2381 - val_loss: 5.4908 - val_mse: 5.4908\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2392 - mse: 5.2392 - val_loss: 5.4730 - val_mse: 5.4730\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2402 - mse: 5.2402 - val_loss: 5.5162 - val_mse: 5.5162\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2434 - mse: 5.2434 - val_loss: 5.5868 - val_mse: 5.5868\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2421 - mse: 5.2421 - val_loss: 5.6441 - val_mse: 5.6441\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2476 - mse: 5.2476 - val_loss: 5.7520 - val_mse: 5.7520\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2643 - mse: 5.2643 - val_loss: 5.8758 - val_mse: 5.8758\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.2863 - mse: 5.2863 - val_loss: 6.0172 - val_mse: 6.0172\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.3208 - mse: 5.3208 - val_loss: 6.1733 - val_mse: 6.1733\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.3722 - mse: 5.3722 - val_loss: 6.2236 - val_mse: 6.2236\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.3829 - mse: 5.3829 - val_loss: 6.1838 - val_mse: 6.1838\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.3697 - mse: 5.3697 - val_loss: 6.1593 - val_mse: 6.1593\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.3617 - mse: 5.3617 - val_loss: 6.1493 - val_mse: 6.1493\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.3592 - mse: 5.3592 - val_loss: 6.1438 - val_mse: 6.1438\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 5.3565 - mse: 5.3565 - val_loss: 6.1434 - val_mse: 6.1434\n",
      "wandb: Agent Finished Run: j85mfrf0 \n",
      "\n",
      "wandb: Agent Starting Run: tjyrtxre with config:\n",
      "\tbatch_size: 64\n",
      "\tnn_units: 64\n",
      "\tepochs: 40\n",
      "\tdout_rate: 0.6\n",
      "wandb: Agent Started Run: tjyrtxre\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/sweeps/78fkzlsk</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/fiscal-forcast/IMF/runs/tjyrtxre\" target=\"_blank\">https://app.wandb.ai/fiscal-forcast/IMF/runs/tjyrtxre</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "Error generating diff: Command '['git', 'diff', '--submodule=diff', 'HEAD']' timed out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 67 samples, validate on 30 samples\n",
      "Epoch 1/40\n",
      "67/67 [==============================] - 5s 70ms/step - loss: 19.1032 - mse: 19.1032 - val_loss: 12.4402 - val_mse: 12.4402\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 18.1296 - mse: 18.1296 - val_loss: 9.8002 - val_mse: 9.8002\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 14.3979 - mse: 14.3979 - val_loss: 5.2792 - val_mse: 5.2792\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 7.6903 - mse: 7.6903 - val_loss: 4.8625 - val_mse: 4.8625\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4962 - mse: 5.4962 - val_loss: 5.5053 - val_mse: 5.5053\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2350 - mse: 5.2350 - val_loss: 6.5393 - val_mse: 6.5393\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4838 - mse: 5.4838 - val_loss: 7.3406 - val_mse: 7.3406\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.8364 - mse: 5.8364 - val_loss: 7.5505 - val_mse: 7.5505\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.9468 - mse: 5.9468 - val_loss: 6.9523 - val_mse: 6.9523\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.6469 - mse: 5.6469 - val_loss: 5.9933 - val_mse: 5.9933\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 5.3243 - mse: 5.3243 - val_loss: 5.3451 - val_mse: 5.3451\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2348 - mse: 5.2348 - val_loss: 5.0302 - val_mse: 5.0302\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3362 - mse: 5.3362 - val_loss: 4.9007 - val_mse: 4.9007\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.4394 - mse: 5.4394 - val_loss: 4.8367 - val_mse: 4.8367\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.5418 - mse: 5.5418 - val_loss: 4.8183 - val_mse: 4.8183\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.5715 - mse: 5.5715 - val_loss: 4.8965 - val_mse: 4.8965\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4254 - mse: 5.4254 - val_loss: 5.1419 - val_mse: 5.1419\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2869 - mse: 5.2869 - val_loss: 5.5310 - val_mse: 5.5310\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2323 - mse: 5.2323 - val_loss: 5.9471 - val_mse: 5.9471\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3047 - mse: 5.3047 - val_loss: 6.3565 - val_mse: 6.3565\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4177 - mse: 5.4177 - val_loss: 6.7772 - val_mse: 6.7772\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.5963 - mse: 5.5963 - val_loss: 7.0302 - val_mse: 7.0302\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6951 - mse: 5.6951 - val_loss: 6.8549 - val_mse: 6.8549\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.6174 - mse: 5.6174 - val_loss: 6.5016 - val_mse: 6.5016\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.4723 - mse: 5.4723 - val_loss: 6.1034 - val_mse: 6.1034\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.3695 - mse: 5.3695 - val_loss: 5.8914 - val_mse: 5.8914\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2924 - mse: 5.2924 - val_loss: 5.8101 - val_mse: 5.8101\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2810 - mse: 5.2810 - val_loss: 5.7554 - val_mse: 5.7554\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2666 - mse: 5.2666 - val_loss: 5.7168 - val_mse: 5.7168\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2627 - mse: 5.2627 - val_loss: 5.6691 - val_mse: 5.6691\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 5.2533 - mse: 5.2533 - val_loss: 5.6649 - val_mse: 5.6649\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2525 - mse: 5.2525 - val_loss: 5.5796 - val_mse: 5.5796\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2401 - mse: 5.2401 - val_loss: 5.4118 - val_mse: 5.4118\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2466 - mse: 5.2466 - val_loss: 5.3203 - val_mse: 5.3203\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 1s 20ms/step - loss: 5.2437 - mse: 5.2437 - val_loss: 5.3099 - val_mse: 5.3099\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2450 - mse: 5.2450 - val_loss: 5.2890 - val_mse: 5.2890\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2479 - mse: 5.2479 - val_loss: 5.2625 - val_mse: 5.2625\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 1s 20ms/step - loss: 5.2517 - mse: 5.2517 - val_loss: 5.2317 - val_mse: 5.2317\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2566 - mse: 5.2566 - val_loss: 5.1824 - val_mse: 5.1824\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 5.2737 - mse: 5.2737 - val_loss: 5.2014 - val_mse: 5.2014\n",
      "wandb: Agent Finished Run: tjyrtxre \n",
      "\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweepid, function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
